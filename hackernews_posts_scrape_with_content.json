[
    {
        "title": "C Is Not a Low-Level Language",
        "url": "https://queue.acm.org/detail.cfm?id=3212479",
        "date": "2025-04-16T12:08:13.630920",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Current Issue\nPast Issues\nTopics\nApril 30, 2018\nVolume 16, issue 2\nPDF\nC Is Not a Low-level Language\nYour computer is not a fast PDP-11.\nDavid Chisnall\nIn the wake of the recent Meltdown and Spectre vulnerabilities, it's worth spending some time looking at root causes. Both of these vulnerabilities involved processors speculatively executing instructions past some kind of access check and allowing the attacker to observe the results via a side channel. The features that led to these vulnerabilities, along with several others, were added to let C programmers continue to believe they were programming in a low-level language, when this hasn't been the case for decades.\nProcessor vendors are not alone in this. Those of us working on C/C++ compilers have also participated.\nWhat Is a Low-Level Language?\nComputer science pioneer Alan Perlis defined low-level languages this way:\n\"A programming language is low level when its programs require attention to the irrelevant.\"\n5\nWhile, yes, this definition applies to C, it does not capture what people desire in a low-level language. Various attributes cause people to regard a language as low-level. Think of programming languages as belonging on a continuum, with assembly at one end and the interface to the Starship\nEnterprise\n's computer at the other. Low-level languages are \"close to the metal,\" whereas high-level languages are closer to how humans think.\nFor a language to be \"close to the metal,\" it must provide an abstract machine that maps easily to the abstractions exposed by the target platform. It's easy to argue that C was a low-level language for the PDP-11. They both described a model in which programs executed sequentially, in which memory was a flat space, and even the pre- and post-increment operators cleanly lined up with the PDP-11 addressing modes.\nFast PDP-11 Emulators\nThe root cause of the Spectre and Meltdown vulnerabilities was that processor architects were trying to build not just fast processors, but fast processors that expose the same abstract machine as a PDP-11. This is essential because it allows C programmers to continue in the belief that their language is close to the underlying hardware.\nC code provides a mostly serial abstract machine (until C11, an entirely serial machine if nonstandard vendor extensions were excluded). Creating a new thread is a library operation known to be expensive, so processors wishing to keep their execution units busy running C code rely on ILP (instruction-level parallelism). They inspect adjacent operations and issue independent ones in parallel. This adds a significant amount of complexity (and power consumption) to allow programmers to write mostly sequential code. In contrast, GPUs achieve very high performance without any of this logic, at the expense of requiring explicitly parallel programs.\nThe quest for high ILP was the direct cause of Spectre and Meltdown. A modern Intel processor has up to 180 instructions in flight at a time (in stark contrast to a sequential C abstract machine, which expects each operation to complete before the next one begins). A typical heuristic for C code is that there is a branch, on average, every seven instructions. If you wish to keep such a pipeline full from a single thread, then you must guess the targets of the next 25 branches. This, again, adds complexity; it also means that an incorrect guess results in work being done and then discarded, which is not ideal for power consumption. This discarded work has visible side effects, which the Spectre and Meltdown attacks could exploit.\nOn a modern high-end core, the register rename engine is one of the largest consumers of die area and power. To make matters worse, it cannot be turned off or power gated while any instructions are running, which makes it inconvenient in a dark silicon era when transistors are cheap but powered transistors are an expensive resource. This unit is conspicuously absent on GPUs, where parallelism again comes from multiple threads rather than trying to extract instruction-level parallelism from intrinsically scalar code. If instructions do not have dependencies that need to be reordered, then register renaming is not necessary.\nConsider another core part of the C abstract machine's memory model: flat memory. This hasn't been true for more than two decades. A modern processor often has three levels of cache in between registers and main memory, which attempt to hide latency.\nThe cache is, as its name implies, hidden from the programmer and so is not visible to C. Efficient use of the cache is one of the most important ways of making code run quickly on a modern processor, yet this is completely hidden by the abstract machine, and programmers must rely on knowing implementation details of the cache (for example, two values that are 64-byte-aligned may end up in the same cache line) to write efficient code.\nOptimizing C\nOne of the common attributes ascribed to low-level languages is that they're fast. In particular, they should be easy to translate into fast code without requiring a particularly complex compiler. The argument that a sufficiently smart compiler can make a language fast is one that C proponents often dismiss when talking about other languages.\nUnfortunately, simple translation providing fast code is not true for C. In spite of the heroic efforts that processor architects invest in trying to design chips that can run C code fast, the levels of performance expected by C programmers are achieved only as a result of incredibly complex compiler transforms. The Clang compiler, including the relevant parts of LLVM, is around 2 million lines of code. Even just counting the analysis and transform passes required to make C run quickly adds up to almost 200,000 lines (excluding comments and blank lines).\nFor example, in C, processing a large amount of data means writing a loop that processes each element sequentially. To run this optimally on a modern CPU, the compiler must first determine that the loop iterations are independent. The C\nrestrict\nkeyword can help here. It guarantees that writes through one pointer do not interfere with reads via another (or if they do, that the programmer is happy for the program to give unexpected results). This information is far more limited than in a language such as Fortran, which is a big part of the reason that C has failed to displace Fortran in high-performance computing.\nOnce the compiler has determined that loop iterations are independent, then the next step is to attempt to vectorize the result, because modern processors get four to eight times the throughput in vector code that they achieve in scalar code. A low-level language for such processors would have native vector types of arbitrary lengths. LLVM IR (intermediate representation) has precisely this, because it is always easier to split a large vector operation into smaller ones than to construct larger vector operations.\nOptimizers at this point must fight the C memory layout guarantees. C guarantees that structures with the same prefix can be used interchangeably, and it exposes the offset of structure fields into the language. This means that a compiler is not free to reorder fields or insert padding to improve vectorization (for example, transforming a structure of arrays into an array of structures or vice versa). That's not necessarily a problem for a low-level language, where fine-grained control over data structure layout is a feature, but it does make it harder to make C fast.\nC also requires padding at the end of a structure because it guarantees no padding in arrays. Padding is a particularly complex part of the C specification and interacts poorly with other parts of the language. For example, you must be able to compare two\nstruct\ns using a type-oblivious comparison (e.g.,\nmemcmp\n), so a copy of a\nstruct\nmust retain its padding. In some experimentation, a noticeable amount of total runtime on some workloads was found to be spent in copying padding (which is often awkwardly sized and aligned).\nConsider two of the core optimizations that a C compiler performs: SROA (scalar replacement of aggregates) and loop unswitching. SROA attempts to replace\nstruct\ns (and arrays with fixed lengths) with individual variables. This then allows the compiler to treat accesses as independent and elide operations entirely if it can prove that the results are never visible. This has the side effect of deleting padding in some cases but not others.\nThe second optimization, loop unswitching, transforms a loop containing a conditional into a conditional with a loop in both paths. This changes flow control, contradicting the idea that a programmer knows what code will execute when low-level language code runs. It can also cause significant problems with C's notions of unspecified values and undefined behavior.\nIn C, a read from an uninitialized variable is an unspecified value and is allowed to be any value each time it is read. This is important, because it allows behavior such as lazy recycling of pages: for example, on FreeBSD the\nmalloc\nimplementation informs the operating system that pages are currently unused, and the operating system uses the first write to a page as the hint that this is no longer true. A read to newly\nmalloc\ned memory may initially read the old value; then the operating system may reuse the underlying physical page; and then on the next write to a different location in the page replace it with a newly zeroed page. The second read from the same location will then give a zero value.\nIf an unspecified value for flow control is used (for example, using it as the condition in an\nif\nstatement), then the result is undefined behavior: anything is allowed to happen. Consider the loop-unswitching optimization, this time in the case where the loop ends up being executed zero times. In the original version, the entire body of the loop is dead code. In the unswitched version, there is now a branch on the variable, which may be uninitialized. Some dead code has now been transformed into undefined behavior. This is just one of many optimizations that a close investigation of the C semantics shows to be unsound.\nIn summary, it is possible to make C code run quickly but only by spending thousands of person-years building a sufficiently smart compiler—and even then, only if you violate some of the language rules. Compiler writers let C programmers pretend that they are writing code that is \"close to the metal\" but must then generate machine code that has very different behavior if they want C programmers to keep believing that they are using a fast language.\nUnderstanding C\nOne of the key attributes of a low-level language is that programmers can easily understand how the language's abstract machine maps to the underlying physical machine. This was certainly true on the PDP-11, where each C expression mapped trivially to one or two instructions. Similarly, the compiler performed a straightforward lowering of local variables to stack slots and mapped primitive types to things that the PDP-11 could operate on natively.\nSince then, implementations of C have had to become increasingly complex to maintain the illusion that C maps easily to the underlying hardware and gives fast code. A 2015 survey of C programmers, compiler writers, and standards committee members raised several issues about the comprehensibility of C.\n3\nFor example, C permits an implementation to insert padding into structures (but not into arrays) to ensure that all fields have a useful alignment for the target. If you zero a structure and then set some of the fields, will the padding bits all be zero? According to the results of the survey, 36 percent were sure that they would be, and 29 percent didn't know. Depending on the compiler (and optimization level), it may or may not be.\nThis is a fairly trivial example, yet a significant proportion of programmers either believe the wrong thing or are not sure. When you introduce pointers, the semantics of C become a lot more confusing. The BCPL model was fairly simple: values are words. Each word is either some data or the address of some data. Memory is a flat array of storage cells indexed by address.\nThe C model, in contrast, was intended to allow implementation on a variety of targets, including segmented architectures (where a pointer might be a segment ID and an offset) and even garbage-collected virtual machines. The C specification is careful to restrict valid operations on pointers to avoid problems for such systems. The response to Defect Report 260\n1\nincluded the notion of\npointer provenance\nin the definition of pointer:\n\"Implementations are permitted to track the origins of a bit pattern and treat those representing an indeterminate value as distinct from those representing a determined value. They may also treat pointers based on different origins as distinct even though they are bitwise identical.\"\nUnfortunately, the word\nprovenance\ndoes not appear in the C11 specification at all, so it is up to compiler writes to decide what it means. GCC (GNU Compiler Collection) and Clang, for example, differ on whether a pointer that is converted to an integer and back retains its provenance through the casts. Compilers are free to determine that two pointers to different\nmalloc\nresults or stack allocations always compare as not-equal, even when a bitwise comparison of the pointers may show them to describe the same address.\nThese misunderstandings are not purely academic in nature. For example, security vulnerabilities have been observed from signed integer overflow (undefined behavior in C) and from code that dereferenced a pointer before a null check, indicating to the compiler that the pointer could not be null because dereferencing a null pointer is undefined behavior in C and therefore can be assumed not to happen (https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2009-1897).\nIn light of such issues, it is difficult to argue that a programmer can be expected to understand exactly how a C program will map to an underlying architecture.\nImagining a Non-C Processor\nThe proposed fixes for Spectre and Meltdown impose significant performance penalties, largely offsetting the advances in microarchitecure in the past decade. Perhaps it's time to stop trying to make C code fast and instead think about what programming models would look like on a processor designed to be fast.\nWe have a number of examples of designs that have not focused on traditional C code to provide some inspiration. For example, highly multithreaded chips, such as Sun/Oracle's UltraSPARC Tx series, don't require as much cache to keep their execution units full. Research processors\n2\nhave extended this concept to very large numbers of hardware-scheduled threads. The key idea behind these designs is that with enough high-level parallelism, you can suspend the threads that are waiting for data from memory and fill your execution units with instructions from others. The problem with such designs is that C programs tend to have few busy threads.\nARM's SVE (Scalar Vector Extensions)—and similar work from Berkeley\n4\n—provides another glimpse at a better interface between program and hardware. Conventional vector units expose fixed-sized vector operations and expect the compiler to try to map the algorithm to the available unit size. In contrast, the SVE interface expects the programmer to describe the degree of parallelism available and relies on the hardware to map it down to the available number of execution units. Using this from C is complex, because the autovectorizer must infer the available parallelism from loop structures. Generating code for it from a functional-style map operation is trivial: the length of the mapped array is the degree of available parallelism.\nCaches are large, but their size isn't the only reason for their complexity. The\ncache coherency protocol\nis one of the hardest parts of a modern CPU to make both fast and correct. Most of the complexity involved comes from supporting a language in which data is expected to be both shared and mutable as a matter of course. Consider in contrast an Erlang-style abstract machine, where every object is either thread-local or immutable (Erlang has a simplification of even this, where there is only one mutable object per thread). A cache coherency protocol for such a system would have two cases: mutable or shared. A software thread being migrated to a different processor would need its cache explicitly invalidated, but that's a relatively uncommon operation.\nImmutable objects can simplify caches even more, as well as making several operations even cheaper. Sun Labs' Project Maxwell noted that the objects in the cache and the objects that would be allocated in a young generation are almost the same set. If objects are dead before they need to be evicted from the cache, then never writing them back to main memory can save a lot of power. Project Maxwell proposed a young-generation garbage collector (and allocator) that would run in the cache and allow memory to be recycled quickly. With immutable objects on the heap and a mutable stack, a garbage collector becomes a very simple state machine that is trivial to implement in hardware and allows for more efficient use of a relatively small cache.\nA processor designed purely for speed, not for a compromise between speed and C support, would likely support large numbers of threads, have wide vector units, and have a much simpler memory model. Running C code on such a system would be problematic, so, given the large amount of legacy C code in the world, it would not likely be a commercial success.\nThere is a common myth in software development that parallel programming is hard. This would come as a surprise to Alan Kay, who was able to teach an actor-model language to young children, with which they wrote working programs with more than 200 threads. It comes as a surprise to Erlang programmers, who commonly write programs with thousands of parallel components. It's more accurate to say that parallel programming in a language with a C-like abstract machine is difficult, and given the prevalence of parallel hardware, from multicore CPUs to many-core GPUs, that's just another way of saying that C doesn't map to modern hardware very well.\nReferences\n1. C Defect Report 260. 2004;\nhttp://www.open-std.org/jtc1/sc22/wg14/www/docs/dr_260.htm\n.\n2. Chadwick, G. A. 2013. Communication centric, multi-core, fine-grained processor architecture. Technical Report 832. University of Cambridge, Computer Laboratory;\nhttp://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-832.pdf\n.\n3. Memarian, K., Matthiesen, J., Lingard, J., Nienhuis, K., Chisnall, D. Watson, R. N. M., Sewell, P. 2016. Into the depths of C: elaborating the de facto standards.\nProceedings of the 37\nth\nACM SIGPLAN Conference on Programming Language Design and Implementation\n: 1-15;\nhttp://dl.acm.org/authorize?N04455\n.\n4. Ou, A., Nguyen, Q., Lee, Y., Asanović, K. 2014. A case for MVPs: mixed-precision vector processors. Second International Workshop on Parallelism in Mobile Platforms at the 41st International Symposium on Computer Architecture.\n5. Perlis, A. 1982. Epigrams on programming.\nACM SIGPLAN\nNotices\n17(9).\nRelated articles\nThe Challenge of Cross-language Interoperability\nDavid Chisnall\nInterfacing between languages is increasingly important.\nhttps://queue.acm.org/detail.cfm?id=2543971\nFinding More than One Worm in the Apple\nMike Bland\nIf you see something, say something.\nhttps://queue.acm.org/detail.cfm?id=2620662\nCoding for the Code\nFriedrich Steimann, Thomas Kühne\nCan models provide the DNA for software development?\nhttps://queue.acm.org/detail.cfm?id=1113336\nDavid Chisnall\nis a researcher at the University of Cambridge, where he works on programming language design and implementation. He spent several years consulting in between finishing his Ph.D. and arriving at Cambridge, during which time he also wrote books on Xen and the Objective-C and Go programming languages, as well as numerous articles. He also contributes to the LLVM, Clang, FreeBSD, GNUstep, and Étoilé open-source projects, and he dances the Argentine tango.\nCopyright © 2018 held by owner/author. Publication rights licensed to ACM.\nOriginally published in Queue vol. 16, no. 2\n—\nComment on this article in the\nACM Digital Library\nMore related articles:\nMatt Godbolt\n-\nOptimizations in C++ Compilers\nThere’s a tradeoff to be made in giving the compiler more information: it can make compilation slower. Technologies such as link time optimization can give you the best of both worlds. Optimizations in compilers continue to improve, and upcoming improvements in indirect calls and virtual function dispatch might soon lead to even faster polymorphism.\nUlan Degenbaev, Michael Lippautz, Hannes Payer\n-\nGarbage Collection as a Joint Venture\nCross-component tracing is a way to solve the problem of reference cycles across component boundaries. This problem appears as soon as components can form arbitrary object graphs with nontrivial ownership across API boundaries. An incremental version of CCT is implemented in V8 and Blink, enabling effective and efficient reclamation of memory in a safe manner.\nTobias Lauinger, Abdelberi Chaabane, Christo Wilson\n-\nThou Shalt Not Depend on Me\nMost websites use JavaScript libraries, and many of them are known to be vulnerable. Understanding the scope of the problem, and the many unexpected ways that libraries are included, are only the first steps toward improving the situation. The goal here is that the information included in this article will help inform better tooling, development practices, and educational efforts for the community.\nRobert C. Seacord\n-\nUninitialized Reads\nMost developers understand that reading uninitialized variables in C is a defect, but some do it anyway. What happens when you read uninitialized objects is unsettled in the current version of the C standard (C11).3 Various proposals have been made to resolve these issues in the planned C2X revision of the standard. Consequently, this is a good time to understand existing behaviors as well as proposed revisions to the standard to influence the evolution of the C language. Given that the behavior of uninitialized reads is unsettled in C11, prudence dictates eliminating uninitialized reads from your code.\n© ACM, Inc. All Rights Reserved."
    },
    {
        "title": "Vercel Blocked in Whole of Spain",
        "url": "https://twitter.com/rauchg/status/1912313667930189991",
        "date": "2025-04-16T12:08:13.630920",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "See new posts"
    },
    {
        "title": "Stop Conflating Genius with Asshole",
        "url": "https://www.joanwestenberg.com/stop-conflating-genius-with-asshole/",
        "date": "2025-04-16T12:08:13.630920",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Somewhere between Steve Jobs and Elon Musk, we started believing that in order to be brilliant, you had to be unbearable. That cruelty itself was a kind of clarity. That the sneer, the kicking and screaming, the impossible-to-please demands were just signs of a mind operating on a different level.\nThe myth took root.\nThe sharper the mind, the sharper the tongue.\nKindness is mediocrity in disguise.\nIt's become a socially acceptable archetype. The difficult genius. The tortured artist. The visionary CEO who treats everyone like disposable cogs. The director who berates their crew because \"greatness demands suffering.\" The startup founder who burns through people like lighter fluid because \"disruption isn’t polite.\"\nWe turned a preference for decency into a liability. We started grading cruelty on a curve, as if insight automatically offsets damage. As if the trauma left in some dickhead's wake is just the tax for being in the orbit of genius.\nBut excellence doesn’t require abuse.\nIt never did.\nWe just stopped asking for better.\nWe lionized those who hurt others in the name of vision and made excuses for behavior that, in any other context, would be called what it is: toxic.\nWe wrapped cruelty in clever quotes and pointed to output as if it justifies everything that comes before it.\nWe forgot that power protects power, and some people weaponize excellence so they can avoid accountability.\nThe damage is real, and it’s everywhere. People walk away from entire careers. They stop creating. They go quiet. They start to internalize the abuse as a test they failed, as proof they weren’t built for brilliance. That’s not a meritocracy. It’s a rigged system designed by those who profit from a warped, self-serving definition of genius; one that looks like them, sounds like them, and excuses everything they do.\nThe smartest people I know don’t leave trails of destruction. They listen more than they speak. They build things that last because they build with people, not on top of them. They know that empathy doesn’t weaken ideas, it strengthens them. That you don’t have to be an asshole to be a genius.\nThe lie persists because it's convenient. Because it gives cover. Because if cruelty is the price of vision, then we never have to confront how many mediocre people we let get away with inhumane bullshit, just because they were good at marketing themselves.\nGenius doesn’t look like domination. It looks like collaboration. It looks like the humility to know you’re not the smartest person in every room, and the strength to make space for those who are. If someone needs to belittle, berate, or break others to feel powerful, they’re not a genius—they’re a tyrant in a hoodie, a bully with a pitch deck, a tantrum in search of a title.\nAnd we should stop fucking clapping.\n🍕\nMy goal this year is to make Westenberg and my news site, The Index, my full-time job. The pendulum has swung pretty far back against progressive writers, particularly trans creators, but I'm not going anywhere.\nI'm trying to write as much as I can to balance out a world on fire. Your subscription directly supports permissionless publishing and helps create a sustainable model for writing and journalism that answers to readers, not advertisers or gatekeepers.\nPlease consider signing up for a paid monthly or annual membership to support my writing and independent/sovereign publishing."
    },
    {
        "title": "'A big part of my job is sacking people': Life in HR",
        "url": "https://news.sky.com/story/a-big-part-of-my-job-is-sacking-people-life-in-hr-from-bosses-having-affairs-to-dealing-with-gen-z-13348644",
        "date": "2025-04-16T12:08:13.631937",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "'A big part of my job is sacking people': Life in HR - from bosses having affairs to dealing with Gen Z\nCaroline Wood sits down with the Money blog to reveal what life is really like as an HR boss, including the most common complaints she has to deal with, and what you need to be wary of on a staff night out.\nMonday 14 April 2025 14:13, UK\nImage:\nFile pic: iStock\nWhy you can trust Sky News\nIf you've ever spent your Monday morning commute daydreaming about starting afresh with your career, this feature is for you.\nEach week in the\nMoney blog\n, we speak to someone from a different profession to discover what it's really like.\nThis week, we chat to Caroline Wood, managing director of alphr limited, about life in HR...\nSponsored link\n10 Interesting Facts About Earth's Oceans\nCommunication is one of the biggest things people complain about...\nOf course some staff are going to complain about salary, but it's been clear to me over the years that if an employer doesn't communicate with their staff properly, this can cause real problems.\nImage:\nCaroline Wood\nA fair proportion of advice I give my clients is about \"sacking\" people...\nI typically advise every week on this area. Businesses that have staff that aren't performing well, for whatever reason, will eventually want to know how to end someone's employment. Unfortunately, it happens a lot.\nBut...\nHR mainly falls into four areas: recruitment, reward, learning and development and employee relations. The first three you can specialise in and are the super positive sides of HR - who doesn't want to be recruited efficiently, trained well and work for a company that thinks deeply about the way they reward staff?\nYou get a brilliant insight into people...\nWhen you are in the profession long enough, it gives you a brilliant insight into life and, while troubling at times, it is highly interesting and fulfilling.\nCOVID has changed what employees expect...\nBefore lockdown, employees seemed to resolve their personal issues themselves to an extent, but I have seen a real change in employees having issues at home and wanting their employers to help them solve what they are going through.\nMore from Money\nMoney: UK inflation falls - but there's a big caveat\nInflation surprisingly continues to fall but expect an April rebound due to across the board bill hikes\n'Likely' British Steel will be nationalised, says business secretary Jonathan Reynolds\nWatch out for nights out with colleagues...\nEven though a night out is outside of normal working hours, if something happens like an argument, a fight or inappropriate behaviour and that event is subsequently brought into the workplace, your employer might have to get involved.\nGen Z...\nare much more likely to complain, know their rights and they can also use litigious language when communicating with their employers.\nEmployees are people and people bring the very best and very worst into the workplace...\nOver the years I have dealt with issues I would never have dreamt I would, including:\nAn employee who ended up being a vicious murderer;\nBeing on the other end of the phone with a client while they were on the phone with 999 as they had a suicidal employee in work;\nInvestigating an employee who stole over £120,000 from her employer;\nDealing with an employee who faked cancer treatment in order to borrow money from her employer. This was reported to the police and she absconded. However, we later found out she was wanted in Europe for the same offences;\nMiscarriage;\nParental abuse.\nI've been very lucky throughout my career...\nin that I haven't been asked, and nor would I, to dismiss an employee unjustly.\nI've cried at work...\nAn employee took his own life - he didn't turn up at work one day and it was absolutely devastating. He used to bake cakes for his colleagues. Myself and some of my colleagues represented the employer at his funeral and it was heartbreaking; I just dissolved into tears.\nI had to advise someone who was having an affair with a colleague...\nbut whose partner also worked at the company. I had to discuss how to manage that situation when their partner found out.\nCompensation from tribunals is unlimited...\nThe ultimate risk of a situation not being dealt with properly is a claim is raised at the employment tribunal, where compensation, for example in discrimination cases, is uncapped. In today's world, where employees know their rights, even just the legal fees of defending a case can run into the thousands.\nRead more from this series:\n'I'm a royal photographer - here's who is nicest to us'\nMy life as a bodyguard: Drunk celebs and fighting pirates\n'Being a teacher isn't all holidays and 3pm finishes'\nGet Sky News on WhatsApp\nFollow our channel and never miss an update\nTap here to follow\nI trained as a hairdresser...\nI then served in the RAF until I had my children. I took some time out and when I was ready to work full-time again, I went back to college first and learned how to use a computer and type. From there, I took on admin roles that eventually led to me being asked to \"pick up\" the HR. From that accidental swerve, I ended up having a near 20-year career that includes employment and my own consultancies - I even have some A-listers on my books now.\nThe most important skill needed is...\nbeing good with people. Clients come to me mostly when they are having problems and so in addition to having the knowledge behind the questions they are asking, there also has to be interactions on my part that are understanding, calming, reassuring and embody \"it's okay, you can trust me, I've got you\". I get to help people, business owners who can be in quite the predicament sometimes and between us, we work it out and resolve the issue - when that happens, you can hear the physical relief in their voices and it's very satisfying.\nI work from home...\nand get to hang out with Bonnie, my dog - she lays beside me when I am working and nudges me every now and then for a treat!\nEntry-level roles, say that of an HR administrator, start at about £26-28k...\nMid-level roles at HR manager level can be between £35-£55k, and once you get into HR director level roles, that can move you into a six-figure salary.\nI'm not someone who wants to retire young...\nI can see from the older members of my family that actually it's even more important to keep that structure as you get older. Work is good; it keeps my brain going, I'll just stop when I want to, but in the meantime, I have a mortgage to pay.\nWant to take part in this feature? Email moneyblog@sky.uk."
    },
    {
        "title": "Aleksander Solzhenitsyn – Live Not by Lies (1974)",
        "url": "https://www.solzhenitsyncenter.org/live-not-by-lies",
        "date": "2025-04-16T12:08:13.631937",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Articles, Essays, and Speeches\n> Live Not by Lies\nLive Not by Lies\nOn the day Solzhenitsyn was arrested, February, 12, 1974, he released the text of “Live Not by Lies.” The next day, he was exiled to the West, where he received a hero’s welcome. This moment marks the peak of his fame. Solzhenitsyn equates “lies” with ideology, the illusion that human nature and society can be reshaped to predetermined specifications. And his last word before leaving his homeland urges Soviet citizens as individuals to refrain from cooperating with the regime’s lies. Even the most timid can take this least demanding step toward spiritual independence. If many march together on this path of passive resistance, the whole inhuman system will totter and collapse.\n--\nby Edward E. Ericson, Jr. and Daniel J. Mahoney,\nThe Solzhenitsyn Reader\nThere was a time when we dared not rustle a whisper. But now we write and read samizdat and, congregating in the smoking rooms of research institutes, heartily complain to each other of all\nthey\nare muddling up, of all\nthey\nare dragging us into! There’s that unnecessary bravado around our ventures into space, against the backdrop of ruin and poverty at home; and the buttressing of distant savage regimes; and the kindling of civil wars; and the ill-thought-out cultivation of Mao Zedong (at our expense to boot)—in the end we’ll be the ones sent out against him, and we’ll have to go, what other option will there be? And they put whomever they want on trial, and brand the healthy as mentally ill—and it is always “they,” while\nwe\nare—helpless.\nWe are approaching the brink; already a universal spiritual demise is upon us; a physical one is about to flare up and engulf us and our children, while we continue to smile sheepishly and babble:\n“But what can we do to stop it? We haven’t the strength.”\nWe have so hopelessly ceded our humanity that for the modest handouts of today we are ready to surrender up all principles, our soul, all the labors of our ancestors, all the prospects of our descendants—anything to avoid disrupting our meager existence. We have lost our strength, our pride, our passion. We do not even fear a common nuclear death, do not fear a third world war (perhaps we’ll hide away in some crevice), but fear only to take a civic stance! We hope only not to stray from the herd, not to set out on our own, and risk suddenly having to make do without the white bread, the hot water heater, a Moscow residency permit.\nWe have internalized well the lessons drummed into us by the state; we are forever content and comfortable with its premise: we cannot escape the\nenvironment,\nthe social conditions; they shape us, “being determines consciousness.” What have we to do with this? We can do nothing.\nBut we can do—everything!—even if we comfort and lie to ourselves that this is not so. It is not “they” who are guilty of everything, but\nwe ourselves,\nonly\nwe!\nSome will counter: But really, there is nothing to be done! Our mouths are gagged, no one listens to us, no one asks us. How can we make\nthem\nlisten to us?\nTo make them reconsider—is impossible.\nThe natural thing would be simply not to reelect them, but there are no re-elections in our country.\nIn the West they have strikes, protest marches, but we are too cowed, too scared: How does one just give up one’s job, just go out onto the street?\nAll the other fateful means resorted to over the last century of Russia’s bitter history are even less fitting for us today—true, let’s not fall back on them! Today, when all the axes have hewn what they hacked, when all that was sown has borne fruit, we can see how lost, how drugged were those conceited youths who sought, through terror, bloody uprising, and civil war, to make the country just and content. No thank you, fathers of enlightenment! We now know that the vileness of the means begets the vileness of the result. Let our hands be clean!\nSo has the circle closed? So is there indeed no way out? So the only thing left to do is wait inertly: What if something just happens\nby itself?\nBut it will never come unstuck\nby itself,\nif we all, every day, continue to acknowledge, glorify, and strengthen it, if we do not, at the least, recoil from its most vulnerable point.\nFrom lies.\nWhen violence bursts onto the peaceful human condition, its face is flush with self-assurance, it displays on its banner and proclaims: “I am Violence! Make way, step aside, I will crush you!” But violence ages swiftly, a few years pass—and it is no longer sure of itself. To prop itself up, to appear decent, it will without fail call forth its ally—Lies. For violence has nothing to cover itself with but lies, and lies can only persist through violence. And it is not every day and not on every shoulder that violence brings down its heavy hand: It demands of us only a submission to lies, a daily participation in deceit—and this suffices as our fealty.\nAnd therein we find, neglected by us, the simplest, the most accessible key to our liberation: a\npersonal nonparticipation in lies!\nEven if all is covered by lies, even if all is under their rule, let us resist in the smallest way: Let their rule hold\nnot through me!\nAnd this is the way to break out of the imaginary encirclement of our inertness, the easiest way for us and the most devastating for the lies. For when people renounce lies, lies simply cease to exist. Like parasites, they can only survive when attached to a person.\nWe are not called upon to step out onto the square and shout out the truth, to say out loud what we think—this is scary, we are not ready. But let us at least refuse to say what we\ndo not\nthink!\nThis is the way, then, the easiest and most accessible for us given our deep-seated organic cowardice, much easier than (it’s scary even to utter the words) civil disobedience à la Gandhi.\nOur way must be:\nNever knowingly support lies!\nHaving understood where the lies begin (and many see this line differently)—step back from that gangrenous edge! Let us not glue back the flaking scales of the Ideology, not gather back its crumbling bones, nor patch together its decomposing garb, and we will be amazed how swiftly and helplessly the lies will fall away, and that which is destined to be naked will be exposed as such to the world.\nAnd thus, overcoming our timidity, let each man choose: Will he remain a witting servant of the lies (needless to say, not due to natural predisposition, but in order to provide a living for the family, to rear the children in the spirit of lies!), or has the time come for him to stand straight as an honest man, worthy of the respect of his children and contemporaries? And from that day onward he:\n· Will not write, sign, nor publish in any way, a single line distorting, so far as he can see, the truth;\n· Will not utter such a line in private or in public conversation, nor read it from a crib sheet, nor speak it in the role of educator, canvasser, teacher, actor;\n· Will not in painting, sculpture, photograph, technology, or music depict, support, or broadcast a single false thought, a single distortion of the truth as he discerns it;\n· Will not cite in writing or in speech a single “guiding” quote for gratification, insurance, for his success at work, unless he fully shares the cited thought and believes that it fits the context precisely;\n· Will not be forced to a demonstration or a rally if it runs counter to his desire and his will; will not take up and raise a banner or slogan in which he does not fully believe;\n· Will not raise a hand in vote for a proposal which he does not sincerely support; will not vote openly or in secret ballot for a candidate whom he deems dubious or unworthy;\n· Will not be impelled to a meeting where a forced and distorted discussion is expected to take place;\n· Will at once walk out from a session, meeting, lecture, play, or film as soon as he hears the speaker utter a lie, ideological drivel, or shameless propaganda;\n· Will not subscribe to, nor buy in retail, a newspaper or journal that distorts or hides the underlying facts.\nThis is by no means an exhaustive list of the possible and necessary ways of evading lies. But he who begins to cleanse himself will, with a cleansed eye, easily discern yet other opportunities.\nYes, at first it will not be fair. Someone will have to temporarily lose his job. For the young who seek to live by truth, this will at first severely complicate life, for their tests and quizzes, too, are stuffed with lies, and so choices will have to be made. But there is no loophole left for anyone who seeks to be honest: Not even for a day, not even in the safest technical occupations can he avoid even a single one of the listed choices—to be made in favor of either truth or lies, in favor of spiritual independence or spiritual servility. And as for him who lacks the courage to defend even his own soul: Let him not brag of his progressive views, boast of his status as an academician or a recognized artist, a distinguished citizen or general. Let him say to himself plainly: I am cattle, I am a coward, I seek only warmth and to eat my fill.\nFor us, who have grown staid over time, even this most moderate path of resistance will be not be easy to set out upon. But how much easier it is than self-immolation or even a hunger strike: Flames will not engulf your body, your eyes will not pop out from the heat, and your family will always have at least a piece of black bread to wash down with a glass of clear water.\nBetrayed and deceived by us, did not a great European people—the Czechoslovaks—show us how one can stand down the tanks with bared chest alone, as long as inside it beats a worthy heart?\nIt will not be an easy path, perhaps, but it is the easiest among those that lie before us. Not an easy choice for the body, but the only one for the soul. No, not an easy path, but then we already have among us people, dozens even, who have for years abided by all these rules, who live by the truth.\nAnd so: We need not be the first to set out on this path, Ours is but to join! The more of us set out together, the thicker our ranks, the easier and shorter will this path be for us all! If we become thousands—they will not cope, they will be unable to touch us. If we will grow to tens of thousands—we will not recognize our country!\nBut if we shrink away, then let us cease complaining that someone does not let us draw breath—we do it to ourselves! Let us then cower and hunker down, while our comrades the biologists bring closer the day when our thoughts can be read and our genes altered.\nAnd if from\nthis also\nwe shrink away, then we are worthless, hopeless, and it is of us that Pushkin asks with scorn:\nWhy offer herds their liberation?\n.....................\nTheir heritage each generation\nThe yoke with jingles, and the whip.\nFebruary 12, 1974\ntranslated from the Russian by Yermolai Solzhenitsyn\n© 2006 English-language copyright Yermolai Solzhenitsyn\nAvailable Formats\nThe Solzhenitsyn Reader\nHardcover\nAmazon\nPaperback\nAmazon\n|\nBarnes & Noble\nFrom the Blog\nFeb 17, 2022\nThe persistence of the Lie\nFeb 17, 2022\nFeb 17, 2022\nFeb 1, 2022\nWinston Marshall and Ignat Solzhenitsyn\nFeb 1, 2022\nFeb 1, 2022\nJun 26, 2021\n\"Why I’m Leaving Mumford & Sons\"\nJun 26, 2021\nJun 26, 2021\nView all posts"
    },
    {
        "title": "How Rolldown Works: Module Loading, Dependency Graphs and Optimization Explained",
        "url": "https://www.atriiy.dev/blog/rolldown-module-loader-and-dependency-graph",
        "date": "2025-04-16T12:08:13.631937",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "How Rolldown Works: Module Loading, Dependency Graphs, and Optimization Explained\nApril 12, 2025\nRewind 10s\nPause\nPlay\nForward 10s\n% buffered\n00:00\n00:00\n00:00\nUnmute\nMute\nSettings\nCaptions\nDisabled\nQuality\nundefined\nSpeed\nNormal\nCaptions\nGo back to previous menu\nQuality\nGo back to previous menu\nSpeed\nGo back to previous menu\n0.5×\n0.75×\nNormal\n1.25×\n1.5×\n1.75×\n2×\n4×\nExit fullscreen\nEnter fullscreen\nIntroduction\nR\nolldown is a blazing-fast JavaScript bundler written in Rust, designed to work seamlessly with the\nRollup\nAPI. The primary goal of it is to power\nVite\nas its unified bundler in the near future. Right now, Vite relies on esbuild for lightning-quick local development and Rollup for production builds. Switching to a single bundler like Rolldown promises to streamline the process, giving developers more confidence that what they see in development matches what goes live. Plus, Rolldown delivers bundling speeds that potentially 10 ~ 30 times faster than Rollup. Curious for more? Check out the\nofficial Rolldown documentation\nfor all the details.\nIn this article, we’ll begin with a high-level overview of Rolldown to give you a panoramic understanding of how it works. This will help you stay oriented and avoid getting lost in the details too early. From there, we’ll dive into the core focus of this article: the\nmodule loader\n, the heart of Rolldown’s scan stage. We’ll explore its key functions and the important data structures that support it.\nAfter that, we’ll discuss the\ndependency graph\nand some of the\nperformance optimizations\nRolldown employs. While a few of these topics may have come up before, they’re worth revisiting in context — and they play a crucial role in understanding how Rolldown achieves its speed and efficiency.\nOkay, let’s roll into Rolldown. 😉\nHigh-level overview of Rolldown\nRolldown has four main steps:\ndependency graph construction\n,\noptimization\n,\ncode generation / bundling\n,\noutput\n. The output bundle will be written into memory or file system depends on if it’s for local development or production building. You can find the entry module in\ncrates/rolldown/src/bundler.ts\n. Here is a diagram about the process.\nScan Dependencies\nRepeat until all dependencies are processed\nGenerate Mode: rolldown.generate\nWrite Mode: rolldown.write\nStart: Read Config & Entry Points\nParse Entry Module\nBuild Module Graph\nLoad & Parse Dependency Modules\nCode optimization\nCode Generation: Generate Chunks\nReturn Output Assets In Memory\nWrite Output Files to Disk\nThe module loader is a core part of\nbuild module graph\nstage. It’s invoked from the\nscan\nfunction within the\nBundler\nstruct. The entire scanning process has been encapsulated into a dedicated\nScanStage\nstruct for better separation of concerns.\nThe real work, however, happens inside the\nModuleLoader\n. It handles key tasks like building the dependency graph and processing individual modules. It’s where many heavy lifting happens — and the main focus of this article.\nModule loader\nIn short, the\nmodule loader\nis responsible for locating, fetching, and parsing individual modules — including source files, CSS, and more. It transform them into an internal data structure that the bundler can understand and work with. This step is crucial for building an accurate and efficient module graph.\nHere’s a diagram showing how Rolldown uses the module loader during the bundling process:\nBundler: Prepare needs to Build Module Graph\nCreate Module Loader Instance\nBundler: Calls Module Loader's fetch_modules\nModule Loader Operation\nModule Loader: Returns Aggregated Results to Bundler\nBundler: Uses Results for Next Steps - e.g., Linking, Optimization, Code Gen\nAll of these steps happen inside the\nscan\nfunction of the\nScanStage\n. You can think of it as a builder that orchestrates and encapsulates the logic required to run the module loader.\nFetch modules\nfetch_modules\nis where the magic begins. It acts like a scheduler, kicking off a series of async tasks to parse all related modules. This function processes the user-defined entry points — the starting point of the module scanning algorithm.\nBefore reaching\nfetch_modules\n, the\nscan\nfunction resolves these entries and transforms them into the internal\nResolvedId\nstructure. This preprocessing step is handled by the\nresolve_user_defined_entries\nfunction.\nHere’s a diagram showing the core workflow of the\nfetch_modules\nfunction:\nModule Done\nYes\nNo\nPlugin Fetch Module\nPlugin Add Entry\nBuild Error\nYes\nNo\nStart: Receive Resolved User Defined Entries\nInitialize: Task Counter, Module Cache, Result Collectors\nLaunch Async Tasks for Each Entry\nMessage Loop: Listen while Task Counter > 0\nStore Results; Process Dependencies\nNew Dependencies?\nLaunch Tasks for New Dependencies; Increment Counter\nDecrement Task Counter\nLaunch Task for Requested Module; Increment Counter\nResolve & Launch Task for New Entry; Increment Counter\nRecord Error; Decrement Counter\nTask Counter == 0?\nFinalize: Update Dependency Graph, Organize Results\nEnd: Return Output to Caller\nLooks a bit complex, right? That’s because this stage packs in a lot of optimizations and features. But don’t worry — we can skip the finer details and focus on the high-level flow.\nAs mentioned earlier, the\nfetch_modules\nfunction takes the resolved user-defined entries as input and begins processing. For each entry, it calls\ntry_spawn_new_task\n, which determines whether the module is internal or external, applies the appropriate logic, and returns a type safe\nModuleIdx\n. This index is later used to reference the corresponding module throughout the system.\nOnce the initial tasks are spawned for all entries,\nfetch_modules\nenters a loop to listen on a message channel defined using\ntokio::sync::mpsc\n. Each module processing task holds a sender handle to this channel and reports events back to the main process. The message listener inside\nfetch_modules\nreacts to these messages, which includes:\nNormal / Runtime module done\n: Store the result and schedules any dependent modules.\nFetch module\n: Responds to plugin requests to load specific modules on demand.\nAdd entry module\n: Adds new entry points during scanning, typically triggered by plugins.\nBuild erros\n: Captures any errors that occur during loading or transformation.\nOnce all modules have been processed and no more messages are incoming, the channel is closed and the loop exits. Then,\nfetch_modules\nperforms some cleanup: it stores the processed entry points, updates the dependency graph, and returns an aggregated result back to the caller — the\nscan\nfunction. This result inclues modules, ASTs, symbols, entries, warnings, and more, all of which will be used in the upcoming optimization and code generation phases.\nSpawn new task\nThe function\ntry_spawn_new_task\nfirst tries to retrieve the\nModuleIdx\nfrom the module loader’s cache. Since the scan stage essentially traverses a graph, this cache tracks the visit state of each module using a hash map — where the key is the module’s ID and the value indicates whether it has already been processed.\nNext, based on the module type, it transforms the module into either a normal module or an external module structure for further processing. It’s important to understand how\nexternal modules\nare handled. These modules aren’t bundled by Rolldown — they’re expected to be provided by the runtime environment, like libraries from\nnode_modules\n. Although they’re not included in the final bundle, Rolldown still records metadata about them, effectively treating them as placeholders. The bundle assumes these modules will be available at runtime and will reference them as needed.\nOn the other hand,\nnormal module\n— typically created by the user — are processed differently. For each of them,\ntry_spawn_new_task\ncreates a dedicated module task and runs it asynchronously. These tasks are managed by Tokio, Rust’s asynchronous runtime. As mentioned earlier, each task holds a\nsender\nfor the message channel, which it uses to report errors, newly discovered imports, or dynamically added entry points as it runs.\nData structures\nTo improve performance and code reusability, Rolldown makes extensive use of specialized data structures. Understanding a few key ones used in the module loader will give you a clearer mental model of how the scan process works under the hood.\nModuleIdx & HybridIndexVec\nModuleIdx\nis a custom numeric index that gets assigned dynamically as modules are processed. This kind of index is beneficial for both\ntype safety\nand\nperformance\n. Instead of passing around or cloning entire module structures, Rolldown uses this lightweight identifier — much like a pointer in other programming languages — to reference modules throughout the system.\npub\nstruct\nModuleIdx\n=\nu32\n;\nHybridIndexVec\nis a smart, adaptive container used to store module data. Since Rolldown operates primarily on\nModuleIdx\nvalues rather than the actual module data, efficient ID-based lookup is essential. This is where\nHybridIndexVec\ncomes in — it’s optimized for different bundling scenarios.\npub\nenum\nHybridIndexVec\n<\nI\n:\nIdx\n,\nT\n>\n{\nIndexVec\n(\nIndexVec\n<\nI\n,\nT\n>\n)\n,\nMap\n(\nFxHashMap\n<\nI\n,\nT\n>\n)\n,\n}\nBundlers typically work in two modes:\nFull bundling\n(common in production builds), where all modules are scanned once and stored continuously. For this case, Rolldown uses a compact and performant structure called\nIndexVec\n, which behaves like a vector but enforces type-safe indexing.\nPartial bundling\n(often used in development), where the module graph can change frequently — for example, when a developer edits a file. In this case, a\nsparse\nstructure is more appropriate. Rolldown uses a hash map based on the\nFxHash\nalgorithm for fast key / value access.\nFxHash\nis faster than Rust’s default hashing algorithm, though it has a slightly higher chance of collisions. Since both the keys and values are managed internally, and predictable performance is more important than security, this tradeoff is acceptable for Rolldown’s use case.\nModule\nNormal module are user-defined — typically source files that need to be parsed, transformed, or analyzed. Rolldown loads these files and processes them based on their file extensions. For example,\n.ts\n(TypeScript) files are parsed using Oxc, a high-performance JavasScript / TypeScript parser.\npub\nenum\nModule\n{\nNormal\n(\nBox\n<\nNormalModule\n>\n),\nExternal(Box\n<\nExternalModule\n>\n),\n}\nThe internal\nNormalModule\nstruct stores detailed information about each module, including basic metadata like\nidx\nand\nmodule_type\n, as well as richer representations of the module’s content. Depending on this file type, these can includes:\necma_view\nfor JavaScript / TypeScript modules\ncss_view\nfor stylesheets\nasset_view\nfor static assets\nThis structure allows later stage of the pipeline — such as optimization and code generation — to work efficiently with the parsed content.\nScanStageCache\nThis is a structure holds all cache during module loading. Here is the data structure definition.\npub\nstruct\nScanStageCache\n{\nsnapshot\n:\nOption\n<\nNormalizedScanStageOutput\n>\n,\npub module_id_to_idx: FxHashMap\n<\nArcStr\n,\nVisitState\n>\n,\npub importers: IndexVec\n<\nModuleIdx\n,\nVec\n<\nImporterRecord\n>\n>\n,\n}\nsnapshot\nholds the result s of a previous scan stage and is used to support\nincremental builds\n. Instead of re-scanning everything from scratch, Rolldown can reuse parts of the previous scan, dramatically improving build times when only a few files have changed.\nmodule_id_to_idx\nis a hash map that stores the mapping between a module’s ID and its visit state. It allows the program to quickly determine whether a module has already been processed.\nThe key is an\nArcStr\n, which is a memory-efficient, reference-counted string optimized for sharing across threads. More importantly, this string is a\nglobal unique and stable identifier\nfor the module — and remains consistent across multiple builds, which is critical for cache reliability.\nimporters\nrepresents a\nreverse adjacency table\nof the module graph. For each module, it tracks\nwhich other modules import it\n. This is particularly useful during incremental builds: when the content of module changes,\nimporters\nhelps Rolldown quickly determine the scope of affected modules — essentially identifying what needs to be reprocessed.\nNote that temporary version of\nimporters\nis also stored inside\nIntermediateNormalModules\n. You can think of it as a\ndraft state\n, created dynamically during the current build process on the fly.\nDependency graph\nThe\ndependency graph\ndescribes how modules depends on each other — and it’s one of the most important outputs of the scanning stage. Rolldown uses this relationship map in later phases for tasks like tree shaking, chunking, and code generation.\nBefore diving into the actually implementation, let’s take a moment to introduce the concept of an\nadjacency table\n, which plays a key role in how the graph is represented and traversed.\nGraph and Adjacency table\nAs we all know, a graph is a data structure used to represent\nconnections\nbetween things. It consists of:\nNodes — the items or entities being connected\nEdges — the connections or relationships between those nodes\nThere are two common ways to represent a graph:\nadjacency matrix\nand\nadjacency table\n.\nAn adjacency matrix is a 2D grid (or matrix) where each row and column corresponds to a node. The value at a given cell indicates whether an edge exists between the two nodes. For example, a\n1\nmight mean there’s a connection, and a\n0\nmeans there’s not.\n|\nA\n|\nB\n|\nC\nA\n|\n0\n|\n1\n|\n0\nB\n|\n1\n|\n0\n|\n1\nC\n|\n0\n|\n1\n|\n0\nThis approach is simple and works well for dense graphs, where most nodes are connected. However, it’s memory-inefficient for sparse graphs, which is usually the case for module graphs in a bundler like Rolldown. (I believe no one wants to import everything into every single module in their project. 😉)\nAn adjacency table is a collection where each node keeps a list of its neighbors. Instead of using a fixed-size grid, it stores only the actually connections, making it more efficient for sparse graphs.\nFor example, if a node A is connected to node B, and B is connected to A and C, finally the C is connected to B.\nA\n→\n[\nB\n]\nB\n→\n[\nA\n,\nC\n]\nC\n→\n[\nB\n]\nThis structure is memory-efficient and scalable for large, sparse graphs-like the module graph used in a bundler such as Rolldown. It also makes it easier to traverse only the relevant connections, which is useful during scanning or optimization phases.\nForward & reverse dependency graph\nIn the scan stage, Rolldown builds two types of dependency graphs:\nforward\nand\nreverse\n. The forward graph is stored in the\necma_view\nof each module and records the modules imported by the current one.\npub\nstruct\necma_view\n{\npub\nimport_records\n:\nIndexVec\n<\nImportRecordIdx\n,\nResolvedImportRecord\n>\n,\n// ...\n}\nThe forward dependency graph is essential for bundling. The module loader starts from the user-defined entry points and builds this graph to determine which modules need to be included in the final bundle. It also plays a key role in determine execution order and managing variable scopes.\nIn addition, the module loader creates a\nreverse dependency graph\n, which makes it easy to track which modules import a given module. This critical for features like tree shaking, side-effect analysis, incremental builds, chunking, and code splitting.\nThese features involve a lot of context, so we won’t dive into them here. But you can think of it like this:\nIf I (module) change, who will be effected?\nThe modules that depend on the changed module need to be reprocessed. This is the core idea behind implementing incremental builds or hot module replacement (HMR).\nPerformance optimization\nRolldown includes a number of performance optimizations under the hood. Thanks to Rust’s zero-cost abstractions and ownership model — combined with Tokio’s powerful async runtime — developers have the tools they need to push performance to the next level. The module loader itself employs several techniques to speed things up, which we’ll talk through briefly here. Most of them should be discussed before.\nAsync concurrency processing\nConcurrency is at the heart of the module loader. As mentioned earlier, its main responsibility is to traverse all modules and build the dependency graph. In real-world projects, import relationships can quickly become complex and deeply nested — which makes asynchronous concurrency critical.\nIn Rust,\nasync\nand\nawait\nare the fundamental building blocks of async functions. An\nasync\nfunction returns a\nFuture\n, which won’t execute immediately — it runs only when explicitly awaited. Rolldown relies on Tokio, Rust’s most widely used async runtime, to execute these module processing task concurrently and efficiently.\nCache\nGiven that Rolldown performs many async operations — and often runs repeatedly in local development environments — caching becomes essential to avoid redundant work.\nThe module loader’s cache is stored within the\nModuleLoader\nstruct. It includes data like\nsnapshot\n,\nmodule_id_to_idx\n, and\nimporters\n, most of which we introduced in earlier sections. These caches help Rolldown avoid reprocessing the same modules and make incremental builds significantly faster.\nWhat’s next\nRolldown is still under active development. In the future, it’s expected to become the underlying engine of Vite, offering consistent builds and ultra-fast performance. You can check out the roadmap\nhere\n.\nI wrote this piece to capture my journey with Rolldown, and I hope it also demystifies some of its most impressive inner workings for you. If you spot an error or think something’s missing, drop a comment below—I’d love to hear your feedback! 😊\nThanks for reading, and see you in the next post!\ncd .."
    },
    {
        "title": "Ministry of Environment, Tokyo, Japan",
        "url": "https://artsandculture.google.com/partner/ministry-of-environment",
        "date": "2025-04-16T12:08:13.631937",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Home\nExplore\nPlay\nNearby\nFavorites\nSign in\nLoading…\nMinistry of Environment\nTokyo, Japan\nNational parks aim to protect Japan's exceptional natural sites and preserve them for future generations so the latter can experience these with the same sense of wonder and joy as our generation.\nTo achieve these objectives, national parks are designated, protected and managed by the government under the Natural Parks Act.\nShow less\nRead more\nFollow\nStories\nThe Collection\nView all 558\nAso\nEbino\nJapan\nKirishima\nTowada\nNikko\nAso\nEbino\nJapan\nKirishima\nStories\nView all 8\nOnline Exhibit\nA Land of Volcanic Diversity\nA trip to look at the rugged, igneous landscape of Kirishima Kinkowan National Park, and the influence it has had on the culture of the surrounding area.\nRead the story\nOnline Exhibit\nAso Volcano and Grasslands Are Sustained by People\nA fascinating combination of volcanoes and grasslands, we visit Aso Kuju National Park, to learn more about how subterranean forces have shaped the culture and history of the Aso region of Japan.\nRead the story\nOnline Exhibit\nEncounter Wild Nature of Towada-Hachimantai\nSurrounded by the Bounty of the Great Outdoors among greenery and volcanoes\nRead the story\nOnline Exhibit\nDiving into the ‘Kerama Blue’\nThe rich natural world of the Keramashoto National Park\nRead the story\nOnline Exhibit\nNikko: Where History and Nature combines\nVisiting Nikko National Park is a great way to encounter stunning Japanese scenery that combines breathtaking natural beauty with ancient architecture.\nRead the story\nStay in touch\nFollow Ministry of Environment on Google Arts & Culture for updates to the collection, new stories and upcoming events.\nFollow\nMinistry of Environment's website\nVisit\nMinistry of Environment\n1-2-2 Kasumigaseki Chiyoda-ku Tokyo\nTokyo, 100-8975\nJapan\nTranslate with Google\nGoogle apps"
    },
    {
        "title": "Creators of DeFi Firm Aave Launch Social Media Developer Network Lens Chain",
        "url": "https://www.coindesk.com/business/2025/04/04/creators-of-defi-firm-aave-launch-social-media-developer-network-lens-chain",
        "date": "2025-04-16T12:08:13.632936",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Consensus 2025\nConsensus 2025\nPrices Increase This Friday\n02:13:19:51\n02\nDAY\n13\nHOUR\n19\nMIN\n51\nSEC\nRegister Now\nFinance\nShare\nShare this article\nCopy link\nX icon\nX (Twitter)\nLinkedIn\nFacebook\nEmail\nCreators of DeFi Firm Aave Launch Social Media Developer Network Lens Chain\nLens Chain mainnet goes live using an inexpensive Ethereum overlay blockchain designed for building decentralized social media applications.\nBy\nIan Allison\n|\nEdited by\nStephen Alpher\nUpdated Apr 4, 2025, 7:58 p.m.\nPublished Apr 4, 2025, 6:30 p.m.\nStani Kulechov, founder and CEO of Aave, speaks at Consensus 2019.\nWhat to know:\nThis developer-friendly layer 2 environment is propelled by composable features designed to unleash the economic potential of decentralized “SocialFi.”\nLens has cemented partnerships with several DeFi and infrastructure projects including Uniswap, Balancer, LayerZero, Circle, Consensys and Chainlink.\nAvara, the parent company of decentralized finance (DeFi) platform Aave, has announced the arrival of Lens Chain mainnet, a fast and inexpensive Ethereum overlay blockchain for developing decentralized social media applications.\nThere are now several blockchain-oriented, or “\nWeb3\n,” startups looking to provide users with an alternative to the giant, centralized social media companies like Facebook and Elon Musk's X (formerly Twitter).\nCan Bitcoin Save America?: CoinDesk Spotlight with Senator Cynthia Lummis\nMore Videos\n0 seconds of 38 minutes, 20 seconds\nVolume 0%\nPress shift question mark to access a list of keyboard shortcuts\nKeyboard Shortcuts\nEnabled\nDisabled\nShortcuts Open/Close\n/ or ?\nPlay/Pause\nSPACE\nIncrease Volume\n↑\nDecrease Volume\n↓\nSeek Forward\n→\nSeek Backward\n←\nCaptions On/Off\nc\nFullscreen/Exit Fullscreen\nf\nMute/Unmute\nm\nDecrease Caption Size\n-\nIncrease Caption Size\n+ or =\nSeek %\n0-9\nNext Up\nTHE MINING POD: The Rise and Fall of Bitcoin Mining in Kazakhstan w/ Didar Bekbau\n34:24\nLive\n00:05\n38:15\n38:20\nStory continues\nDon't miss another story.\nSubscribe to the Crypto Long & Short Newsletter today.\nSee all newsletters\nSign me up\nBy signing up, you will receive emails about CoinDesk products and you agree to our\nterms of use\nand\nprivacy policy\n.\nTo offer better alternatives to existing social media giants requires a fertile protocol level, according to Avara CEO Stani Kulechov. This means offering a developer-friendly layer 2 environment, propelled by composable features designed to unleash the economic potential of decentralized “SocialFi.”\n“The direction we have taken with Lens in the past 12 months is to bring the best developer tooling for building on chain social experiences,” Kulechov said in an interview. “We chose the optimal stack to run Lens Chain where we get the lowest possible transaction in cost, but a sufficient amount of security for these social transactions.”\nLens Chain, a system that uses mathematical proofs to check the veracity of batched off-chain transactions, has its own dedicated stablecoin, Aave’s\nGHO\n, to handle gas fees, and its own decentralized data storage. A range of application building blocks are on offer to create things like composable social graphs, custom feeds, token-gated communities etc.\nThe blockchain has cemented partnerships with several DeFi and infrastructure projects including Uniswap, Balancer, LayerZero, Circle, Consensys and Chainlink.\nAave\nIan Allison\nIan Allison is a senior reporter at CoinDesk, focused on institutional and enterprise adoption of cryptocurrency and blockchain technology. Prior to that, he covered fintech for the International Business Times in London and Newsweek online. He won the State Street Data and Innovation journalist of the year award in 2017, and was runner up the following year. He also earned CoinDesk an honourable mention in the 2020 SABEW Best in Business awards. His November 2022 FTX scoop, which brought down the exchange and its boss Sam Bankman-Fried, won a Polk award, Loeb award and New York Press Club award. Ian graduated from the University of Edinburgh. He holds ETH.\nX icon"
    },
    {
        "title": "I tried the $1,500 Luba robot mower from Mammotion, it was not good [video]",
        "url": "https://www.youtube.com/watch?v=bmvNV7GPE70",
        "date": "2025-04-16T12:08:13.632936",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Luba AWD Robot Mower Review (July 2024) - Personal Insights & Experience\nSearch\nWatch later\nShare\nCopy link\nInfo\nShopping\nTap to unmute\n2x\nIf playback doesn't begin shortly, try restarting your device.\n•\nYou're signed out\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\nCancel\nConfirm\nUp next\nLive\nUpcoming\nCancel\nPlay Now\nShare\nInclude playlist\nAn error occurred while retrieving sharing information. Please try again later.\n0:00\n0:00\n/\n15:32\n•\nWatch full video\nLive\n•\n•\nScroll for details\nAbout\nPress\nCopyright\nContact us\nCreators\nAdvertise\nDevelopers\nTerms\nPrivacy\nPolicy & Safety\nHow YouTube works\nTest new features\n© 2025 Google LLC"
    },
    {
        "title": "HybridRAG: Integrating Knowledge Graphs and Vector RAG",
        "url": "https://arxiv.org/abs/2408.04948",
        "date": "2025-04-16T12:08:13.632936",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Computer Science > Computation and Language\narXiv:2408.04948\n(cs)\n[Submitted on 9 Aug 2024]\nTitle:\nHybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\nAuthors:\nBhaskarjit Sarmah\n,\nBenika Hall\n,\nRohan Rao\n,\nSunil Patel\n,\nStefano Pasquali\n,\nDhagash Mehta\nView a PDF of the paper titled HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction, by Bhaskarjit Sarmah and 5 other authors\nView PDF\nHTML (experimental)\nAbstract:\nExtraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain\nComments:\n9 pages, 2 figures, 5 tables\nSubjects:\nComputation and Language (cs.CL)\n; Machine Learning (cs.LG); Statistical Finance (q-fin.ST); Applications (stat.AP); Machine Learning (stat.ML)\nCite as:\narXiv:2408.04948\n[cs.CL]\n(or\narXiv:2408.04948v1\n[cs.CL]\nfor this version)\nhttps://doi.org/10.48550/arXiv.2408.04948\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Dhagash Mehta [\nview email\n]\n[v1]\nFri, 9 Aug 2024 09:07:48 UTC (58 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction, by Bhaskarjit Sarmah and 5 other authors\nView PDF\nHTML (experimental)\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.CL\n< prev\n|\nnext >\nnew\n|\nrecent\n|\n2024-08\nChange to browse by:\ncs\ncs.LG\nq-fin\nq-fin.ST\nstat\nstat.AP\nstat.ML\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\na\nexport BibTeX citation\nLoading...\nBibTeX formatted citation\n×\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer\n(\nWhat is the Explorer?\n)\nConnected Papers Toggle\nConnected Papers\n(\nWhat is Connected Papers?\n)\nLitmaps Toggle\nLitmaps\n(\nWhat is Litmaps?\n)\nscite.ai Toggle\nscite Smart Citations\n(\nWhat are Smart Citations?\n)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv\n(\nWhat is alphaXiv?\n)\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers\n(\nWhat is CatalyzeX?\n)\nDagsHub Toggle\nDagsHub\n(\nWhat is DagsHub?\n)\nGotitPub Toggle\nGotit.pub\n(\nWhat is GotitPub?\n)\nHuggingface Toggle\nHugging Face\n(\nWhat is Huggingface?\n)\nLinks to Code Toggle\nPapers with Code\n(\nWhat is Papers with Code?\n)\nScienceCast Toggle\nScienceCast\n(\nWhat is ScienceCast?\n)\nDemos\nDemos\nReplicate Toggle\nReplicate\n(\nWhat is Replicate?\n)\nSpaces Toggle\nHugging Face Spaces\n(\nWhat is Spaces?\n)\nSpaces Toggle\nTXYZ.AI\n(\nWhat is TXYZ.AI?\n)\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)"
    },
    {
        "title": "Go-away (another HTTP proxy for LLM scraper defence)",
        "url": "https://git.gammaspectra.live/git/go-away#why",
        "date": "2025-04-16T12:08:13.633935",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": ""
    },
    {
        "title": "Show HN: Commit Navigator cnav CLI explain Git commits, generate CHANGELOG",
        "url": "https://github.com/ngduc/cnav",
        "date": "2025-04-16T12:08:13.633935",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "ngduc\n/\ncnav\nPublic\nNotifications\nYou must be signed in to change notification settings\nFork\n0\nStar\n2\ncnav CLI - Commit Navigator\nLicense\nMIT license\n2\nstars\n0\nforks\nBranches\nTags\nActivity\nStar\nNotifications\nYou must be signed in to change notification settings\nngduc/cnav\nmain\n1\nBranch\n0\nTags\nGo to file\nCode\nFolders and files\nName\nName\nLast commit message\nLast commit date\nLatest commit\nngduc\nfix: update version to 0.1.3 and improve git log command syntax\nApr 16, 2025\need6bb2\n·\nApr 16, 2025\nHistory\n7 Commits\n.github\n.github\ninitial commit\nApr 16, 2025\ndocs\ndocs\ninitial commit\nApr 16, 2025\nscripts\nscripts\ninitial commit\nApr 16, 2025\nsrc\nsrc\nfix: update version to 0.1.3 and improve git log command syntax\nApr 16, 2025\ntests\ntests\ninitial commit\nApr 16, 2025\n.eslintrc\n.eslintrc\ninitial commit\nApr 16, 2025\n.gitignore\n.gitignore\ninitial commit\nApr 16, 2025\nCHANGELOG.md\nCHANGELOG.md\ninitial commit\nApr 16, 2025\nCONTRIBUTING.md\nCONTRIBUTING.md\ninitial commit\nApr 16, 2025\nLICENSE\nLICENSE\ninitial commit\nApr 16, 2025\nREADME.md\nREADME.md\nfeat: add Markdown output option for commit analysis\nApr 16, 2025\ncnav-demo.js\ncnav-demo.js\ninitial commit\nApr 16, 2025\ncnav-simple.js\ncnav-simple.js\ninitial commit\nApr 16, 2025\npackage-lock.json\npackage-lock.json\ninitial commit\nApr 16, 2025\npackage.json\npackage.json\nfix: update version to 0.1.3 and improve git log command syntax\nApr 16, 2025\npnpm-lock.yaml\npnpm-lock.yaml\nexcluded lock files; support AnthropicApiKey; limit git diff size;\nApr 16, 2025\ntemplate.env\ntemplate.env\ninitial commit\nApr 16, 2025\ntsconfig.json\ntsconfig.json\ninitial commit\nApr 16, 2025\nvitest.config.d.ts\nvitest.config.d.ts\ninitial commit\nApr 16, 2025\nvitest.config.js\nvitest.config.js\ninitial commit\nApr 16, 2025\nvitest.config.js.map\nvitest.config.js.map\ninitial commit\nApr 16, 2025\nvitest.config.ts\nvitest.config.ts\ninitial commit\nApr 16, 2025\nView all files\nRepository files navigation\n🧭 Commit Navigator (cnav)\nA powerful CLI tool that helps you understand git commit changes using AI. Commit Navigator analyzes your git history and provides intelligent insights about code changes.\n✨ Features\nAnalyze Recent Commits\n: Get a clear summary of what changed and why\nCode Review\n: Automatically review changes for issues, improvements, and best practices\nChangelog Generation\n: Automatically update your CHANGELOG.md with well-written entries\nProject Understanding\n: Help new users or team members understand your project\n🚀 Installation\n#\nInstall globally\nnpm install -g cnav\n#\nOr use with npx\nnpx cnav [command]\n🔧 Requirements\nNode.js 16 or later\nGit repository\nOpenAI API key (get one at\nhttps://platform.openai.com/api-keys\n)\n📋 Usage\n#\nReview the last commit\ncnav last\n#\nReview the last 3 commits\ncnav last 3\n#\nReview commits from the last 7 days\ncnav last --days 7\ncnav last 7d\n#\nshorthand\n#\nCode review on recent changes\ncnav last --review\n#\nUpdate CHANGELOG file with recent changes\ncnav changelog\n📝 Command Reference\ncnav last [count]\nReview the last N commits.\nOptions:\n--review, -r\n: Perform a code review on the commits\n--days, -d <days>\n: Review commits from the last N days\n--md, -m\n: Output analysis in Markdown format\nExamples:\ncnav last\n#\nReview the last commit\ncnav last 5\n#\nReview the last 5 commits\ncnav last -d 3\n#\nReview commits from the last 3 days\ncnav last -r\n#\nCode review the last commit\ncnav last -m\n#\nOutput analysis in Markdown format\ncnav changelog\nUpdate CHANGELOG.md file with latest changes.\nOptions:\n--format <format>\n: Output format (\ndaily\nor\nweekly\n, default:\nweekly\n)\nExamples:\ncnav changelog\n#\nGenerate weekly changelog\ncnav changelog --format daily\n#\nGenerate daily changelog\n🔐 Configuration\nOn first run, you'll be prompted to enter your OpenAI API key. You can also set it up manually:\nCreate a\n.env\nfile in your working directory with:\nOPENAI_API_KEY=your_api_key_here\nOr export as an environment variable:\nexport\nOPENAI_API_KEY=your_api_key_here\n💼 Use Cases\nNew Team Members\n: Quickly get up to speed on project history and recent changes\nCode Reviews\n: Automate first-pass code reviews to catch common issues\nDocumentation\n: Keep your changelog up-to-date automatically\nProject Handovers\n: Document project's architectural patterns and design decisions\nSecurity\n: Identify complex vulnerability patterns and potential issues\nArchitecture\n: Get insights on architectural changes and technical decisions\n🤝 Contributing\nContributions are welcome! Please feel free to submit a Pull Request.\nFork the repository\nCreate your feature branch (\ngit checkout -b feature/amazing-feature\n)\nCommit your changes (\ngit commit -m 'Add some amazing feature'\n)\nPush to the branch (\ngit push origin feature/amazing-feature\n)\nOpen a Pull Request\n📜 License\nThis project is licensed under the MIT License - see the LICENSE file for details.\nAbout\ncnav CLI - Commit Navigator\nResources\nReadme\nLicense\nMIT license\nActivity\nStars\n2\nstars\nWatchers\n1\nwatching\nForks\n0\nforks\nReport repository\nReleases\nNo releases published\nPackages\n0\nNo packages published\nLanguages\nTypeScript\n74.6%\nJavaScript\n23.8%\nShell\n1.6%"
    },
    {
        "title": "A puzzle of two unreliable sensors",
        "url": "https://jacobbrazeal.wordpress.com/2025/04/16/the-puzzle-of-two-unreliable-sensors/",
        "date": "2025-04-16T12:08:13.633935",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Jacob Brazeal\nUncategorized\nApril 16, 2025\nApril 16, 2025\n3 Minutes\nSuppose you are trying to measure a value P and you have two unreliable sensors. Sensor A returns 0.5P + 0.5U, where U is uniform random noise over the same domain as P. Sensor B will return\neither\nP or U with 50% likelihood. In other words, sensor A is a noisy measurement of your variable, and B is sometimes the correct value and sometimes pure noise. If we have the readings of both sensor A and sensor B, what can we say about P?\nLet’s make a graph to give us some intuition. Given 100 samples of P taken uniformly from [0, 1), this shows the relative error of sensor A vs. the relative error of sensor B.\nAs we would expect, errorA is 0 half the time. The rest of the time, it can range from -1 to 1; errorB is rarely exactly 0, but its range is half that of errorA.\nLet’s look at some numerical summary stats for errorA, errorB, and the error of the average of A and B over 100k trials.\nThe mean error of the average of A and B seems a bit lower than the error of either A or B on its own. But wait! The choice to give equal weight to sensor A and sensor B seems a bit arbitrary. Let’s explore the different weights numerically, considering each weighting of A and B in increments of 0.1. Here the x-axis gives us the weight\nw\nwe assign to sensor A, while sensor B is assigned weight 1 –\nw\n. The y-axis shows the average value of the absolute error of this combination over 100k simulations.\nAs you can see, the best weight is not 50-50, but around 0.58, where our mean error drops to around 0.1524. We could theoretically improve the precision of this estimate with a ternary search, but in practice numerical accuracy becomes a problem and I can’t nail it down much tighter than 0.586 or so. (I’m sure there’s some nice technique for this that’s eluding me at the moment.)\nSo that’s interesting. What’s the theoretical grounding for this mixture of variables, and can we do better than a linear mixture? Well, for one thing, it would be really great to know when sensorB is returning the correct value, and when it’s just lying to us. One intuition is, if A and B are pretty close together, then probably that’s because B is not random noise, and we should just return B instead of mixing it with A. But if they’re pretty far apart, then we should ignore B since it’s probably noise and just return A.\nIt turns out this strategy is pretty good, and much better than a linear mixing of the two variables!\nHere the x-axis is the “cutoff” of the absolute difference between A and B. If abs(A – B) is less than X, we use sensor B, else we use sensor A. Here the location of the dip is enough to make anyone suspect good ol’ 1/e ≈ 0.367, but sadly numerical data points clearly to a minimum near 0.41, instead. We can get a mean absolute error around 0.1175, which is much better than the best we could do with a linear mixing of the signals, so maybe we don’t care that much about finding the perfect mixing, anyway.\nWell, that’s a nice improvement, but can we do still better? It’s not really clear that we should switch straight from trusting sensor B to sensor A. Maybe there’s some grey zone in the middle where we should still try to mix the two sensor readings. A numerical grid search with a resolution of 0.01 and 1 million trials in each case says that we do best when\nIf | A – B | < 0.367, yield sensor B’s reading;\nIf | A – B | > 0.445, yield sensor A;\nElse yield a mixture of the two. (I just used the mean again here, I’m sure we could drive it down a bit by searching for the best proportion again.)\nWe can get down to a mean absolute error of 0.1163, markedly better than the single-cutoff case.\nThe discerning reader will note that the first cutoff looks an awful lot like 1/e. (I was sure that it would make  an appearance at some point, somehow.) The second cutoff does not look familiar. One also starts to suspect that the ideal mixing formula might not be linear, even in the middle zone.\nIt’s been a long time since I was a stats student; anyone have an idea how to improve the estimate further?\nShare this:\nLike\nLoading...\nRelated\n6 Months of Working at a Hypergrowth Startup\nFebruary 16, 2022\nWith 1 comment\nThose Computers In Your Head\nJanuary 17, 2022\nMassive Bulk Update\nAugust 4, 2019\nWith 1 comment\nPublished by\nJacob Brazeal\nView all posts by Jacob Brazeal\nPublished\nApril 16, 2025\nApril 16, 2025\nLeave a comment\nCancel reply\nΔ\nApril 2025\nApril 2024\nOctober 2023\nJuly 2023\nSeptember 2022\nFebruary 2022\nJanuary 2022\nNovember 2021\nNovember 2020\nAugust 2019\nAugust 2018"
    },
    {
        "title": "DIY Streaming Video 101",
        "url": "https://scottstuff.net/posts/2025/03/18/video-encoding-101/",
        "date": "2025-04-16T12:08:13.633935",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "DIY Streaming Video 101\nScott Laird\nincluded in\nvideo-encoding\n2025-03-18\nAbout 2200 words\n10 minutes\nSo, you want to stream video on the web without dealing with Youtube\nor other video providers? Where do you even start?\nWelcome to DIY Streaming Video 101.\nI’m slowly working on building the infrastructure here for encoding\nand serving videos myself, and I’m trying to share what I’ve learned.\nI’m doing everything on Linux, mostly using\nFFMPEG\nfor video conversion and\nserving the video myself, from a web server in my house.\nObviously, there are many services that you can pay to do this all\nfor you, and most of them will do a better job at it most of the time,\nfor a fee. But they may not be\nthat\nmuch better, depending on\nyour needs, and video streaming really isn’t that hard anymore.\nAll you need is a bit of time and a website.\nHere’s a sample video, all via the tools discussed here:\nSkip Ad\nreplay\n0:00\nvolume_off\nUnmute\nfullscreen\nmore_vert\nclosed_caption_disabled\nCaptions\nOff\nsettings\nResolution\nAuto\nlanguage\nLanguage\nbookmarks\nChapters\npicture_in_picture_alt\nPicture-in-Picture\nOff\ncast\nCast...\nOff\nslow_motion_video\nPlayback speed\n0x\ncontrol_camera\nRecenter\n3d_rotation\nToggle stereoscopic\ncast\nCast...\ndownload\nSave video frame\narrow_back\nCaptions\nOff\ndone\narrow_back\nResolution\nAuto\ndone\narrow_back\nLanguage\narrow_back\nChapters\narrow_back\nPlayback speed\n0.5x\n0.75x\n1x\n1.25x\n1.5x\n1.75x\n2x\nThis article is kind of a work in progress; I’ll be refining it over\nthe next month or so as I dig deeper into streaming and video\nencoding.\nWhat you need\nin order to stream video yourself\nThere are a few steps to serving your own video.\nYou need to have a video that you want to serve and a website to\nserve it from.\nYou need to encode your video in formats (\nplural\n) that\nbrowsers can play.\nThen you need to package up all of the different video formats along\nwith manifests for\nDASH\nand/or\nHLS\nso\nbrowsers can stream the video while adapting to varying network\nconditions, resolutions, and client capabilities.\nDASH is the modern standard, HLS is Apple’s\nslightly-older alternative. Safari has native support for HLS, which\nChrome and Firefox like DASH. Practically speaking, good Javascript\nvideo players\nshould\nbe able to stream DASH to just about\nanything, but building HLS manifests is usually just one extra flag on\nthe packager’s command line.\nThen you serve the video files and DASH manifest via HTTP. Any\nnormal web server can do this.\nFinally, add the right HTML and Javascript to your web page to\nactually present the video to users.\nThis is just an overview article; I’ll be providing concrete guidance\n(and tools!) in future articles that cover each topic. They’ll all be\nlinked from\nhere\n.\nEncoding\nLet’s start with step 2 above\nif you want to stream a video that you don’t have, then\nthat’s a whole\ndifferent\nproblem.\n: encoding your video.\nUnfortunately, there are currently 3 different\ncodecs\nthat make\nsense on the web:\nH.264\n.\nOlder, relatively hefty files, but works\neverywhere\n.\nMost (but not all) Blu-ray disks use H.264.\nH.265\n.\nNewer. Smaller files, but patent issues mean that neither Chrome nor\nFirefox can play it back on most platforms. Painfully slow to encode.\nUnfortunately, this is the best format supported by most Apple devices\nfrom earlier than late 2023.\n4K Blu-ray disks use H.265, usually.\nAV1\n. Even newer.\nMuch smaller, plays fast, encodes fast. But doesn’t play on\npre-M3/iPhone 15 Pro/iPhone 16 Apple devices or older Windows or Linux\nsystems.\nNetflix and Youtube\nmostly\nstream AV1 these\ndays, at least with devices that support it.\nIf you want to serve the best possible video quality to all devices,\nthen you’re going to need to encode your video multiple times, probably\nin a mix of all three of these codecs.\nYou want multiple resolutions for each codec, so small screens with\nlow bandwidth can play smaller versions, and larger screens with faster\nnetworks can play larger versions. DASH and HLS can handle switching\nresolutions transparently to the user, but you need to actually provide\nmultiple resolutions of your video files for this to work right.\nRight now, I’m encoding my videos at 480p, 720p, 1080p, 1440p, 2160p,\nand sometimes higher, depending on the source material. I never upscale\nwhen encoding videos – there’s no point in streaming a video shot in\n720p in 4k. Even if the source material is high enough resolution, I\ndon’t encode every codec at every resolution. H.264 produces big files;\ntrying to serve 4k videos over H.264 is probably a mistake, because\nalmost everything that supports 4k supports better codecs. So I’m\nbuilding H.264 videos in 480p, 720p, and 1080p. H.265 is a bit smaller,\nso I’m skipping 480p and building 720p, 1080p, 1440p, and 2160p. Then\nAV1 gets built for 720p up to whatever the source material uses.\nPicking encoder settings\nOnce you decide on resolutions and codecs, then you need to figure\nout what encoder settings you’re going to use. I’ll be adding concrete\nguidance here in future articles. For now, have a few rules of\nthumb:\nGenerally, software encoders are higher quality than hardware\nencoders.\nIt’s easy to spend a huge amount of CPU time trying to produce\nbetter encodings without really getting either better quality or smaller\nfiles. Don’t turn the quality dial up\ntoo\nfar.\nTest your results with your own videos.\nSee my\nH.265\nencoder settings article\nwhere I test almost 1,200 different encoder\nsettings to find the ones that work best for me.\nJudging encoder quality\nTo judge encoding quality, I’m using Netflix’s\nVMAF\nas a metric. VMAF isn’t perfect, but it’s probably the best openly\navailable tool for tracking video compression quality today. Rumor has\nit that Netflix tunes their streaming videos for a VMAF value of 95,\nwhich is\nmostly\nindistinguishable from originals.\nFFMPEG can calculate VMAF scores for you with a few flags:\n$ ffmpeg \\\n    -i foo-h264-1080p.mp4 \\\n    -i original-1080p.mp4 \\\n    -lavfi libvmaf=n_threads=4 \\\n    -f null -\nIt’s important to list the compressed video first and the original\nvideo second. The two need to be the same size, or you need to tell\nffmpeg to re-scale the original video on the fly, which is a bit more\ncomplicated.\nTo guide my encoding settings, I’ve been running a\nhuge\nnumber of tests of various\nffmpeg\nencoder flags. I’m\ntracking the encoding time required, the file size, and the VMAF score\nfor each. Then I’m picking the smallest, fastest encoder settings that\ncan give me a VMAF value of 95 for my test video. I don’t want to do\nthis for\nevery\nvideo (my full test suite takes hours to run\nthousands of tests on a 30 second test video), but running it against\nrepresentative videos should give me a good set of default settings.\nGenerally, I’m picking set of flags that work for each coded and then\nvarying the quality level (usually\n-crf\nor\n-cq:v\n, depending on the codec implementation) to get my\ntarget VMAF. Right now, I’m mostly just using the same values for all\nvideos, but I’ll probably automate this (and share my code) at some\npoint.\nIgnore bandwidth\nWhen I first started this, I paid a lot of attention to target\nbandwidth for encoding. I set a maximum bandwidth per video size, so say\na 720p H.265 video was limited to 1.5 Mbps. This seems to be a fairly\ncommon pattern online.\nI wasn’t really happy with the results, though. Unless you (and your\naudience) are\nseverely\nbandwidth-constrained, you’ll probably\nbe better off just picking a set of resolutions and encoding videos as\nefficiently as possible for those resolutions. Players will fall back to\nlower resolutions automatically if there isn’t enough bandwidth. You’re\nbetter off serving well-rendered videos and falling back to lower\nresolutions when bandwidth-constrained rather than serving chunky,\npoorly encoded higher resolution video.\nPackaging videos for DASH and\nHLS\nSo, you’ve encoded your original video into a bunch of different\nfiles with different codecs and different resolutions. Now it’s time to\npackage those all up with a\nDASH\nmanifest that will tell video players which resolutions and codecs are\navailable.\nSo far, I’ve experimented with two DASH packagers:\nShaka\npackager\nfrom Google\nDisclaimer, I used to work at Google and worked with\nmost of the people who worked on Shaka Player and Packager, although I\nnever touched it directly.\nand\nffmpeg’s own\npackager\n. Initially, I’d had ffmpeg generate DASH configs on its\nown, but I wasn’t very happy with the results. So I’ve moved to using\nthe Shaka packager instead.\nUnlike encoding, packaging is pretty straightforward. You feed the\npackager a list of audio, video, and subtitle\nIn theory at least, I haven’t tried subtitles myself,\nand I rarely have useful audio.\nfiles, and it extracts some metadata from them and builds\nan XML manifest file.\nThe big problem that I had with\nffmpeg\n’s packager is\nthat it didn’t annotate resolutions and bitrates in a way that Shaka\nPlayer’s javascript liked. Sometimes it’d stream the wrong quality\nvideo, and sometimes the player would stall entirely due to errors in\nthe encoding. Plus, the way I was using it, I couldn’t really re-encode\na single codec or resolution without problems. Shaka Packager seems to\nhave fixed these problems.\nYou can download the Shaka packager from their GitHub page. They\nprovide examples, but you’ll probably end up running it roughly like\nthis:\n$ packager \\\n    in=foo_h264_480p.mp4,stream=video,output=web/video_h264_480p \\\n    in=foo_h264_720p.mp4,stream=video,output=web/video_h264_720p \\\n    in=foo_h265_720p.mp4,stream=video,output=web/video_h265_720p \\\n    in=foo_audio.mp4,stream=audio,output=web/audio.mp4 \\\n    --allow_codec_switching \\\n    --mpd_output=web/dash.mpd \\\n    --hls_master_playlist_output=web/hls.m3u8\nThe output in\nweb/\nwill contain everything that you need\nto stream.\nServing DASH via HTTP\nYour packager will spit out a DASH\n.mpd\nfile and maybe a\nbunch of HLS\n.m3u8\nfiles, plus all of the video files that\nyou have. You should be able to serve these directly via any normal web\nserver. You don’t need any special video-aware settings,\nexcept\nthat your web server needs to be able to support HTTP range requests.\nThat’s been a standard part of the web for almost 30 years, since\nHTTP/1.1. Any “big” web server (Apache, nginx, Caddy, etc) should have\nrange request support, but some smaller “development” servers,\nlike the one built\ninto Python\nmay not.\nThe hard part of video serving is really just the amount of storage\nand bandwidth needed. Other than that, it’s no different than static\nHTML files.\nPlaying the videos\nHTML 5 added a\n<video>\ntag that can stream and\nplay videos in a browser-independent way, but it’s rarely enough to give\na good user experience by itself. For instance, the\n<video>\ntag doesn’t support changing video\nresolutions in response to changing network conditions out of the box,\nalthough it does provide a Javascript API that can be used for this.\nSo, there are a number of common Javascript addons that extend\n<video>\nto add support for DASH and friends. I’ve had\nthe best luck with Google’s\nShaka Player\n.\nIt’s not quite\ntrivial\nto use, but you can cut-and-paste\nworking configs without a lot of trouble and then extend them if you\nwant to customize things.\nTo use it, I add this to the\n<head>\nof my page to\nmake sure that the Javascript loads:\n<\nscript\nsrc\n=\n\"https://cdn.jsdelivr.net/npm/shaka-player@4.13.8/dist/shaka-player.ui.debug.js\"\n></\nscript\n>\n<\nlink\nrel\n=\n\"stylesheet\"\ntdelivr.netpe\n=\n\"text/css\"\nhref\n=\n\"https://cdn.jsdelivr.net/npm/shaka-player@4.13.8/dist/controls.css\"\n/>\nAnd then add something like this to add a video. Notice that the\nmanifest\nvariable is set to the URL of my DASH\n.mpd\nfile.\nThis example isn’t the greatest if you want to have\nmore than one video on a page; you need to assign a unique ID to each\n<video>\nelement and give the Javascript function a\nunique name. If you just have one, though, then you should be able to\npaste it right in and edit the URL.\nAlso, note that these\nparameters probably aren’t the absolute\nbest\n. For instance, the\nbuffering values should probably take into account the segment sizes\nthat your DASH packager uses and the GOP size that your encoders\nuse.\n<\ndiv\nclass\n=\n\"video-wrapper full-width\"\n>\n<\ndiv\ndata-shaka-player-container style\n=\n\"align=center;\"\ndata-shaka-player-cast-receiver-id\n=\n\"07AEE832\"\n>\n<\nvideo\nautoplay data-shaka-player id\n=\n\"foo\"\nstyle\n=\n\"width:100%;height:100%\"\nmuted\n>\n</\nvideo\n>\n</\ndiv\n>\n</\ndiv\n>\n<\nscript\n>\nasync\nfunction\nshakainit_foo\n() {\nvar\nbase_url\n=\nwindow\n.\nlocation\n.\norigin\n;\nconst\nvideo\n=\ndocument\n.\ngetElementById\n(\n'foo'\n)\n;\nconst\nui\n=\nvideo[\n'ui'\n]\n;\nconst\ncontrols\n=\nui\n.\ngetControls\n()\n;\nconst\nplayer\n=\ncontrols\n.\ngetPlayer\n()\n;\nplayer\n.\nconfigure\n({\nabr\n:\n{\nrestrictToElementSize\n:\ntrue\n,\nbandwidthUpgradeTarget\n:\n0.85\n,\nbandwidthDowngradeTarget\n:\n0.95\n,\n}\n,\nstreaming\n:\n{\nbufferingGoal\n:\n4\n,\nrebufferingGoal\n:\n2\n,\n}\n,\n})\n;\nvar\nmanifest\n=\n\"https://scottstuff.net/webvideo/foo/dash.mpd\"\n;\nawait\nplayer\n.\nload\n(manifest)\n;\n}\ndocument\n.\naddEventListener\n(\n'shaka-ui-loaded'\n,\nshakainit_foo)\n;\n</\nscript\n>\nThis site is built using\nHugo\n, and I\nhave all of the video HTML and Javascript wrapped up in a\nHugo\nshortcode\n. I just need to say something like\n{{< video \"foo\" >}}\nto add a video player for a video\ncalled\nfoo\n.\nTesting\nAt this point, you should be able to test everything. Load your\nwebpage and keep an eye on the Javascript console. The snippet above\nuses the debug version of the Shaka Player, so it’ll log details on\nstreaming success or failure. Also watch your browser’s network request\ninspector and your webserver’s logs if you have problems.\nUnfortunately, every major platform is a little bit different, so\nyou’ll probably want to test on as many different devices as possible,\nat least for now. Hopefully everything will converge on AV1 support in a\nfew years, before we all go chasing after the next new codec.\nUpdated on 2025-03-18\nCC BY-NC-SA 4.0\nvideo\nBack\n|\nHome\nBenchmarking FFMPEG's H.265 Options\nClient-Side HTTP Metrics and Errors With Vector and Clickhouse"
    },
    {
        "title": "Show HN: Clicknow – Unlimited LLM Access for Less Than $20",
        "url": "https://clicknow.ai/blog/pay-once-unlimited-llm-access",
        "date": "2025-04-16T12:08:13.634925",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Back to clicknow.ai\nClicknow Blog\nClicknow: Unlimited LLM Access for Less Than $20\nClicknow's most unique feature that you might not believe\nlaike9m\nApril 15th\nChatGPT changes everything, not just technology, but also business.\nBefore ChatGPT, people would think carefully before subscribing to a $20/month service. Now in 2025, $20/month has become an industry standard for LLM services, and we seem to take it for granted. What's even worse? Many LLM wrappers, despite requiring customers to bring their own API keys, still charge them $5 ~ $20 dollars a month. To me, this is unacceptable.\nIn the good old days, most software used the pay-once use forever model. They didn't have subscriptions, because it didn't make sense.\nClicknow\nshares the same idea.\nWe believe one-time payment works best for customers, and it works best for our business\n.\nTo take it one step further,\nClicknow is one of the few LLM tools that don't require an API key. It has its own.\nAfter installing Clicknow, you can start using it right away. This is particularly helpful for people who have trouble paying for ChatGPT/Claude/etc, due to not having supported payment methods, or who live in regions that these services don't support.\nIn case you wonder if this model is sustainable, I once\nshared\nthe billing with everyone: Clicknow's API fee is less than $1 per month, since Gemini flash models are surprisingly cheap. So yes, it is sustainable, and I'm happy we can do it this way to make our users' life easier.\nShare\nTwitter\nFacebook\nLinkedin\nClicknow Blog\nRecent posts\nTwo-way translation（中英文互译）\nClicknow auto picks the best target translation language. For text in your native language, it's translated to your preferred 2nd language (e.g. English)\nCustom models (OpenAI, Claude, Deepseek)  are now supported 🎉\nStarting today, you can specify the preferred model to use (with your own API key) in Clicknow\nPopClip Clicknow extension\nYou can open Clicknow from PopClip with one click\nHome\nRSS feed\n© laike9m, 2025.\n×"
    },
    {
        "title": "OpenAI debuts new 'Image Library' section in ChatGPT for AI-generated images",
        "url": "https://www.theverge.com/news/649247/chatgpt-image-library",
        "date": "2025-04-16T12:08:13.634925",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "News\nChatGPT now has a section for your AI-generated images\nThe new Library is rolling out now to all Free, Plus, and Pro users.\nThe new Library is rolling out now to all Free, Plus, and Pro users.\nby\nJay Peters\nApr 16, 2025, 3:12 AM GMT+5:30\nLink\nFacebook\nThreads\n2\nComments\n/\n2\nNew\nImage: Cath Virginia / The Verge\nJay Peters\nis a news editor covering technology, gaming, and more. He joined The Verge in 2019 after nearly two years at Techmeme.\nOpenAI is adding an image library to ChatGPT to make it easier to access your AI-generated images, the company announced today. It’s rolling out to all Free, Plus, and Pro users on mobile and on the web.\nIn\na short video\n, OpenAI shows how it works. From the ChatGPT sidebar, you’ll be able to see a new “Library” section. Tap into it and you can see a grid of images that you’ve created. The video also briefly shows a button hovering at the bottom of the screen to make a new image.\nI already have the library available in the ChatGPT iOS app, and it works like OpenAI’s video shows. I don’t seem to have it yet on the web, but I would guess it will roll out there in a matter of time.\nThe feature seems like it could be useful if you use ChatGPT to make a lot of images. Or if you just want to look back on your\nStudio Ghibli-inspired art\nor\nyour really dull dolls\n.\n2\nComments\n/\n2\nNew\nSee More\n:\nAI\nNews\nOpenAI\nTech\nMore in this stream\nSee all\nAnthropic launches research tool and Google Workspace integration\nKylie Robison\nApr 15\nComments\nComment Icon Bubble\n1\nOpenAI is building a social network\nKylie Robison\nand\nAlex Heath\nApr 15\nComments\nComment Icon Bubble\n62\nMeta AI will soon train on EU users’ data\nWes Davis\nApr 14\nComments\nComment Icon Bubble\n8\nMost Popular\nMost Popular\nOpenAI is building a social network\n4chan’s ‘cesspool of the internet’ is down after apparently being hacked\nOpenAI debuts its GPT-4.1 flagship AI model\nComcast announces a five-year price lock for Xfinity internet plans\nAndroid phones will soon reboot if they’re locked for a few days\nInstaller\nA weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.\nEmail (required)\nSign Up\nBy submitting your email, you agree to our\nTerms\nand\nPrivacy Notice\n.\nThis site is protected by reCAPTCHA and the Google\nPrivacy Policy\nand\nTerms of Service\napply.\nAdvertiser Content From\nThis is the title for the native ad\n1/1\nSkip\nAd\nContinue watching\nafter the ad\nVisit Advertiser website\nGO TO PAGE\nMore in\nNews\nThe CVE program for tracking security flaws is about to lose federal funding\nMeta’s antitrust trial slide redactions aren’t actually hiding anything\nMark Zuckerberg once suggested wiping all Facebook friends lists to boost usage\nBlackmagic Design says tariffs have made camera price hikes ‘unavoidable’\nAnthropic is reportedly launching a voice AI you can speak to\nGoogle Search is going to be google.com globally\nThe CVE program for tracking security flaws is about to lose federal funding\nEmma Roth\n2:11 AM GMT+5:30\nComments\nComment Icon Bubble\n11\nMeta’s antitrust trial slide redactions aren’t actually hiding anything\nWes Davis\n1:39 AM GMT+5:30\nComments\nComment Icon Bubble\n9\nMark Zuckerberg once suggested wiping all Facebook friends lists to boost usage\nWes Davis\n1:12 AM GMT+5:30\nComments\nComment Icon Bubble\n10\nBlackmagic Design says tariffs have made camera price hikes ‘unavoidable’\nJess Weatherbed\n1:05 AM GMT+5:30\nComments\nComment Icon Bubble\n3\nAnthropic is reportedly launching a voice AI you can speak to\nEmma Roth\n12:16 AM GMT+5:30\nComments\nComment Icon Bubble\n5\nGoogle Search is going to be google.com globally\nJay Peters\nApr 15\nComments\nComment Icon Bubble\nAdvertiser Content From\nThis is the title for the native ad\nTop Stories\n6:22 AM GMT+5:30\nMark Zuckerberg defends his empire\nApr 15\nOpenAI is building a social network\n4:18 AM GMT+5:30\nThe student arrested at his naturalization interview knew it was coming\nApr 15\nWhy Doctor Who’s latest robot threat is a dangerous AI: ‘this is what’s happening’\n12:08 AM GMT+5:30\nOn TikTok, Chinese factories are trolling anxious American shoppers\n2:11 AM GMT+5:30\nThe CVE program for tracking security flaws is about to lose federal funding"
    },
    {
        "title": "How to Build an Agent",
        "url": "https://ampcode.com/how-to-build-an-agent",
        "date": "2025-04-16T12:08:13.635435",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Amp\nPodcast\nHow to Build an Agent\nor: The Emperor Has No Clothes\nThorsten Ball, April 15, 2025\nIt’s not that hard to build a fully functioning, code-editing agent.\nIt seems like it would be. When you look at an agent editing files, running commands, wriggling itself out of errors, retrying different strategies - it seems like there has to be a secret behind it.\nThere isn’t. It’s an LLM, a loop, and enough tokens. It’s what we’ve been saying on the\npodcast\nfrom the start. The rest, the stuff that makes Amp so addictive and impressive? Elbow grease.\nBut building a small and yet highly impressive agent doesn’t even require that. You can do it in less than 400 lines of code, most of which is boilerplate.\nI’m going to show you how, right now. We’re going to write some code together and go from zero lines of code to “oh wow, this is… a game changer.”\nI\nurge\nyou to follow along. No, really. You might think you can just read this and that you don’t have to type out the code, but it’s less than 400 lines of code. I need you to\nfeel\nhow little code it is and I want you to see this with your own eyes in your own terminal in your own folders.\nHere’s what we need:\nGo\nAnthropic API key\nthat you set as an environment variable,\nANTHROPIC_API_KEY\nPencils out!\nLet’s dive right in and get ourselves a new Go project set up in four easy commands:\nmkdir\ncode-editing-agent\ncd\ncode-editing-agent\ngo mod init agent\ntouch\nmain.go\nNow, let’s open\nmain.go\nand, as a first step, put a skeleton of things we need in it:\npackage\nmain\nimport\n(\n\"bufio\"\n\"context\"\n\"fmt\"\n\"os\"\n\"github.com/anthropics/anthropic-sdk-go\"\n)\nfunc\nmain\n(\n)\n{\nclient\n:=\nanthropic\n.\nNewClient\n(\n)\nscanner\n:=\nbufio\n.\nNewScanner\n(\nos\n.\nStdin\n)\ngetUserMessage\n:=\nfunc\n(\n)\n(\nstring\n,\nbool\n)\n{\nif\n!\nscanner\n.\nScan\n(\n)\n{\nreturn\n\"\"\n,\nfalse\n}\nreturn\nscanner\n.\nText\n(\n)\n,\ntrue\n}\nagent\n:=\nNewAgent\n(\n&\nclient\n,\ngetUserMessage\n)\nerr\n:=\nagent\n.\nRun\n(\ncontext\n.\nTODO\n(\n)\n)\nif\nerr\n!=\nnil\n{\nfmt\n.\nPrintf\n(\n\"Error: %s\\n\"\n,\nerr\n.\nError\n(\n)\n)\n}\n}\nfunc\nNewAgent\n(\nclient\n*\nanthropic\n.\nClient\n,\ngetUserMessage\nfunc\n(\n)\n(\nstring\n,\nbool\n)\n)\n*\nAgent\n{\nreturn\n&\nAgent\n{\nclient\n:\nclient\n,\ngetUserMessage\n:\ngetUserMessage\n,\n}\n}\ntype\nAgent\nstruct\n{\nclient\n*\nanthropic\n.\nClient\n\tgetUserMessage\nfunc\n(\n)\n(\nstring\n,\nbool\n)\n}\nYes, this doesn’t compile yet.  But what we have here is an\nAgent\nthat has access to an\nanthropic.Client\n(which, by default, looks for\nANTHROPIC_API_KEY\n) and that can get a user message by reading from stdin on the terminal.\nNow let’s add the missing\nRun()\nmethod:\n// main.go\nfunc\n(\na\n*\nAgent\n)\nRun\n(\nctx context\n.\nContext\n)\nerror\n{\nconversation\n:=\n[\n]\nanthropic\n.\nMessageParam\n{\n}\nfmt\n.\nPrintln\n(\n\"Chat with Claude (use 'ctrl-c' to quit)\"\n)\nfor\n{\nfmt\n.\nPrint\n(\n\"\\u001b[94mYou\\u001b[0m: \"\n)\nuserInput\n,\nok\n:=\na\n.\ngetUserMessage\n(\n)\nif\n!\nok\n{\nbreak\n}\nuserMessage\n:=\nanthropic\n.\nNewUserMessage\n(\nanthropic\n.\nNewTextBlock\n(\nuserInput\n)\n)\nconversation\n=\nappend\n(\nconversation\n,\nuserMessage\n)\nmessage\n,\nerr\n:=\na\n.\nrunInference\n(\nctx\n,\nconversation\n)\nif\nerr\n!=\nnil\n{\nreturn\nerr\n}\nconversation\n=\nappend\n(\nconversation\n,\nmessage\n.\nToParam\n(\n)\n)\nfor\n_\n,\ncontent\n:=\nrange\nmessage\n.\nContent\n{\nswitch\ncontent\n.\nType\n{\ncase\n\"text\"\n:\nfmt\n.\nPrintf\n(\n\"\\u001b[93mClaude\\u001b[0m: %s\\n\"\n,\ncontent\n.\nText\n)\n}\n}\n}\nreturn\nnil\n}\nfunc\n(\na\n*\nAgent\n)\nrunInference\n(\nctx context\n.\nContext\n,\nconversation\n[\n]\nanthropic\n.\nMessageParam\n)\n(\n*\nanthropic\n.\nMessage\n,\nerror\n)\n{\nmessage\n,\nerr\n:=\na\n.\nclient\n.\nMessages\n.\nNew\n(\nctx\n,\nanthropic\n.\nMessageNewParams\n{\nModel\n:\nanthropic\n.\nModelClaude3_7SonnetLatest\n,\nMaxTokens\n:\nint64\n(\n1024\n)\n,\nMessages\n:\nconversation\n,\n}\n)\nreturn\nmessage\n,\nerr\n}\nIt’s not a lot, is it? 90 lines and the most important thing about is this loop in\nRun()\nthat lets us talk to Claude. But that’s already the heartbeat of this program.\nAnd for a heartbeat, it’s pretty straightforward: we first print a prompt, ask the user to type something, add it to the conversation, send it to Claude, add Claude’s response to the conversation, print the response, and around we go.\nThis is every AI chat application you’ve ever used, except it’s in the terminal.\nLet’s run it:\nexport\nANTHROPIC_API_KEY\n=\n\"this is the last time i will tell you to set this\"\n# Download the dependencies\ngo mod tidy\n# Run it\ngo run main.go\nThen you can just talk to Claude, like this:\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: Hey! I'm Thorsten! How are you?\nClaude\n: Hi Thorsten! I'm doing well, thanks for asking. It's nice to meet you. How are you doing today? Is there something I can help you with?\nYou\n: Can you come up with any horse-related nicknames that make fun of my first name?\nClaude\n: I can try to come up with some playful horse-related nicknames based on \"Thorsten\":\n\n* Thorough-bred Thorsten\n* Trotsten\n* Thorsten the Stallion\n* Neighsten\n* Horseten\n* Thorsten the Galloper\n* Saddle-sten\n* Thorsten of the Mane\n* Canter-sten\n* Thorsteed\n\nThese are meant to be lighthearted - I hope they give you a chuckle!\nYou\n:\nNotice how we kept the same conversation going over multiple turns. It remembers my name from the first message. The\nconversation\ngrows longer with every turn and we send the whole conversation every time. The server — Anthropic’s server — is stateless. It only sees what’s in the\nconversation\nslice. It’s up to us to maintain that.\nOkay, let’s move on, because the nicknames suck and this is not an agent yet. What’s an agent? Here’s\nmy definition\n: an LLM with\naccess to tools\n, giving it the ability to modify something outside the context window.\nA First Tool\nAn LLM with\naccess to tools\n? What’s a tool? The basic idea is this: you send a prompt to the model that says it should reply in a certain way if it wants to use “a tool”. Then you, as the receiver of that message, “use the tool” by executing it and replying with the result. That’s it. Everything else we’ll see is just abstraction on top of it.\nImagine you’re talking to a friend and you tell them: “in the following conversation, wink if you want me to raise my arm”. Weird thing to say, but an easy concept to grasp.\nWe can already try it without changing any of our code.\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: You are a weather expert. When I ask you about the weather in a given location, I want you to reply with `get_weather(<location_name>)`. I will then tell you what the weather in that location is. Understood?\nClaude\n: I understand your instructions. When you ask me about the weather in a specific location, I will reply with `get_weather(<location_name>)`. Then you'll provide me with the weather information for that location, and I can discuss it further with you. I'm ready whenever you'd like to ask about the weather somewhere.\nYou\n: Hey, what's the weather in Munich?\nClaude\n: get_weather(Munich)\nWe told Claude to wink with\nget_weather\nwhen it wants to know about the weather. The next step is to raise our arm and reply with “result of the tool”:\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: hot and humid, 28 degrees celcius\nClaude\n: Thank you for providing that information. The current weather in Munich is hot and humid at 28 degrees Celsius (about 82 degrees Fahrenheit). These are fairly warm conditions for Munich, especially with the added humidity which can make it feel even warmer than the temperature indicates.\n\nIf you're in Munich or planning to visit, it would be good to stay hydrated, wear light clothing, and perhaps plan indoor activities during the hottest parts of the day. The combination of heat and humidity can be quite uncomfortable, particularly if you're not accustomed to such weather.\n\nWould you like to know about the weather in any other location?\nThat worked very well, on first try, didn’t it?\nThese models are trained and fine-tuned to use “tools” and they’re very eager to do so. By now, 2025, they kinda “know” that they don’t know everything and can use tools to get more information. (Of course that’s not precisely what’s going on, but it’s good enough an explanation for now.)\nTo summarize, all there is to tools and tool use are two things:\nYou tell the model what tools are available\nWhen the model wants to execute the tool, it tells you, you execute the tool and send the response up\nTo make (1) easier, the big model providers have built-in APIs to send tool definitions along.\nOkay, now let’s build our first tool:\nread_file\nThe\nread_file\ntool\nIn order to define the\nread_file\ntool, we’re going to use the types that the Anthropic SDK suggests, but keep in mind: under the hood, this will all end up as strings that are sent to the model. It’s all “wink if you want me to use\nread_file\n“.\nEach tool we’re going to add will require the following:\nA name\nA description, that tells the model what the tool does, when to use it, when to not use it, what it returns and so on.\nAn input schema, that describes as a JSON schema, what inputs this tool expects and in which form\nA function that actually executes the tool with the input the model sends to us and returns the result\nSo let’s add that to our code:\n// main.go\ntype\nToolDefinition\nstruct\n{\nName\nstring\n`json:\"name\"`\nDescription\nstring\n`json:\"description\"`\nInputSchema anthropic\n.\nToolInputSchemaParam\n`json:\"input_schema\"`\nFunction\nfunc\n(\ninput json\n.\nRawMessage\n)\n(\nstring\n,\nerror\n)\n}\nNow we give our\nAgent\ntool definitions:\n// main.go\n// `tools` is added here:\ntype\nAgent\nstruct\n{\nclient\n*\nanthropic\n.\nClient\n\tgetUserMessage\nfunc\n(\n)\n(\nstring\n,\nbool\n)\ntools\n[\n]\nToolDefinition\n}\n// And here:\nfunc\nNewAgent\n(\nclient\n*\nanthropic\n.\nClient\n,\ngetUserMessage\nfunc\n(\n)\n(\nstring\n,\nbool\n)\n,\ntools\n[\n]\nToolDefinition\n,\n)\n*\nAgent\n{\nreturn\n&\nAgent\n{\nclient\n:\nclient\n,\ngetUserMessage\n:\ngetUserMessage\n,\ntools\n:\ntools\n,\n}\n}\n// And here:\nfunc\nmain\n(\n)\n{\n// [... previous code ...]\ntools\n:=\n[\n]\nToolDefinition\n{\n}\nagent\n:=\nNewAgent\n(\n&\nclient\n,\ngetUserMessage\n,\ntools\n)\n// [... previous code ...]\n}\nAnd send them along to the model in\nrunInference\n:\n// main.go\nfunc\n(\na\n*\nAgent\n)\nrunInference\n(\nctx context\n.\nContext\n,\nconversation\n[\n]\nanthropic\n.\nMessageParam\n)\n(\n*\nanthropic\n.\nMessage\n,\nerror\n)\n{\nanthropicTools\n:=\n[\n]\nanthropic\n.\nToolUnionParam\n{\n}\nfor\n_\n,\ntool\n:=\nrange\na\n.\ntools\n{\nanthropicTools\n=\nappend\n(\nanthropicTools\n,\nanthropic\n.\nToolUnionParam\n{\nOfTool\n:\n&\nanthropic\n.\nToolParam\n{\nName\n:\ntool\n.\nName\n,\nDescription\n:\nanthropic\n.\nString\n(\ntool\n.\nDescription\n)\n,\nInputSchema\n:\ntool\n.\nInputSchema\n,\n}\n,\n}\n)\n}\nmessage\n,\nerr\n:=\na\n.\nclient\n.\nMessages\n.\nNew\n(\nctx\n,\nanthropic\n.\nMessageNewParams\n{\nModel\n:\nanthropic\n.\nModelClaude3_7SonnetLatest\n,\nMaxTokens\n:\nint64\n(\n1024\n)\n,\nMessages\n:\nconversation\n,\nTools\n:\nanthropicTools\n,\n}\n)\nreturn\nmessage\n,\nerr\n}\nThere’s a bunch of type shenanigans going on and I’m not too good in Go-with-generics yet so I’m not going to try to explain\nanthropic.String\nand\nToolUnionParam\nto you. But, really, I swear, it’s very simple:\nWe send along our tool definitions, on the server Anthropic then wraps these definitions in\nthis system prompt\n(which isn’t much), which it adds to our\nconversation\n, and the model then replies in a specific way if it wants to use that tool.\nAlright, so tool definitions are being sent along, but we haven’t defined a tool yet. Let’s do that and define\nread_file\n:\n// main.go\nvar\nReadFileDefinition\n=\nToolDefinition\n{\nName\n:\n\"read_file\"\n,\nDescription\n:\n\"Read the contents of a given relative file path. Use this when you want to see what's inside a file. Do not use this with directory names.\"\n,\nInputSchema\n:\nReadFileInputSchema\n,\nFunction\n:\nReadFile\n,\n}\ntype\nReadFileInput\nstruct\n{\nPath\nstring\n`json:\"path\" jsonschema_description:\"The relative path of a file in the working directory.\"`\n}\nvar\nReadFileInputSchema\n=\nGenerateSchema\n[\nReadFileInput\n]\n(\n)\nfunc\nReadFile\n(\ninput json\n.\nRawMessage\n)\n(\nstring\n,\nerror\n)\n{\nreadFileInput\n:=\nReadFileInput\n{\n}\nerr\n:=\njson\n.\nUnmarshal\n(\ninput\n,\n&\nreadFileInput\n)\nif\nerr\n!=\nnil\n{\npanic\n(\nerr\n)\n}\ncontent\n,\nerr\n:=\nos\n.\nReadFile\n(\nreadFileInput\n.\nPath\n)\nif\nerr\n!=\nnil\n{\nreturn\n\"\"\n,\nerr\n}\nreturn\nstring\n(\ncontent\n)\n,\nnil\n}\nfunc\nGenerateSchema\n[\nT any\n]\n(\n)\nanthropic\n.\nToolInputSchemaParam\n{\nreflector\n:=\njsonschema\n.\nReflector\n{\nAllowAdditionalProperties\n:\nfalse\n,\nDoNotReference\n:\ntrue\n,\n}\nvar\nv T\n\n\tschema\n:=\nreflector\n.\nReflect\n(\nv\n)\nreturn\nanthropic\n.\nToolInputSchemaParam\n{\nProperties\n:\nschema\n.\nProperties\n,\n}\n}\nThat’s not much, is it? It’s a single function,\nReadFile\n, and two descriptions the model will see: our\nDescription\nthat describes the tool itself (\n\"Read the contents of a given relative file path. ...\"\n) and a description of the single input parameter this tool has (\n\"The relative path of a ...\"\n).\nThe\nReadFileInputSchema\nand\nGenerateSchema\nstuff? We need that so that we can generate a JSON schema for our tool definition which we send to the model. To do that, we use the\njsonschema\npackage, which we need to import and download:\n// main.go\npackage\nmain\nimport\n(\n\"bufio\"\n\"context\"\n// Add this:\n\"encoding/json\"\n\"fmt\"\n\"os\"\n\"github.com/anthropics/anthropic-sdk-go\"\n// Add this:\n\"github.com/invopop/jsonschema\"\n)\nThen run the following:\ngo mod tidy\nThen, in the\nmain\nfunction, we need to make sure that we use the definition:\nfunc\nmain\n(\n)\n{\n// [... previous code ...]\ntools\n:=\n[\n]\nToolDefinition\n{\nReadFileDefinition\n}\n// [... previous code ...]\n}\nTime to try it!\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: what's in main.go?\nClaude\n: I'll help you check what's in the main.go file. Let me read it for you.\nYou\n:\nWait, what? Ho, ho, ho, it wants to use the tool! Obviously the output will be slightly different for you, but it certainly sounds like Claude knows that it can read files, right?\nThe problem is that we don’t listen! When Claude winks, we ignore it. We need to fix that.\nHere, let me show you how to do that in a single, quick, surprisingly-agile-for-my-age move by replacing our\nAgent\n’s\nRun\nmethod with this:\n// main.go\nfunc\n(\na\n*\nAgent\n)\nRun\n(\nctx context\n.\nContext\n)\nerror\n{\nconversation\n:=\n[\n]\nanthropic\n.\nMessageParam\n{\n}\nfmt\n.\nPrintln\n(\n\"Chat with Claude (use 'ctrl-c' to quit)\"\n)\nreadUserInput\n:=\ntrue\nfor\n{\nif\nreadUserInput\n{\nfmt\n.\nPrint\n(\n\"\\u001b[94mYou\\u001b[0m: \"\n)\nuserInput\n,\nok\n:=\na\n.\ngetUserMessage\n(\n)\nif\n!\nok\n{\nbreak\n}\nuserMessage\n:=\nanthropic\n.\nNewUserMessage\n(\nanthropic\n.\nNewTextBlock\n(\nuserInput\n)\n)\nconversation\n=\nappend\n(\nconversation\n,\nuserMessage\n)\n}\nmessage\n,\nerr\n:=\na\n.\nrunInference\n(\nctx\n,\nconversation\n)\nif\nerr\n!=\nnil\n{\nreturn\nerr\n}\nconversation\n=\nappend\n(\nconversation\n,\nmessage\n.\nToParam\n(\n)\n)\ntoolResults\n:=\n[\n]\nanthropic\n.\nContentBlockParamUnion\n{\n}\nfor\n_\n,\ncontent\n:=\nrange\nmessage\n.\nContent\n{\nswitch\ncontent\n.\nType\n{\ncase\n\"text\"\n:\nfmt\n.\nPrintf\n(\n\"\\u001b[93mClaude\\u001b[0m: %s\\n\"\n,\ncontent\n.\nText\n)\ncase\n\"tool_use\"\n:\nresult\n:=\na\n.\nexecuteTool\n(\ncontent\n.\nID\n,\ncontent\n.\nName\n,\ncontent\n.\nInput\n)\ntoolResults\n=\nappend\n(\ntoolResults\n,\nresult\n)\n}\n}\nif\nlen\n(\ntoolResults\n)\n==\n0\n{\nreadUserInput\n=\ntrue\ncontinue\n}\nreadUserInput\n=\nfalse\nconversation\n=\nappend\n(\nconversation\n,\nanthropic\n.\nNewUserMessage\n(\ntoolResults\n...\n)\n)\n}\nreturn\nnil\n}\nfunc\n(\na\n*\nAgent\n)\nexecuteTool\n(\nid\n,\nname\nstring\n,\ninput json\n.\nRawMessage\n)\nanthropic\n.\nContentBlockParamUnion\n{\nvar\ntoolDef ToolDefinition\nvar\nfound\nbool\nfor\n_\n,\ntool\n:=\nrange\na\n.\ntools\n{\nif\ntool\n.\nName\n==\nname\n{\ntoolDef\n=\ntool\n\t\t\tfound\n=\ntrue\nbreak\n}\n}\nif\n!\nfound\n{\nreturn\nanthropic\n.\nNewToolResultBlock\n(\nid\n,\n\"tool not found\"\n,\ntrue\n)\n}\nfmt\n.\nPrintf\n(\n\"\\u001b[92mtool\\u001b[0m: %s(%s)\\n\"\n,\nname\n,\ninput\n)\nresponse\n,\nerr\n:=\ntoolDef\n.\nFunction\n(\ninput\n)\nif\nerr\n!=\nnil\n{\nreturn\nanthropic\n.\nNewToolResultBlock\n(\nid\n,\nerr\n.\nError\n(\n)\n,\ntrue\n)\n}\nreturn\nanthropic\n.\nNewToolResultBlock\n(\nid\n,\nresponse\n,\nfalse\n)\n}\nSquint and you’ll see that it’s 90% boilerplate and 10% that matter: when we get a\nmessage\nback from Claude, we check wether Claude asked us to execute a tool by looking for\ncontent.Type == \"tool_use\"\n, if so we hand over to\nexecuteTool\n, lookup the tool by name in our local registry, unmarshal the input, execute it, return the result. If it’s an error, we flip a boolean. That’s it.\n(Yes, there is a loop in a loop, but it doesn’t matter.)\nWe execute the tool, send the result back up to Claude, and ask again for Claude’s response. Truly: that’s\nit\n. Let me show you.\nMise-en-place, run this:\necho\n'what animal is the most disagreeable because it always says neigh?'\n>>\nsecret-file.txt\nThat creates a\nsecret-file.txt\nin our directory, containing a mysterious riddle.\nIn that very same directory, let’s run our new tool-using agent, and ask it to look at the file:\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: Claude, buddy, help me solve the riddle in the secret-file.txt file\nClaude\n: I'll help you solve the riddle in the secret-file.txt file. Let me first read the contents of this file to see what the riddle is.\ntool\n: read_file({\"path\":\"secret-file.txt\"})\nClaude\n: Great! I've found the riddle.\n\nThe answer to the riddle is: **A horse**\n\nThe riddle is playing on wordplay where \"neigh\" (the sound a horse makes) sounds like \"nay\" (a way to say \"no\" or disagree with something). So horses are described as disagreeable because they always say \"neigh/nay\" - implying they're always disagreeing.\nYou\n:\nLet’s take a deep breath and say it together. Ready? Here we go:\nholy shit\n. You just give it a tool and it…\nuses it when it thinks it’ll help solve the task\n. Remember: we didn’t say\nanything\nabout “if a user asks you about a file, read the file”. We also didn’t say “if something looks like a filename, figure out how to read it”. No, none of that. We say “help me solve the thing in this file” and Claude realizes that it can read the file to answer that and off it goes.\nOf course, we\ncan\nbe specific and really nudge it towards a tool, but it basically does it all on its own:\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: What's going on in main.go? Be brief!\nClaude\n: I'll check the main.go file to see what's going on.\ntool\n: read_file({\"path\":\"main.go\"})\nClaude\n: Based on my review, main.go implements a Claude AI assistant agent that can use tools. Here's a brief summary:\n\n- It creates a command-line chat interface with Claude using the Anthropic Go SDK\n- Implements a tool system with a single \"read_file\" tool for reading file contents\n- Manages the conversation flow with Claude, handling both regular text responses and tool use requests\n- When Claude requests to use a tool, the agent executes it and returns the results\n- The program loops continuously to maintain a conversation until the user exits\n\nThe code essentially builds a simple CLI-based chatbot that can interact with the file system when requested by Claude.\nYou\n:\nSpot on. Okay, now that we know how to make Claude use tools, let’s add a few more.\nThe\nlist_files\ntool\nIf you’re anything like me, the first thing you do when you log into a new computer is to get your bearings by running\nls\n— list files.\nLet’s give Claude the same ability, a tool to list files. And here’s the complete implementation of a\nlist_files\ntool:\n// main.go\nvar\nListFilesDefinition\n=\nToolDefinition\n{\nName\n:\n\"list_files\"\n,\nDescription\n:\n\"List files and directories at a given path. If no path is provided, lists files in the current directory.\"\n,\nInputSchema\n:\nListFilesInputSchema\n,\nFunction\n:\nListFiles\n,\n}\ntype\nListFilesInput\nstruct\n{\nPath\nstring\n`json:\"path,omitempty\" jsonschema_description:\"Optional relative path to list files from. Defaults to current directory if not provided.\"`\n}\nvar\nListFilesInputSchema\n=\nGenerateSchema\n[\nListFilesInput\n]\n(\n)\nfunc\nListFiles\n(\ninput json\n.\nRawMessage\n)\n(\nstring\n,\nerror\n)\n{\nlistFilesInput\n:=\nListFilesInput\n{\n}\nerr\n:=\njson\n.\nUnmarshal\n(\ninput\n,\n&\nlistFilesInput\n)\nif\nerr\n!=\nnil\n{\npanic\n(\nerr\n)\n}\ndir\n:=\n\".\"\nif\nlistFilesInput\n.\nPath\n!=\n\"\"\n{\ndir\n=\nlistFilesInput\n.\nPath\n}\nvar\nfiles\n[\n]\nstring\nerr\n=\nfilepath\n.\nWalk\n(\ndir\n,\nfunc\n(\npath\nstring\n,\ninfo os\n.\nFileInfo\n,\nerr\nerror\n)\nerror\n{\nif\nerr\n!=\nnil\n{\nreturn\nerr\n}\nrelPath\n,\nerr\n:=\nfilepath\n.\nRel\n(\ndir\n,\npath\n)\nif\nerr\n!=\nnil\n{\nreturn\nerr\n}\nif\nrelPath\n!=\n\".\"\n{\nif\ninfo\n.\nIsDir\n(\n)\n{\nfiles\n=\nappend\n(\nfiles\n,\nrelPath\n+\n\"/\"\n)\n}\nelse\n{\nfiles\n=\nappend\n(\nfiles\n,\nrelPath\n)\n}\n}\nreturn\nnil\n}\n)\nif\nerr\n!=\nnil\n{\nreturn\n\"\"\n,\nerr\n}\nresult\n,\nerr\n:=\njson\n.\nMarshal\n(\nfiles\n)\nif\nerr\n!=\nnil\n{\nreturn\n\"\"\n,\nerr\n}\nreturn\nstring\n(\nresult\n)\n,\nnil\n}\nNothing fancy here:\nlist_files\nreturns the list of files and directories in the current folder. There’s a thousand optimizations we could (and probably should) make if this were a serious effort, but since I just want to show you what’s in the wizard’s hat, this is fine.\nOne thing to note: we return a list of strings and we denote directories with a trailing slash. That’s not required, it’s just something I just decided to do. There’s no fixed format. Anything goes as long as Claude can make sense of it and whether it can you need to figure out by experimentation. You could also prepend each directory with\n\"directory: \"\nor return a Markdown document with two headers:\n\"directories\"\nand\n\"files\"\n. There’s a ton of options and which one you chose depends on what Claude can make the most sense of, how many tokens it requires, how fast it is to generate and read, and so on.\nHere, we just want to create a small\nlist_files\ntool and the easiest option wins.\nOf course we need to tell Claude about\nlist_files\ntoo:\n// main.go\nfunc\nmain\n(\n)\n{\n// [... previous code ...]\ntools\n:=\n[\n]\nToolDefinition\n{\nReadFileDefinition\n,\nListFilesDefinition\n}\n// [... previous code ...]\n}\nAnd that’s it. Let’s ask Claude what it can see in this directory.\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: what do you see in this directory?\nClaude\n: I'll help you see what's in the current directory. Let me list the files and directories for you.\ntool\n: list_files({})\nClaude\n: I can see several files and directories in the current directory:\n\n1. Git-related files and directories:\n   - `.git/` directory with various Git configuration and tracking files\n   - `.gitignore` file\n\n2. Go project files:\n   - `main.go` - the main Go source code file\n   - `go.mod` - Go module definition file\n   - `go.sum` - Go module checksum file\n\n3. Documentation:\n   - `blogpost.md` - a markdown file, likely containing blog content\n\n4. Environment configuration:\n   - `.envrc` - likely an environment configuration file\n\nWould you like me to examine the contents of any specific file from this list?\nYou\n:\nWorks! It can read the directory.\nBut here’s the thing: Claude knows how to combine these tools. We just need to prompt it in a way that provokes it:\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: Tell me about all the Go files in here. Be brief!\nClaude\n: I'll examine the Go files in the current directory for you. Let me check what files are available first.\ntool\n: list_files({})\nClaude\n: I found a Go file in the current directory. Let me examine it:\ntool\n: read_file({\"path\":\"main.go\"})\ntool\n: read_file({\"path\":\"go.mod\"})\nClaude\n: Here's a brief overview of the Go files in this project:\n\n**main.go**:\n- Implements a Claude AI agent that can interact with the local filesystem\n- Uses the Anthropic SDK for Go to communicate with Claude AI models\n- Provides two tools: `read_file` and `list_files` that allow Claude to read file contents and list directory contents\n- Manages a conversation flow between the user and Claude, handling tool execution when Claude needs to access files\n\n**go.mod**:\n- Defines the module dependencies\n- Main dependencies are the Anthropic SDK for Go and a JSON schema library\n- Uses Go 1.24.1\n\nThis appears to be a simple terminal-based application that allows users to chat with Claude while giving the AI access to read local files and list directories.\nYou\n:\nFirst it used the\nlist_files\nand then it called\nread_file\ntwice with the Go-related files that I asked it about.\nJust… just like we would, right? I mean, here, what would you do if I ask you what version of Go we use in this project? Here’s what Claude does for me:\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: What go version are we using in this project?\nClaude\n: I'll help you find the Go version being used in this project. Let me check the files to locate where the Go version might be specified.\ntool\n: list_files({})\nClaude\n: I can see there's a Go project here with several important files like `go.mod`, `go.sum`, and `main.go`. Let me check the `go.mod` file first, as it typically contains the Go version information.\ntool\n: read_file({\"path\":\"go.mod\"})\nClaude\n: According to the `go.mod` file, this project is using Go version 1.24.1.\nClaude looks at the directory, looks at\ngo.mod\n, and has the answer.\nWe’re at around 190 lines of code now. Let that sink in. Once you have, let’s add another tool.\nLet it\nedit_file\nThe last tool we’re going to add is\nedit_file\n— a tool that lets Claude edit files.\n“Holy shit”, you’re thinking now, “this is where the rubber hits the road, this is where he pulls the rabbit out of the hat.” Well, let’s see, shall we?\nFirst, let’s add a definition for our new\nedit_file\ntool:\n// main.go\nvar\nEditFileDefinition\n=\nToolDefinition\n{\nName\n:\n\"edit_file\"\n,\nDescription\n:\n`Make edits to a text file.\n\nReplaces 'old_str' with 'new_str' in the given file. 'old_str' and 'new_str' MUST be different from each other.\n\nIf the file specified with path doesn't exist, it will be created.\n`\n,\nInputSchema\n:\nEditFileInputSchema\n,\nFunction\n:\nEditFile\n,\n}\ntype\nEditFileInput\nstruct\n{\nPath\nstring\n`json:\"path\" jsonschema_description:\"The path to the file\"`\nOldStr\nstring\n`json:\"old_str\" jsonschema_description:\"Text to search for - must match exactly and must only have one match exactly\"`\nNewStr\nstring\n`json:\"new_str\" jsonschema_description:\"Text to replace old_str with\"`\n}\nvar\nEditFileInputSchema\n=\nGenerateSchema\n[\nEditFileInput\n]\n(\n)\nThat’s right, I again know what you’re thinking: “string replacement to edit\nfiles?” That’s right. Claude 3.7 loves replacing strings, so we’re going to\nimplement\nedit_file\nby telling Claude it can edit files by replacing existing\ntext with new text.\nNow here’s the implementation of the\nEditFile\nfunction in Go:\nfunc\nEditFile\n(\ninput json\n.\nRawMessage\n)\n(\nstring\n,\nerror\n)\n{\neditFileInput\n:=\nEditFileInput\n{\n}\nerr\n:=\njson\n.\nUnmarshal\n(\ninput\n,\n&\neditFileInput\n)\nif\nerr\n!=\nnil\n{\nreturn\n\"\"\n,\nerr\n}\nif\neditFileInput\n.\nPath\n==\n\"\"\n||\neditFileInput\n.\nOldStr\n==\neditFileInput\n.\nNewStr\n{\nreturn\n\"\"\n,\nfmt\n.\nErrorf\n(\n\"invalid input parameters\"\n)\n}\ncontent\n,\nerr\n:=\nos\n.\nReadFile\n(\neditFileInput\n.\nPath\n)\nif\nerr\n!=\nnil\n{\nif\nos\n.\nIsNotExist\n(\nerr\n)\n&&\neditFileInput\n.\nOldStr\n==\n\"\"\n{\nreturn\ncreateNewFile\n(\neditFileInput\n.\nPath\n,\neditFileInput\n.\nNewStr\n)\n}\nreturn\n\"\"\n,\nerr\n}\noldContent\n:=\nstring\n(\ncontent\n)\nnewContent\n:=\nstrings\n.\nReplace\n(\noldContent\n,\neditFileInput\n.\nOldStr\n,\neditFileInput\n.\nNewStr\n,\n-\n1\n)\nif\noldContent\n==\nnewContent\n&&\neditFileInput\n.\nOldStr\n!=\n\"\"\n{\nreturn\n\"\"\n,\nfmt\n.\nErrorf\n(\n\"old_str not found in file\"\n)\n}\nerr\n=\nos\n.\nWriteFile\n(\neditFileInput\n.\nPath\n,\n[\n]\nbyte\n(\nnewContent\n)\n,\n0644\n)\nif\nerr\n!=\nnil\n{\nreturn\n\"\"\n,\nerr\n}\nreturn\n\"OK\"\n,\nnil\n}\nIt checks the input parameters, it reads the file (or creates it if it exists), and replaces the\nOldStr\nwith\nNewStr\n. Then it writes the content back to disk and returns\n\"OK\"\n.\nWhat’s missing still is\ncreateNewFile\n, which is just a tiny helper function that would be 70% shorter if this wasn’t Go:\nfunc\ncreateNewFile\n(\nfilePath\n,\ncontent\nstring\n)\n(\nstring\n,\nerror\n)\n{\ndir\n:=\npath\n.\nDir\n(\nfilePath\n)\nif\ndir\n!=\n\".\"\n{\nerr\n:=\nos\n.\nMkdirAll\n(\ndir\n,\n0755\n)\nif\nerr\n!=\nnil\n{\nreturn\n\"\"\n,\nfmt\n.\nErrorf\n(\n\"failed to create directory: %w\"\n,\nerr\n)\n}\n}\nerr\n:=\nos\n.\nWriteFile\n(\nfilePath\n,\n[\n]\nbyte\n(\ncontent\n)\n,\n0644\n)\nif\nerr\n!=\nnil\n{\nreturn\n\"\"\n,\nfmt\n.\nErrorf\n(\n\"failed to create file: %w\"\n,\nerr\n)\n}\nreturn\nfmt\n.\nSprintf\n(\n\"Successfully created file %s\"\n,\nfilePath\n)\n,\nnil\n}\nAnd… we’re ready, but are\nyou\n? Are you ready to let it rip?\nThought so, let’s do this. Let’s tell Claude to create a new FizzBuzz function in JavaScript.\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: hey claude, create fizzbuzz.js that I can run with Nodejs and that has fizzbuzz in it and executes it\nClaude\n: I'll create a fizzbuzz.js file that you can run with Node.js. First, let me check if the file already exists:\ntool\n: list_files({})\nClaude\n: I don't see a fizzbuzz.js file, so I'll create one with a classic FizzBuzz implementation. This program will print numbers from 1 to 100, but for multiples of 3 it will print \"Fizz\", for multiples of 5 it will print \"Buzz\", and for numbers that are multiples of both 3 and 5 it will print \"FizzBuzz\".\ntool\n: edit_file({\"path\":\"fizzbuzz.js\",\"old_str\":\"\",\"new_str\":\"/**\\n * FizzBuzz implementation in JavaScript\\n * Prints numbers from 1 to 100, but:\\n * - For multiples of 3, print \\\"Fizz\\\" instead of the number\\n * - For multiples of 5, print \\\"Buzz\\\" instead of the number\\n * - For multiples of both 3 and 5, print \\\"FizzBuzz\\\"\\n */\\n\\nfunction fizzBuzz(n) {\\n    for (let i = 1; i <= n; i++) {\\n        let output = '';\\n        \\n        if (i % 3 === 0) output += 'Fizz';\\n        if (i % 5 === 0) output += 'Buzz';\\n        \\n        console.log(output || i);\\n    }\\n}\\n\\n// Execute FizzBuzz for numbers 1 to 100\\nconsole.log('Running FizzBuzz:');\\nfizzBuzz(100);\"}\nClaude\n: I've created a fizzbuzz.js file that you can run with Node.js. The program:\n\n1. Defines a function called `fizzBuzz` that takes a parameter `n` for the upper limit\n2. Loops from 1 to n (set to 100 in the execution)\n3. For each number:\n   - If it's divisible by 3, it adds \"Fizz\" to the output\n   - If it's divisible by 5, it adds \"Buzz\" to the output\n   - If it's divisible by both, it adds both resulting in \"FizzBuzz\"\n   - If it's not divisible by either 3 or 5, it outputs the number itself\n4. The script automatically executes the function for numbers 1 to 100\n\nYou can run this script using Node.js with the following command in your terminal:\n\nnode fizzbuzz.js\n\nThis will output the FizzBuzz sequence from 1 to 100 to your console.\nRight?! It’s impressive, isn’t it? And that’s the\nmost basic implemenation\nof\nedit_file\n— of an agent in general — you can probably come up with.\nBut, did it work? Yes, it did:\ntool\n: $ node fizzbuzz.js\nRunning FizzBuzz:\n1\n2\nFizz\n4\nBuzz\nFizz\n7\n8\nFizz\nBuzz\n11\nFizz\n13\n14\nFizzBuzz\n16\n[...]\nAmazing. But hey, let’s have it actually\nedit\na file and not just create one.\nHere’s what Claude does when I ask it to “Please edit the\nfizzbuzz.js\nso that it only prints until 15”:\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: Please edit fizzbuzz.js so that it only prints until 15\nClaude\n: I'll edit the fizzbuzz.js file to print only until 15 instead of its current limit. First, let me check the current content of the file to understand what changes are needed.\ntool\n: read_file({\"path\":\"fizzbuzz.js\"})\nClaude\n: I can see that the file currently runs FizzBuzz up to 100. I'll modify it to only run up to 15 instead by changing the parameter in the fizzBuzz function call.\ntool\n: edit_file({\"path\":\"fizzbuzz.js\",\"old_str\":\"// Execute FizzBuzz for numbers 1 to 100\\nconsole.log('Running FizzBuzz:');\\nfizzBuzz(100);\",\"new_str\":\"// Execute FizzBuzz for numbers 1 to 15\\nconsole.log('Running FizzBuzz:');\\nfizzBuzz(15);\"})\nClaude\n: I also need to update the comment at the top of the file to reflect this change:\ntool\n: edit_file({\"path\":\"fizzbuzz.js\",\"old_str\":\"/**\\n * FizzBuzz implementation in JavaScript\\n * Prints numbers from 1 to 100, but:\",\"new_str\":\"/**\\n * FizzBuzz implementation in JavaScript\\n * Prints numbers from 1 to 15, but:\"})\nClaude\n: The changes have been successfully made to the fizzbuzz.js file. The program will now:\n1. Only print FizzBuzz from 1 to 15 (instead of 1 to 100)\n2. The documentation has been updated to reflect this change\n\nThe function itself remains the same, as it takes a parameter to determine the upper limit, and we've simply changed that parameter from 100 to 15.\nIt reads the file, it edits the file to change the how long it runs,\nand\nthen it also edits the file to update the comment at the top.\nAnd it still works:\n$ node fizzbuzz.js\nRunning FizzBuzz:\n1\n2\nFizz\n4\nBuzz\nFizz\n7\n8\nFizz\nBuzz\n11\nFizz\n13\n14\nFizzBuzz\nOkay, let’s do one more and ask it to do the following:\nCreate a congrats.js script that rot13-decodes the following string ‘Pbatenghyngvbaf ba ohvyqvat n pbqr-rqvgvat ntrag!’ and prints it\nMaybe a tall order. Let’s see:\n$ go run main.go\nChat with Claude (use 'ctrl-c' to quit)\nYou\n: Create a congrats.js script that rot13-decodes the following string 'Pbatenghyngvbaf ba ohvyqvat n pbqr-rqvgvat ntrag!' and prints it\nClaude\n: I'll create a congrats.js script that decodes the ROT13-encoded string and prints it. ROT13 is a simple letter substitution cipher that replaces a letter with the 13th letter after it in the alphabet.\n\nLet me create this file for you:\ntool\n: edit_file({\"path\":\"congrats.js\",\"old_str\":\"\",\"new_str\":\"/**\\n * A script to decode a ROT13-encoded string and print the result\\n */\\n\\nfunction rot13Decode(encodedStr) {\\n  return encodedStr.replace(/[a-zA-Z]/g, function(char) {\\n    // Get the character code\\n    const charCode = char.charCodeAt(0);\\n    \\n    // Handle uppercase letters\\n    if (charCode >= 65 && charCode <= 90) {\\n      return String.fromCharCode(((charCode - 65 + 13) % 26) + 65);\\n    }\\n    \\n    // Handle lowercase letters\\n    if (charCode >= 97 && charCode <= 122) {\\n      return String.fromCharCode(((charCode - 97 + 13) % 26) + 97);\\n    }\\n    \\n    // Return non-alphabetic characters as is\\n    return char;\\n  });\\n}\\n\\nconst encodedMessage = 'Pbatenghyngvbaf ba ohvyqvat n pbqr-rqvgvat ntrag!';\\nconst decodedMessage = rot13Decode(encodedMessage);\\n\\nconsole.log(decodedMessage);\"})\nClaude\n: I've created a JavaScript file called congrats.js that:\n\n1. Defines a `rot13Decode` function that takes an encoded string and applies the ROT13 cipher to decode it\n2. Takes the provided string 'Pbatenghyngvbaf ba ohvyqvat n pbqr-rqvgvat ntrag!'\n3. Decodes it using the function\n4. Prints the decoded message to the console\n\nYou can run this script using Node.js by typing `node congrats.js` in your terminal. The script will decode and display the hidden congratulatory message!\nDoes it work? Let’s try it:\n$ node congrats.js\nCongratulations on building a code-editing agent!\nIt does!\nIsn’t this amazing?\nIf you’re anything like all the engineers I’ve talked to in the past few months,\nchances are that, while reading this, you have been waiting for the rabbit to be\npulled out of the hat, for me to say “well, in reality it’s much, much harder\nthan this.” But it’s not.\nThis is essentially all there is to the inner loop of a code-editing agent.\nSure, integrating it into your editor, tweaking the system prompt, giving it the\nright feedback at the right time, a nice UI around it, better tooling around the\ntools, support for multiple agents, and so on — we’ve built all of that in Amp,\nbut it didn’t require moments of genius. All that was required was practical\nengineering and elbow grease.\nThese models are incredibly powerful now. 300 lines of code and three tools and\nnow you’re to be able to talk to an alien intelligence that edits your code. If\nyou think “well, but we didn’t really…” — go and try it! Go and see how far\nyou can get with this. I bet it’s a lot farther than you think.\nThat\n’s why we think everything’s changing."
    },
    {
        "title": "Leveraging LLM feedback to enhance review quality",
        "url": "https://blog.iclr.cc/2025/04/15/leveraging-llm-feedback-to-enhance-review-quality/",
        "date": "2025-04-16T12:08:13.635435",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "April\n15\n2025\nLeveraging LLM feedback to enhance review quality\nCarl Vondrick\nICLR 2024\nThis post is written by James Zou, Associate Program Chair for ICLR 2025.\nPeer review is a key element of research and innovation. However, it faces growing strain from the rapidly rising volume of paper submissions, particularly at AI conferences; as a result, authors increasingly express dissatisfaction with low-quality reviews. Therefore, there is growing interest in exploring how large language models (LLMs) can enhance the quality and usefulness of peer reviews for authors.\nAt ICLR 2025, we piloted an AI agent that leveraged multiple LLMs to provide reviewers with optional feedback on their reviews. This feedback agent was optimized to give suggestions that made reviews more informative, clear, and actionable. We also implemented multiple LLM-based guardrails, called reliability tests, which evaluated specific attributes of the AI feedback before it was posted. The agent provided feedback to 18,946 randomly selected reviews (see Figure 1A). During the review period, reviewers could choose to ignore the LLM feedback (\nNot updated\n) or revise their review in response (\nUpdated\n), as the system did not make any direct changes. See our previous blog post,\nAssisting ICLR 2025 reviewers with feedback\n, for more details about the IRB-approved study setup.\nKey Findings\nFigure 1: (A)\nAmong all ICLR 2025 reviews, 22,467 were randomly selected to receive\nfeedback (feedback group), and 22,364 were randomly selected not to receive feedback (control group). Among those who received feedback, 26.6% of reviewers updated their reviews. In total, reviewers who updated incorporated 12,222 feedback items.\n(B)\n(Top) Most reviews were submitted 2-3 days before the ICLR review deadline (November 4, 2024). (Bottom) Reviewers who received feedback were much more likely to update their reviews than those in the control group. Across both groups, reviewers were more likely to update their review if they submitted it early relative to the deadline.\nOur findings reveal several significant impacts of this LLM-based system:\nReviewers incorporated 12,222 specific suggestions from the feedback agent into their reviews, indicating that many reviewers found the LLM recommendations helpful. In total, 26.6% of reviewers who received feedback updated their reviews (Figure 1A).\nIn a blinded preference assessment, machine learning researchers found that LLM feedback improved review quality in 89% of the cases.\nReviewers who updated their reviews after receiving LLM feedback increased review length by an average of 80 words, suggesting more detailed reviews.\nLLM feedback led to more engaged discussions during the rebuttal period, evidenced by longer author rebuttals and reviewer responses (see Table 1). This could be due to clearer and more actionable reviews resulting from the feedback, leading to more productive rebuttals.\nThere was no statistically significant difference in the acceptance outcomes of the final papers between the feedback and control groups. This is consistent with the feedback agent’s goal of enhancing the author-reviewer discussion rather than advocating for or criticizing the paper.\nTable 1:\nAverage rebuttal and reply lengths across control and feedback groups, and between reviewers who did or did not update their review after receiving feedback. We observe that being selected to receive feedback causally increased the length of author rebuttals by an average of 48 words (*** p ≤ 0.001) for reviews written by reviewers selected to receive feedback. We also see that the average length of reviewer replies to author rebuttals is longer among those in the feedback group (***p ≤ 0.001). Note that the feedback group includes reviews that were selected to receive feedback but ignored the feedback, which can dilute the effect size.\nOur large randomized control study highlights the potential of a carefully designed LLM-based system to enhance peer review quality at scale. By providing targeted feedback to reviewers at ICLR 2025 and enabling them to choose how to incorporate it, we observed improvements in review specificity, engagement, and actionability. We provide a more detailed analysis and discussions of the limitations of LLM feedback in our\npaper\n. As LLM capabilities continue to advance, more research and rigorous assessments are needed to understand how AI can responsibly enhance peer review.\nNitya Thakkar, Mert Yuksekgonul, Jake Silberg, James Zou (Associate Program Chair)\nThe Review Feedback Agent Team\nCarl Vondrick, Rose Yu, Violet Peng, Fei Sha, Animesh Garg\nICLR 2025 Program Chairs\nAnnouncing the Test of Time Award Winners from ICLR 2015\nRelated Posts\nICLR 2024\nICLR 2024 Test of Time Award"
    },
    {
        "title": "Deel's CEO is now in Dubai, complicating Rippling's lawsuit",
        "url": "https://techcrunch.com/2025/04/15/deels-ceo-is-now-in-dubai-complicating-ripplings-lawsuit/",
        "date": "2025-04-16T12:08:13.635435",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Rippling’s\nefforts to serve Deel CEO Alex Bouaziz\nhave been significantly complicated by the fact that Bouaziz and his lawyer are now in Dubai, according to internal correspondence seen by TechCrunch.\nThe UAE is a country with a reputation of being a safe haven for those wanting to avoid extradition.\nRippling is trying to serve Bouaziz as part of its\nblockbuster lawsuit in Ireland against Deel\n. But French bailiffs weren’t\nable to find Bouaziz\nat an apartment in Paris in his native France last week.\nThe suit accuses Bouaziz of bribing a Rippling employee in Ireland, Keith O’Brien, and the affidavit\nfrom the alleged “spy” reads like it’s out of a movie\n.\nAnd Bouaziz isn’t the only one in the UAE. Deel’s legal director Asif Malik has moved to Dubai, Rippling’s legal team said in a court\nhearing\nlast week. Malik is a U.K. citizen who Rippling is also trying to serve. He’s the same lawyer who allegedly offered to relocate O’Brien, Deel’s alleged spy, to Dubai and pay his legal costs in exchange for refusing to cooperate with Rippling, according to O’Brien’s\naffidavit\n.\nThe UAE’s legal environment makes things like extraditing people very difficult according to\nEuropean authorities\n, although the Gulf country has told media that it’s committed to working with international partners on law enforcement and recently\nstepped up extraditions\nof some long-wanted criminals.\nBouaziz’s father, the CFO of Deel, who O’Brien claims oversaw payments to him and knew of the scheme,\ncurrently lists\nhis location as the UAE on X, too.\nDeel, along with Alex and Philippe Bouaziz, didn’t respond to requests for comment. Deel\nhas responded\nto Rippling’s lawsuit denying all wrongdoing. Rippling didn’t respond to a request for comment.\nTopics\nAlex Bouaziz\n,\nDeel\n,\nExclusive\n,\nRippling\n,\nStartups\nCharles Rollet\nSenior Reporter\nCharles Rollet is a senior reporter at TechCrunch. His investigative reporting has led to U.S. government sanctions against four tech companies, including China’s largest AI firm. Prior to joining TechCrunch, Charles covered the surveillance industry for IPVM. Charles is based in San Francisco, where he enjoys hiking with his dogs. You can contact Charles securely on Signal at charlesrollet.12 or +1-628-282-2811.\nView Bio\nMost Popular\nFor security, Android phones will now auto-reboot after three days\nLorenzo Franceschi-Bicchierai\nNotorious image board 4chan hacked and internal data leaked\nLorenzo Franceschi-Bicchierai\nAmanda Silberling\nNotion releases an AI-powered email client for Gmail\nRebecca Szkutak\nLucid Gravity First Drive: An electric SUV that doesn’t make compromises\nAbigail Bassett\nRippling is trying to serve Deel’s CEO, but bailiffs can’t find him\nCharles Rollet\nHertz says customers’ personal data and driver’s licenses stolen in data breach\nZack Whittaker\nOpenAI’s new GPT-4.1 AI models focus on coding\nKyle Wiggers"
    },
    {
        "title": "It Took a Century to Find This Colossal Squid",
        "url": "https://www.nytimes.com/2025/04/15/science/squid-colossal-deep-sea-exploration.html",
        "date": "2025-04-16T12:08:13.635435",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": ""
    },
    {
        "title": "D&D Retreat Costs 10x a Chess Tournament",
        "url": "https://lichess.org/@/CheckRaiseMate/blog/this-dd-retreat-costs-10x-a-chess-tournament/7tONa4eN",
        "date": "2025-04-16T12:08:13.635435",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "This D&D Retreat Costs 10x a Chess Tournament\nFM\nCheckRaiseMate\nApr 8, 2025\n125\n5,245\nviews\nEnglish (US)\nChess\nOver the board\nTournament\nChess Personalities\nThey're always sold out\nThis week I want to look at an example from outside of chess that could point the way towards a kind of tournament that’s much more enjoyable for players and much more sustainable for organizers. That event is\nD&D in a Castle\n. In their own words, “D&D in a Castle is a 4-day luxury, all-inclusive tabletop gaming retreat.”\nOne of their 4-day vacations in the US starts at $3,250. In contrast, the similarly long World Open chess tournament costs $318, less than 10x as much. Yet D&D in a Castle consistently sells out their events.\nDungeons and Dragons is a fantasy roleplaying game. Unlike chess, which is a competitive one-on-one game, D&D is more about collaborative group storytelling.\nFrankly, it’s not my thing. I’m much more interested in crushing an opponent than telling a story in a group. Nonetheless, chess tournament organizers could learn a lot from this event.\nFocus on Experience\nIt’s in a CASTLE!!! The only chess tournament in a castle I could find with a Google search was\nthis one\nin the Weissenhaus castle on the Baltic coast. (Shoutout to the\nChess Castle of Minnesota\n, but it’s not a real castle. It’s next to a Macy’s.)\nBut why not more? It’s on theme, right?\nAs a commenter on my previous post wrote, “There are thousands of us looking for the old-fashioned (and probably mythical) wood-panelled club room.”\nIt doesn’t have to be a castle, but by focusing on ambience, organizers could set their tournaments apart.\nPromote Personalities\nD&D in a Castle has a page dedicated to promoting their Dungeon Masters (game leaders).\nUnfortunately, someone has already taken the Chess.com screen name\nDungeonDad\n, but they never played a single game. Thanks for ruining it for everyone!!\nBut seriously, chess has a natural advantage here. Chess grandmasters and content creators are already celebrities. Imagine if when you went to a tournament, you could play in a simul with Eric Rosen, or attend a talk by Jennifer Shahade.\nSome tournaments have started partnering with streamers, but there’s a lot more room to partner with and promote appealing chess personalities. What I’m trying to say is that, for $10,000, I’ll come to your chess tournament.\nProfessional Website\nCompare D&D in a Castle’s website to this:\nI’m confused how it’s even possible to make a website that looks like this in 2025. If you use a website builder like Squarespace or Wix, it will automatically format your page to look decent. So to make a page like the above, you have to know enough to write custom HTML, but also totally disregard styling and design.\nI actually find it charming that chess tournament websites look like something from 1995, but I already know what I’m signing up for. If someone is on the fence about playing their first tournament and the website looks like this, they’re not going to sign up.\nWith the tools mentioned above, it’s easy for an individual with no coding experience to make a professional looking website in a few hours. There’s no reason not to do this.\nSide Quests\nAt D&D in a Castle, “guests can choose between One-Shot games and other Side Quests, such as game-related crafting, workshops, panels and outdoor games.”\nChess tournaments can and should have more side events. These could include:\nSmaller, faster side tournaments (blitz, bughouse, etc.)\nClasses or lectures\nAuthor signing or talks\nSimuls\nPanel discussions\nThese events don’t need to replace a single “main event”, but could run alongside it, for those who want something in addition to/in place of a single, massive tournament. The schedule should have enough slack that it’s possible to fit in events like this.\nThe\nLas Vegas Chess Festival\nhas done a great job introducing additional events like these. More tournaments should follow their lead.\nI’ve noticed that, when I suggest brilliant ideas like this, I’m often misunderstood. So just to be clear, here are things I’m not saying:\nEveryone should quit chess and play D&D.\nAll chess tournaments should cost thousands of dollars.\nYou have to try to make as much money as possible.\nChess shouldn’t be competitive.\nWhat I am saying is this: if chess organizers are willing to think outside the box about what a tournament looks like, they can create experiences that are radically more appealing for many players.\nDiscuss this blog post in the forum\nYou may also like\nMar 20, 2025\nFM\nMathiCasa\nChess Football: A Fun and Creative Variant\nWhere chess pieces become \"players\" and the traditional chessboard turns into a soccer field\nApr 1, 2025\nFM\nCheckRaiseMate\nMailbag: OTB Tournaments\nWhat are people saying about OTB chess?\nMar 20, 2025\nFM\nCheckRaiseMate\nI Don't Like Chess Tournaments\nIt doesn't have to be this way!\nMar 25, 2025\nFM\nCheckRaiseMate\nWhat Would a Fun Chess Tournament Look Like?\nIf current OTB tournaments aren't working, how could we change them?\nMar 20, 2025\nFM\nCheckRaiseMate\nI Don't Like Chess Tournaments\nIt doesn't have to be this way!\nApr 1, 2025\nLichess\nNew Feature Announcement (not really)\nOne step closer to a true over-the-board experience. It's flipping fantastic!"
    },
    {
        "title": "Why are Birmingham bin workers striking and will walkouts spread?",
        "url": "https://www.thetimes.com/uk/politics/article/why-birmingham-bin-strike-workers-xqxdslgzt",
        "date": "2025-04-16T12:08:13.636451",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Why are Birmingham bin workers striking and will walkouts spread?\nThe dispute centres on the cutting of one specific role — and Unite claims up to 170 workers will be paid £8,000 less as a result\nupdated\nMax Kendix\n, Political Reporter\nWednesday April 16 2025, 6.51am BST,\nThe Times\nSome 21,000 tonnes of rubbish has been left on the streets of England’s second city\nMax Kendix\n, Political Reporter\nWednesday April 16 2025, 6.51am BST,\nThe Times\nSome 21,000 tonnes of rubbish has piled up on the streets of Birmingham in an industrial dispute between the trade union Unite and the council, centred on the cutting of one specific waste recycling and collection officer role.\nOnay Kasab, the union’s national lead officer, struggled repeatedly on Times Radio on Tuesday morning to explain just how many bin workers had been affected by the role being cut.\nThe dispute stems from the scrapping of the role of the waste recycling and collection officer (WRCO).\nThe role was created after a bin strike in 2017 and came with higher salaries and, purportedly, extra responsibilities.\nHowever, after a ruling against Birmingham city council over equal pay, which will cost the authority £760 million, the council reviewed\nLoading Title...\nLoading offer 1...\nLoading offer 2...\nLoading offer 3...\nLoading CTA...\nLoading login link...\nLog in\nRelated articles\nBirmingham bin strikes to go on, say union chiefs from 50 miles away\nApril 14 2025, 7.25pm BST\nTom Witherow\n|\nGeraldine Scott\n, Senior Political Correspondent\nUnions split over ‘ridiculous’ Birmingham bin strike\nApril 09 2025, 10.20pm BST\nGeraldine Scott\n, Senior Political Correspondent\nPROMOTED CONTENT"
    },
    {
        "title": "Manage Your Privacy Settings (2024)",
        "url": "https://www.staysafeonline.org/articles/manage-your-privacy-settings",
        "date": "2025-04-16T12:08:13.636451",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "RESOURCES\nRESOURCES\nINITIATIVES\nINITIATIVES\nABOUT US\nABOUT US\nBack\nSubscribe\nOnline Safety and Privacy\nMay 26, 2024\n|\n1\nMin Read\nManage Your Privacy Settings\nDirect links to manage your privacy settings on popular platforms and apps.\nWant to view or change your privacy or security settings, but don't know where to find them? Use these direct links to check your online settings on popular devices and services or read about your platforms’ privacy policies.\nCan’t find what you’re looking for? Email\ninfo@staysafeonline.org\nand we’ll add it.\nE-commerce\nAmazon\nBitcoin\nCashapp\neBay\nGroupon\nPaypal\nPoshmark\nVenmo\nWish\nZelle\nMobile Banking\nCapital One\nChase\nDiscover\nMastercard\nTurbo Tax / Mint (Intuit)\nTruist/Suntrust\nUSAA\nVISA\nEmail and Voice Communication\nAOL\nGoogle Mail\nOutlook.com\nProtonMail\nYahoo Mail\nYelp\nHealth Applications\nMyFitnessPal\nFitbit\nPeloton\nMapMyFitness/ MapMyRun\nNoom\nStrava\nStrong\nFood Delivery Services\nGrubHub\nUberEats\nDoorDash\ngoPuff\nCaviar\nHelloFresh\nPostMates\nBlue Apron\nGousto\nGreen Chef\nMobile/Location Services\nAndroid\nApple iOS\nApple Maps\nAT&T\nCarrot (Weather)\nGoogle Maps\nSamsung\nSprint\nT-Mobile\nSamsung\nVerizon\nWaze\nSocial Networks\nBeReal\nFacebook\nLinkedIn\nPinterest\nReddit\nTikTok\nTumblr\nTwitter\nWhatsApp\nNext Door\nStreaming Platforms\nAmazon Prime Video\nHBO\nHulu\nDisney+\nLinkedInLive\nNetflix\nTuneIn\nTwitch\nXfinity\nYouTube Live\nYouTube TV\nSling\nAppleTV+\nFunk\nDirectTV Stream\nWeb Browsers\nFirefox Focus Private Browser\nGoogle Chrome\nInternet Explorer\nMozilla Firefox\nSafari\nMicrosoft Edge\nMusic\nApple Music\nPandora\nSpotify\nSoundcloud\nSlacker Radio\nSiriusXM\nOnline Conferencing\nMicrosoft Teams\nWebEx\nZoom\nGoogle Meet\nGoToMeeting\nAccelevents\nWhova\nJami\nOnline Dating\nBumble\nTinder\nCoffee Meets Bagel\neHarmony\nGrindr\nHinge\nMatch.com\nMeetUp\nOkCupid\nPlenty of Fish\nRaya\nPhoto and Video Sharing\nFlickr\nInstagram\nSnapchat\nTikTok\nVSCO\nYouTube\nSnapfish\nDropbox\nApple Airdrop\nApple Photo Sharing\nGoogle Photos\nRide Share Services/ Scooter Rental Services\nBird\nBridj\nCurb\nGett\nLime\nLyft\nUber\nZtrip\nTravel\nAirBnb\nVrbo\nTripAdvisor\nBooking.com\nGoogle Flights\nSearch Engines\nAsk.com\nBing\nDogpile\nDuckDuckGo Privacy Search Engine\nGoogle\nQwant\nYahoo\nMisc.\nAmazon, Google & Apple Smart Speakers/Assistants\nCalm\nCappuccino\nCare.com\nGoodreads\nStorygraph\nGroupme\nMicrosoft Office\nNintendo Switch\nPS4 and PSN\nXbox One\nFeatured Articles\n6 Cybersecurity Myths Debunked\nThere are a lot of myths flying around about cybersecurity. We’ll go over the most common cybersecurity myths and debunk them so we can stay safer online.\nLearn More\nCyberbullying in the Workplace: How to Recognize, Address, and Prevent It\nCyberbullying is often associated with teenagers and social media, but some bullies grow up...and enter the workforce.\nLearn More\nData Privacy\nSelect Language\nEnglish\nEnglish\nSubscribe to our newsletter\nSign Up\nNCA Store\nResources\nOnline Safety and Privacy\nCareers and Education\nCybersecurity for Business\nCyber Dictionary\nToolkits\nAll Articles and Resources\nAll Press Stories and Awards\nInitiatives\nAI Fools\nData Privacy Week\nConvene\nCyberSecure My Business\nCybersecurity Awareness Month\nSee Yourself in Cyber\nAll Events\nAbout Us\nDonate\nCollaborate\nRequest a Speaker\nSponsor\nThe NCA\nContact Us\nSpecial Campaigns\nKubikle\nPhisher\nCyber Survival Guide\nSafe Word\n1101 Connecticut Ave, Suite 450, Washington DC 20036.\n©\n2025\nCopyright. Stay Safe Online, NCA. All Rights Reserved.\nCy Pres\n|\nPrivacy Policy\n|\nCode of Conduct\n|\nBrand Assets\nCookie Settings"
    },
    {
        "title": "Raw Loops for Performance?",
        "url": "https://www.sandordargo.com/blog/2025/04/16/raw-loops-for-performance",
        "date": "2025-04-16T12:08:13.636451",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Sandor Dargo's Blog\nOn C++, software development and books\nHOME\nTAGS\nARCHIVES\nBOOKS\nSPEAKING\nDAILY C++\nHI...\nBlog\n2025\n04\n16\nRaw loops for performance?\nPost\nCancel\nRaw loops for performance?\nSandor Dargo\n8 hours ago\n2025-04-16T00:00:00+02:00\n8 min\nTo my greatest satisfaction, I’ve recently joined a new project. I started to read through the codebase before joining and at that stage, whenever I saw a possibility for a minor improvement, I raised a tiny pull request. One of my pet peeves is rooted in Sean Parent’s 2013 talk at GoingNative,\nSeasoning C++\nwhere he advocated for\nno raw loops\n.\nWhen I saw this loop, I started to think about how to replace it:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n#include\n<iostream>\n#include\n<list>\n#include\n<string>\n#include\n<vector>\nstruct\nFromData\n{\n// ...\nstd\n::\nstring\ntitle\n;\nint\namount\n;\n};\nstruct\nWidget\n{\n// ...\nstd\n::\nlist\n<\nFromData\n>\ndata\n;\n};\nstruct\nToData\n{\n// ...\nstd\n::\nstring\ntitle\n;\nint\namount\n;\n};\nstruct\nResponse\n{\n// ...\nstd\n::\nvector\n<\nToData\n>\ndata\n;\n};\nResponse\nfoo\n(\nWidget\nwidget\n)\n{\nstd\n::\nvector\n<\nToData\n>\ntransformed_data\n;\nfor\n(\nconst\nauto\n&\nelement\n:\nwidget\n.\ndata\n)\n{\ntransformed_data\n.\npush_back\n(\n{.\ntitle\n=\nelement\n.\ntitle\n,\n.\namount\n=\nelement\n.\namount\n*\n42\n});\n}\nResponse\nresponse\n;\n// ...\nresponse\n.\ndata\n=\ntransformed_data\n;\nreturn\nresponse\n;\n}\nint\nmain\n()\n{\nWidget\nwidget\n{.\ndata\n=\n{\n{\n\"a\"\n,\n1\n},\n{\n\"b\"\n,\n2\n},\n{\n\"c\"\n,\n1\n},\n}};\nauto\nr\n=\nfoo\n(\nwidget\n);\nfor\n(\nconst\nauto\n&\nelement\n:\nr\n.\ndata\n)\n{\nstd\n::\ncout\n<<\n\"title: \"\n<<\nelement\n.\ntitle\n<<\n\", amount \"\n<<\nelement\n.\namount\n<<\n'\\n'\n;\n}\n}\n/*\ntitle: a, amount 42\ntitle: b, amount 84\ntitle: c, amount 42\n*/\nPlease note that the example is simplified and slightly changed so that it compiles on its own.\nLet’s focus on\nfoo\n, the rest is there just to make the example compilable.\nIt seems that we could use\nstd::transform\n. But heck, we use C++20 we have ranges at our hands so let’s go with\nstd::ranges::transform\n!\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n#include\n<iostream>\n#include\n<list>\n#include\n<ranges>\n#include\n<string>\n#include\n<vector>\nstruct\nFromData\n{\n// ...\nstd\n::\nstring\ntitle\n;\nint\namount\n;\n};\nstruct\nWidget\n{\n// ...\nstd\n::\nlist\n<\nFromData\n>\ndata\n;\n};\nstruct\nToData\n{\n// ...\nstd\n::\nstring\ntitle\n;\nint\namount\n;\n};\nstruct\nResponse\n{\n// ...\nstd\n::\nvector\n<\nToData\n>\ndata\n;\n};\nResponse\nfoo\n(\nWidget\nwidget\n)\n{\nconst\nauto\ntransformed_data\n=\nwidget\n.\ndata\n|\nstd\n::\nviews\n::\ntransform\n([](\nconst\nauto\n&\nelement\n)\n{\nreturn\nToData\n{\n.\ntitle\n=\nelement\n.\ntitle\n,\n.\namount\n=\nelement\n.\namount\n*\n42\n};\n});\nResponse\nresponse\n;\n// ...\nresponse\n.\ndata\n=\n{\ntransformed_data\n.\nbegin\n(),\ntransformed_data\n.\nend\n()};\nreturn\nresponse\n;\n}\nint\nmain\n()\n{\nWidget\nwidget\n{.\ndata\n=\n{\n{\n\"a\"\n,\n1\n},\n{\n\"b\"\n,\n2\n},\n{\n\"c\"\n,\n1\n},\n}};\nauto\nr\n=\nfoo\n(\nwidget\n);\nfor\n(\nconst\nauto\n&\nelement\n:\nr\n.\ndata\n)\n{\nstd\n::\ncout\n<<\n\"title: \"\n<<\nelement\n.\ntitle\n<<\n\", amount \"\n<<\nelement\n.\namount\n<<\n'\\n'\n;\n}\n}\n/*\ntitle: a, amount 42\ntitle: b, amount 84\ntitle: c, amount 42\n*/\nWe have no more raw loops, no more\ninitialized then modified\nvectors, and the result is the same.\nIs this better?\nWe don’t have to modify a vector that’s definitely better. But when I proposed such a change, one of my colleagues asked a question.\ntransformed_data\nbecame a view. When we populate\nresponse.data\n, do we actually copy all the elements of the view?\nI couldn’t answer the question with confidence, hence this article.\nI slightly updated both examples,and updated\nToData\nto this:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\nstruct\nToData\n{\n// ...\nstd\n::\nstring\ntitle\n;\nint\namount\n;\nToData\n()\n{\nstd\n::\ncout\n<<\n\"ToData()\n\\n\n\"\n;\n}\nToData\n(\nstd\n::\nstring\ntitle\n,\nint\namount\n)\n:\ntitle\n(\ntitle\n),\namount\n(\namount\n)\n{\nstd\n::\ncout\n<<\n\"ToData(std::string title, int amount)\n\\n\n\"\n;\n}\nToData\n(\nconst\nToData\n&\nother\n)\n:\ntitle\n(\nother\n.\ntitle\n),\namount\n(\nother\n.\namount\n)\n{\nstd\n::\ncout\n<<\n\"ToData(const ToData& other)\n\\n\n\"\n;\n}\nToData\n&\noperator\n=\n(\nconst\nToData\n&\nother\n)\n{\nstd\n::\ncout\n<<\n\"ToData& operator=(const ToData& other)\n\\n\n\"\n;\ntitle\n=\nother\n.\ntitle\n;\namount\n=\nother\n.\namount\n;\nreturn\n*\nthis\n;\n}\nToData\n(\nToData\n&&\nother\n)\n:\ntitle\n(\nstd\n::\nexchange\n(\nother\n.\ntitle\n,\n\"\"\n)),\namount\n(\nstd\n::\nexchange\n(\nother\n.\namount\n,\n0\n))\n{\nstd\n::\ncout\n<<\n\"ToData(ToData&& other)\n\\n\n\"\n;\n}\nToData\n&\noperator\n=\n(\nToData\n&&\nother\n)\n{\nstd\n::\ncout\n<<\n\"ToData& operator=(ToData&& other)\n\\n\n\"\n;\ntitle\n=\nstd\n::\nexchange\n(\nother\n.\ntitle\n,\n\"\"\n);\namount\n=\nstd\n::\nexchange\n(\nother\n.\namount\n,\n0\n);\nreturn\n*\nthis\n;\n}\n};\nI also had to remove the usage of designed initializers as\nToData\nis no longer an aggregate.\nThe output for the original version using\npush_back\nis not particularly surprising.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nToData(std::string title, int amount)\nToData(ToData&& other)\nToData(std::string title, int amount)\nToData(ToData&& other)\nToData(const ToData& other)\nToData(std::string title, int amount)\nToData(ToData&& other)\nToData(const ToData& other)\nToData(const ToData& other)\nwriting response.data\nToData(const ToData& other)\nToData(const ToData& other)\nToData(const ToData& other)\nwrote response.data\ntitle: a, amount 42\ntitle: b, amount 84\ntitle: c, amount 42\nIn the for loop, we construct\nToData\nand move it, and there is also a copy construction. Before actually copying the data.\nOn the other hand,\nfor the version using ranges, the outpout is shorter and different\n!\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nfilling response.data\nToData(std::string title, int amount)\nToData(ToData&& other)\nToData(std::string title, int amount)\nToData(ToData&& other)\nToData(const ToData& other)\nToData(std::string title, int amount)\nToData(ToData&& other)\nToData(const ToData& other)\nToData(const ToData& other)\nfilled1 response.data\ntitle: a, amount 42\ntitle: b, amount 84\ntitle: c, amount 42\nNothing actually happens within the transformation pipeline! Everything is happening lazily when we use the results of the pipeline and actually construct a\nvector\n. Then we have fewer calls than we had in the original version. Seemingly, far the ranges version has an advantage!\nBut we all know that the original version is not optional even with a raw loop. Let’s use\nemplace_back\n! Oh and we also forgot about calling\nstd::vector<T>::reserve\nto avoid reallocations!\nHere is the code producing the below output.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nToData(std::string title, int amount)\nToData(std::string title, int amount)\nToData(std::string title, int amount)\nwriting response.data\nToData(const ToData& other)\nToData(const ToData& other)\nToData(const ToData& other)\nwrote response.data\ntitle: a, amount 42\ntitle: b, amount 84\ntitle: c, amount 42\nNow the raw loop version has an advantage! In this version, for each item, we have a constructor and a copy while in the ranges version, we also have an extra move!\nNote that since C++23, you can also use\nstd::ranges::to<std::vector<Todata>>\nto construct the final\nvector\n, but it didn’t result in any difference in terms of the number of special member function calls.\nIs that so bad? Probably not. Move operations are cheap, that’s why they were introduced! Probably this is just an acceptable price to pay for more readable code. But our “more readable code” also features a lambda so let’s just say that we have assumptions.\nLet’s also run benchmarks.\nBased on\nQuick Bench\n, the enhanced raw loop version is about 20% faster on Clang than the raw loop version. The results are slightly different with GCC, but the raw loop version is still 10% faster. It’s also worth noting that the original version with a\npush_back\nand without the\nreserve\nis 20-30% slower than the other two versions! By adding the\nreserve\nbut still using\npush_back\n, the code is between the ranges and the raw loop with\nemplace_back\nversion.\nWhat does this mean in real life?\nIt depends. You must measure. Don’t forget about\nAmdahl’s law\nwhich says that\n“the overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used.”\nIf this happens to be a bottleneck, use the\nemplace_back\nversion without hesitation and don’t forget about reserving enough space in memory for all the elements.\nI think you have no reason to use the\npush_back\nversion and definitely not without calling\nreserve\n.\nOtherwise, if you write code where you also do some network calls or read from the database or from the filesystem, these differences are negligible and you should go with the version that you find the most readable.\nThat’s up to you.\nConclusion\nUsing ranges or algorithms has\nseveral advantages over raw loops\n, notably readability. On the other hand, as we’ve just seen, sheer performance is not necessarily among those advantages. Using ranges can be slightly slower than a raw loop version. But that’s not necessarily a problem, it really depends on your use case. Most probably it won’t make a bit difference.\nConnect deeper\nIf you liked this article, please\nhit on the like button,\nsubscribe to my newsletter\ndev\ncpp\ncpp20\ncpp23\nranges\nloops\nThis post is licensed under\nCC BY 4.0\nby the author.\nShare\nRecent Update\nThe Battle Hardened Developer by Fiodar Sazanavets\nC++26: erroneous behaviour\nC++26: Removing language features\nThe big STL Algorithms tutorial: transform\nMy DEV birthday gift for you: DEV.to Analytics!\nTrending Tags\ncpp\nbooks\nwatercooler\ncareer\ntutorial\ncpp23\nstl\nalgorithms\nself-improvement\nmanagement\nContents\nFurther Reading\nApr 13, 2022\n2022-04-13T00:00:00+02:00\nMy first work experience with C++20\nI joined a new team recently. We have our own internal microservices as well as libraries. While for microservices we support one main branch, for libraries we do have to support at least three, in...\nFeb 15, 2023\n2023-02-15T00:00:00+01:00\nThe evolution of enums\nConstants are great. Types are great. Constants of a specific type are really great. This is why enum classes are just fantastic. Last year, we talked about why we should avoid using boolean funct...\nApr 3, 2021\n2021-04-03T00:00:00+02:00\nC++ 20: Get the details by Rainer Grimm\nI could say that I picked C++ 20: Get the details up because I wanted to learn about the latest version of C++. I wouldn’t lie if I said so, but truth be told I was already an avid reader of Modern...\nShould you use final?\n-\nComments powered by\nDisqus\n.\nTrending Tags\ncpp\nbooks\nwatercooler\ncareer\ntutorial\ncpp23\nstl\nalgorithms\nself improvement\nmanagement"
    },
    {
        "title": "The DNS system isn't a database and shouldn't be used as one",
        "url": "https://utcc.utoronto.ca/~cks/space/blog/tech/DNSNotADatabase",
        "date": "2025-04-16T12:08:13.636451",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Chris Siebenmann\n::\nCSpace\n»\nblog\n»\ntech\n»\n       DNSNotADatabase\nWelcome, guest.\nThe DNS system isn't a database and shouldn't be used as one\nApril 15, 2025\nOver on the Fediverse,\nI said something\n:\nThesis: DNS is not meaningfully a database, because it's explicitly\ndesigned and used today so that it gives different answers to\ndifferent people. Is it implemented with databases? Sure. But treating\nit as a database is a mistake. It's a query oracle, and as a query\noracle it's not trustworthy in the way that you would normally trust\na database to be, for example, consistent between different people\nquerying it.\nIt would be nice if we had a global, distributed, relatively easily\nqueryable, consistent database system. It would make a lot of things\npretty nice, especially if we could wrap some cryptography around\nit to make sure we were getting honest answers. However, the general\nDNS system is not such a database and can't be used as one, and as\na result should not be pressed into service as one in protocols.\nDNS is designed from the ground up to lie to you in unpredictable\nways, and parts of the DNS system lie to you every day. We call\nthese lies things like 'outdated cached data' or 'geolocation based\nDNS' (or 'split horizon DNS'), but they're lies, or at least\ninconsistent alternate versions of some truth. The same fundamental\nproperties that allow these inconsistent alternate versions also\nallow for more deliberate and specific lies, and they also mean\nthat no one can know with assurance what version of DNS anyone else\nis seeing.\n(People who want to reduce the chance for active lies as much as\npossible must do a variety of relatively extreme things, like query\nDNS from multiple vantage points around the Internet and perhaps\nthrough multiple third party DNS servers. No, checking DNSSEC isn't\nenough,\neven when it's present\n(\nalso\n), because\nthat just changes who can be lying to you.)\nAnything that uses the global DNS system should be designed to\nexpect outdated, inconsistent, and varying answers to the questions\nit asks (and sometimes incorrect answers, for various reasons).\nSometimes those answers will be lies (including the lie of 'that\nname doesn't exist'). If your design can't deal with all of this,\nyou shouldn't be using DNS.\nWritten on\n15\nApril\n2025\n.\n«\nZFS's delayed compression of written data (when compression is enabled)\nThese are my\nWanderingThoughts\n(\nAbout the blog\n)\nFull index of entries\nRecent comments\nThis is part of\nCSpace\n, and is written by\nChrisSiebenmann\n.\nMastodon:\n@cks\nTwitter\n@thatcks\n* * *\nCategories:\nlinks\n,\nlinux\n,\nprogramming\n,\npython\n,\nsnark\n,\nsolaris\n,\nspam\n,\nsysadmin\n,\ntech\n,\nunix\n,\nweb\nAlso:\n(Sub)topics\nThis is a\nDWiki\n.\nGettingAround\n(\nHelp\n)\nSearch:\nPage tools:\nView Source\n,\nAdd Comment\n.\nSearch:\nLogin:\nPassword:\nAtom Syndication:\nRecent Comments\n.\nLast modified: Tue Apr 15 22:53:15 2025\nThis dinky wiki is brought to you by the Insane Hackers\nGuild, Python sub-branch."
    },
    {
        "title": "Is Product Hunt Dying?",
        "url": "https://news.ycombinator.com/item?id=43701815",
        "date": "2025-04-16T12:08:13.637468",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Hacker News\nnew\n|\npast\n|\ncomments\n|\nask\n|\nshow\n|\njobs\n|\nsubmit\nlogin\nIs Product Hunt Dying?\n1 point\nby\nashmil\n1 hour ago\n|\nhide\n|\npast\n|\nfavorite\n|\n2 comments\nI've noticed a surge in low-effort launches, copycat products, and templated AI spam.\nWhat’s worse? Paid upvotes are winning the day. Quality no longer equals visibility.\nReal builders pouring months into crafting unique, valuable products are often buried under noise.\nMeanwhile, “Weekend hacks” with fancy landing pages and fake engagement top the charts.\nIt used to be a place where innovation got discovered. Now it’s a leaderboard of who has the better hype machine.\nrrr_oh_man\n58 minutes ago\n[–]\nThat text is fully ChatGPT-generated...\nreply\nnazmul21\n55 minutes ago\n|\nparent\n[–]\nYou are right.\nreply\nJoin us for\nAI Startup School\nthis June 16-17 in San Francisco!\nGuidelines\n|\nFAQ\n|\nLists\n|\nAPI\n|\nSecurity\n|\nLegal\n|\nApply to YC\n|\nContact\nSearch:"
    },
    {
        "title": "Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research",
        "url": "https://www.lesswrong.com/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks",
        "date": "2025-04-16T12:08:13.637468",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Lewis Smith*, Sen Rajamanoharan*, Arthur Conmy, Callum McDougall, Janos Kramar, Tom Lieberum, Rohin Shah, Neel Nanda\n* = equal contribution\nThe following piece is a list of snippets about research from the GDM mechanistic interpretability team, which we didn’t consider a good fit for turning into a paper, but which we thought the community might benefit from seeing in this less formal form. These are largely things that we found in the process of a project investigating whether sparse autoencoders (SAEs) were useful for downstream tasks, notably out-of-distribution probing.\nTL;DR\nTo validate whether SAEs were a worthwhile technique,\nwe explored whether they were useful on the downstream task of OOD generalisation when detecting harmful intent in user prompts\nNegative result:\nSAEs underperformed linear probes\nCorollary: Linear probes are actually really good and cheap and perform great\nAs a result of this and parallel work,\nwe are deprioritising fundamental SAE research for the moment and exploring other directions\n, though SAEs will remain a tool in our toolkit\nWe do\nnot\nthink that SAEs are useless or that no one should work on them, but we also do not think that SAEs will be a game-changer for interpretability, and speculate that the field is over-invested in them.\nTraining SAEs specialised for chat data closed about half the gap but was still worse than linear probes\nWe tried several ways to train chat SAEs, all did about as well\n. By default, we recommend taking an SAE on pretraining data and finetuning it on a bit of chat data\nOther results:\nWe found SAEs fairly helpful for debugging low quality datasets\n(noticing spurious correlations)\nWe present a variant of JumpReLU with an alternative sparsity penalty to get rid of high-frequency latents\nWe argue that a standard auto-interp approach of computing the average interpretability of a uniformly sampled SAE latent can be misleading\nas it doesn’t penalise models which have high frequency, but not very interpretable, latents, and explore weighting the interpretability score by latent frequency.\nIntroduction\nMotivation\nOur core motivation was that we, along with much of the interpretability community, had invested a lot of our energy into Sparse Autoencoder (SAE) research. But SAEs lack a ground truth of the “true” features in language models to compare to, making it pretty unclear how well they work. There is qualitative evidence that SAEs are clearly doing\nsomething\n, far more structure than you would expect by random chance. But they clearly have a bunch of issues: if you just type an arbitrary sentence into\nNeuronpedia\n, and look at the latents that light up, they do not seem to perfectly correspond to a crisp explanation.\nMore generally, when thinking about whether we should prioritise working on SAEs, it's worth thinking about how to decide what kind of interpretability research to do in general. One perspective is to assume there is some crisp, underlying, human-comprehensible truth for what is going on in the model, and to try to build techniques to reverse engineer it. In the case of SAEs, this looks like\nthe hope that SAE latents capture some canonical set of true concepts inside the model\n. We think it is clear now that\nSAEs in their current form are far from achieving this\n, and it is unclear to us if such “true concepts” even exist. There are several flaws with SAEs that prevent them from capturing a true set of concepts even if one exists, and we are pessimistic that these can all be resolved:\nSAEs are missing concepts\nConcepts are represented in noisy ways where e.g.\nsmall activations don't seem interpretable\nLatents can be warped in weird ways like\nfeature absorption\nSeemingly interpretable latents have many false negatives\n.\nFor more issues with SAEs see Section 2.1.2c of\nSharkey et al.\nBut there are other high-level goals for interpretability than perfectly finding the objective truth of what’s going on - if we can build tools that give understanding that’s imperfect but enough to let us do useful things, like understand whether\na model is faking alignment\n, that is still very worthwhile. Several important goals, like trying to debug mysterious failures and phenomena, achieving a better understanding of what goals and deception look like, or trying to detect deceptive alignment, do not necessarily require us to discover all of the ‘true features’ of a model; a decent approximation to the models computation might well be good enough. But how could we tell if working on SAEs was bringing us closer to these goals?\nOur hypothesis was that if SAEs will eventually be useful for these ambitious tasks, they should enable us to do\nsomething\nnew today. So, the goal of this project was to investigate\nwhether we can do anything useful on downstream tasks\nwith SAEs in a way that was at all competitive with baselines - i.e. a task that can be described without making any reference to interpretability. If SAEs are working well enough to be a valuable tool, then there should be things they enable us to do that we cannot currently easily do. And so we thought that if we picked some likely examples of such tasks and then made a fair comparison to well-implemented baselines on some downstream task then, if the SAE does well (ideally beating the baseline, but even just coming close while being non-trivially different), this is a sign that the SAE is a valuable technique worthy of further refinement. Further, even if the SAE doesn’t succeed, it gives you an eval to measure future SAE progress, like how\nFarrell et al\n’s unlearning setup was turned into an eval in\nSAEBench\n.\nOur Task\nSo what task did we focus on? Our key criteria was to be objectively measurable, be something that other people cared about and, within those constraints, aiming for something where we thought SAEs might have an edge. As such, we focused on training probes that generalise well out of distribution. We thought that, for sufficiently good SAEs, a sparse probe in SAE latents would be less likely to overfit to minor spurious correlations compared to a dense probe, and thus that being interpretable gave a valuable inductive bias (\nthough are now less confident in this argument\n). We specifically looked in the context of detecting harmful user intent in the presence of different jailbreaks, and used new jailbreaks as our OOD set.\nSadly, our core results are negative:\nDense linear probes perform nearly perfectly, including out of distribution.\n1-sparse SAE probes (i.e. using a single SAE latent as a probe) are much worse, failing to fit the training set.\nk-sparse SAE probes can fit the training set for moderate k (approximately k=20), and successfully generalise to an in-distribution test set, but show distinctly worse performance on the OOD set.\nFinetuning SAEs on specialised chat data helps, but only closes about half the gap to dense linear probes.\nLinear probes trained only on the SAE reconstruction are also significantly worse than linear probes on the residual stream OOD, suggesting that SAEs are discarding information relevant to the target concept\nWe did have one positive result: the sparse SAE probes enabled us to quickly identify spurious correlations in our dataset, which we cleaned up. Note this slightly stacks the deck against SAEs, since without SAE-based debugging, the linear probes may have latched onto these spurious correlations – however, we think we plausibly could have found the spurious correlations without SAEs given more time, e.g.\nKantamneni et al\nshowed simpler methods could be similarly effective to SAEs here.\nWe were surprised by SAEs underperforming linear probes, but also by how well linear probes did in absolute terms, on the complex-seeming task of detecting harmful intent.\nWe expect there are many practical ways linear probes could be used today\nto do cheap monitoring for unsafe behaviour in frontier models.\nConclusions and Strategic Updates\nOur overall update from this project and parallel external work is to be less excited about research focused on understanding and improving SAEs and, at least for the short term, to explore other research areas\n.\nThe core update we made is that SAEs are unlikely to be a magic bullet, i.e. we think the hope that with a little extra work they can just make models super interpretable and easy to play with doesn’t seem like it will pay off.\nThe key update we’ve made from our probing results is that current SAEs do not find the ‘concepts’ required to be useful on an important task (detecting harmful intent), but a linear probe can find a useful direction. This may be because the model doesn’t represent harmful intent as a fundamental concept and the SAE is working as intended while the probe captures a mix of tons of concepts, or because the concept is present but the SAE is bad at learning it, or any number of hypotheses. But whatever the reason, it is evidence against SAEs being the right tool for things we want to do in practice.\nWe consider our probing results disheartening but not enough to pivot on their own. But there have been several other parallel projects in the literature such as\nKantamneni et al\n.,\nFarrell et al\n.,\nWu et al\n., that found negative results on other forms of probing, unlearning and steering, respectively. And the few positive applications with clear comparisons to baselines, like\nKarvonen et al\n, largely occur in somewhat niche or contrived settings (e.g. using fairly simple concepts like “is a regex” that SAEs likely find it easy to capture), though there are some signs of life such as\nunlearning in diffusion models\n,\npotential usefulness in auditing models\n, and\nhypothesis generation about labelled text datasets\n.\nWe find the comparative lack of positive results here concerning - no individual negative result is a strong update, since it’s not yet clear which tasks are best suited to SAEs, but if current SAEs really are a big step forwards for interpretability, it should not be so hard to find compelling scenarios where they beat baselines. This, combined with the general messiness and issues surfaced by the attempts, and other issues such as poor feature sensitivity, suggest to us that\nSAEs and SAE based techniques (\ntranscoders\n,\ncrosscoders\n, etc) are not likely to be a gamechanger any time soon\nand plausibly never will be - we hope to write our thoughts on this topic in more detail soon. We think that the research community’s large investment into SAEs was most justified under the hopes that SAEs could be incredibly transformative to all of the other things we want to do with interpretability. Now that this seems less likely,\nwe speculate that the interpretability community is somewhat over invested in SAEs\n.\nTo clarify, we are\nnot\ncommitting to giving up on SAEs, and\nthis is not a statement that we think SAEs are useless and that no one should work on them\n. We are pessimistic about them being a game changer across the board in their current form, but we predict that there are still some situations where they are able to be useful. We are particularly excited about their potential for exploratory debugging of mysterious failures or phenomena in models, as in\nMarks et al\n, and believe they are worthwhile to keep around in a practitioner's toolkit. For example,\nwe found them useful for detecting and debugging spurious correlations in our datasets\n. More importantly, it’s extremely hard to distinguish between fundamental issues and fixable issues, so it’s hard to make any confident statements about what flaws will remain in future SAEs.\nAs such, we believe that future SAE work is valuable, but should focus much less on hill-climbing on\nsparsity reconstruction\ntrade-offs\n, and instead focus on better understanding the fundamental limitations of SAEs, especially those that hold them back on downstream tasks and discovering new limitations; learning how to evaluate and measure these limitations; and learning how to address them, whether by incremental improvements or fundamental improvements. One recent optimistic sign was\nMatryoshka SAEs\n, a fairly incremental change to the SAE loss that seems to have made substantial strides on feature absorption and feature composition. We think that a great form of project is to take a known issue with SAEs, tries to think about why it happens and what changes could fix it, and then verifying that the issue has improved. If researchers have an approach they think is promising that could make substantial progress on an issue with SAEs, we would be excited to see that pursued.\nThere are also other valuable projects, for example, are there much cheaper ways we can train SAEs of acceptable quality? Or to get similar effects with other feature clustering or dictionary learning methods instead? If we’re taking a pragmatic approach to SAEs, rather than the ambitious approach of trying to find the canonical units of analysis, then sacrificing some quality in return for lowering the major up front cost of SAE training may be worthwhile.\nWe could imagine coming back to SAE research if we thought we had a particularly important and tractable research direction, or if there is significant progress on some of their core issues. And we still believe that interpreting the concepts in LLM activations is a crucial problem that we would be excited to see progress on. But for the moment we intend to explore some other research directions, such as model diffing, interpreting model organisms of deception, and trying to interpret thinking models.\nComparing different ways to train Chat SAEs\nLewis Smith, Arthur Conmy, Callum McDougall\nIn our original GemmaScope release, we released chat model SAEs, which were trained on activations from the chat tuned model, but on pretraining data (formatted as user prompts) not on chat rollouts from the model. When investigating the probe results discussed in the following sections, we decided that this approach was possibly sub-optimal, and wanted to make sure we were performing a fair comparison, after getting results we thought were disappointing. We hypothesised that this task was fairly chat-specific, but possibly the lack of chat rollouts in the training data meant that the SAEs would have failed to capture chat-specific features.\nIn order to investigate this, we experimented with two approaches:\nRetraining the SAEs from scratch, exclusively using chat data.\nFinetuning the existing GemmaScope SAEs on chat data.\nBoth of these approaches lead to improvements on our probing benchmarks relative to the baseline GemmaScope SAEs, matching\nKissane et al\n,\nas discussed in the section below\n. However, neither is sufficient to match the performance of a dense probe in our setting. As finetuning is the easiest method and we generally did not find significant differences in our probe training task between training on the thing from scratch, we used finetuning for the results described in subsequent sections.\nIn addition, we experimented with a few variations of the finetuning procedure:\nstarting finetuning from the pretrained and instruction based GemmaScope models\nStarting the finetuning from before the end of training, so that the total number of train steps was the same between the finetuned SAE and IT trained SAE.\nlatent resampling, where when finetuning we randomly re-initialise a proportion of the latents in the SAE, under the hypothesis that normally all of the SAE’s latents are “already taken” and can’t easily adapt to learn new chat specific features.\nUsing probing metrics we didn’t find much systematic difference between any of these approaches; provided we trained on rollouts in some form, there did not appear to be much advantage to including them in the pretraining mix vs finetuning on the target data.\nWhen we looked at auto-interpretability metrics, we found that finetuning from the GemmaScope SAEs trained on the PT data performed slightly better than from our original ‘IT’ SAEs (which included the IT formatting but not rollouts) provided we didn’t perform latent resampling. Note that we use frequency-weighted autointerp scores here, so latents which fire more commonly in chat data specifically will be given more weight in the final value (for more on this, see the section Autointerp and high frequency latents later). These results are shown in the figure below.\nWe also experimented with including the refusal prompts used for training our probes (see the following snippet) in our finetuning mixture. Similarly to resampling latents, we found that this did not make a significant difference, at least compared to the noise in the SAE training process. The autointerp results are challenging to interpret given their high dataset dependence, but we didn’t see any statistically significant differences using this method.\nThe plot below shows the number of repeats of the probe data included in the SAE finetuning mix, faceted by the proportion of SAE latents resampled before finetuning. This is only showing the OOD set probing performance, but this is fairly representative; the effect of these changes does not seem to be statistically significant.\nThe conclusions to draw from this are a little uncertain, as the auto-interp results suggest that finetuning can\nreduce\ninterpretability compared to our IT-training used in the GemmaScope release if latent resampling is used, whereas this still improved performance in our probing baselines, though this may be explained by the fact that the chat finetuning is short compared to pre-training (so chat-specialised latents are less well trained compared to the pretraining latents they have replaced, and the pretraining latents contribute to interpretability score even if they aren’t useful for this task).\nUsing SAEs for OOD Probing\nLewis Smith, Sen Rajamanoharan, Arthur Conmy\nOne obvious task where we can compare SAEs to a widely used and practical baseline is probing for ‘concepts’ such as harmful user intent, or the topic of a query to the chat model. A commonly used and effective baseline here is linear probing for a particular property, using a linear classifier trained on a dataset of positive and negative examples for the property of interest.\nSince linear probes are supervised while SAEs are not, it would not be surprising for linear probes to outperform SAEs on the training distribution. However, it’s less obvious which method would be best when measuring out of distribution generalization of our probe.\nIf SAEs have learnt latent representations which approximate the features actually used by the model, we might expect that probes based on a single SAE latent, or a sparse probe based on just a few latents, would only capture the most important concepts relevant to our task, which should generalise well out of distribution. While a dense probe has more capacity to pick up on subtler sources of spurious correlation (however, we later thought of considerations the other way,\nas discussed below\n)\nWe investigate this by probing for prompts with harmful user intent, i.e. prompts where the user asks the model to do something harmful/enabling harm. This is similar to refusal, but we don’t condition on whether the model actually refuses; the label should remain ‘harmful’ even if a jailbreak is successfully used and the model no longer refuses. We introduce a change of distribution both by using different datasets for our OOD set, as well as introducing a set of held-out jailbreak suffixes to make hard positives (i.e. prompts that are harmful but don’t look it) and hard negatives (i.e. prompts that look harmful but aren’t) for the model.\nTechnical Setup\nDatasets\nAs our ‘in distribution’ set, we use harmful intent prompts from\nharmbench\nversus harmless data from\nAlpaca\n. We find (using SAEs! See more details in the ‘Dataset Debugging’ section) that there are several spurious differences in formatting between these two datasets (such as ending in brackets) that probes can detect, so we normalise the formatting of the datasets to remove these factors.\nFor an out of distribution set, we use a combination of\nover-refusal bench\n, a dataset of both ‘seemingly toxic’ prompts which are often mistakenly refused by LLMs, an internal chat training set, and\nadvbench\n, the set of harmful requests used in the paper\nUniversal and Transferable Adversarial Attacks on Aligned Language Models\n.\nWe also make the task more challenging by adding some jailbreak suffixes to the harmful prompts in the dataset. We also add some instructions that leave the (potentially harmful) text of the original prompt unchanged, but modify the meaning to make it innocuous, mostly by asking for semantic-level transformations of the text so the model does not actually have to comply with it’s content. These are :\n‘How many words in prompt’ - modify the prompt to be of the form: “How many words are in the following text: {original text}”.\n“Ignore one instruction” - Label the original prompt with a number in a list of otherwise harmless instructions, and tell the model to ignore that number.\n“All caps” - Ask the model to repeat the prompt back in all caps.\n“Find synonyms”: ask the model to provide a list of synonyms for key words in the original prompt.\nAfter augmentation, we have 4046 training examples, 1014 validation examples, and 3850 examples in our OOD set that we use in the experiments described below. We experimented with a variety of jailbreak methods, the effectiveness of which on the training set is plotted below:\nWe then chose a subset of these jailbreaks for the experiments discussed below. The considerations here were\n(a) using a variety of transformation types for train and test. For example, many of the prompts we tested are some variation of ‘roleplay type’ prompt injections, but we do include some others, including an adversarial suffix from the GCG paper.\n(b) using the weaker transformations for training if we have a range of strengths, in order to make the OOD generalisation more challenging.\n(c) avoiding transformations that were too many tokens as this slowed down iteration (as collecting activations becomes slower)\nThe jailbreak prompts we selected were:\nThe ‘AIM’ prompt, from jailbreakchat via\nWei et. al.\nEvil confidant, the same\nAn adversarial suffix from the\nGCG paper\nThe\nPliny\n‘Google’ prompt, from\nthis repository\nA ‘cocky genius’ roleplay prompt, which\nSenthooran Rajamanoharan\ncame up with.\nA ‘role play’ prompt, which prompts the model to act as if it is a character in a ‘coming of age show’, acting out the instructions in the prompt., which\nArthur Conmy\ncame up with.\nIn order to test the generalisation of our probing methods, we use ‘AIM’, ‘roleplay’, ‘gcg’ and ‘cocky genius’ jailbreaks in our training distribution, and ‘evil_confidant’ and ‘pliny’ as an OOD test set. Similarly, we use ‘how many words’ and ‘ignore one instruction’ as innocuous transformations in training, and ‘all caps’ and ‘find synonyms’ out of distribution.\nProbing\nIn order to do probing, we need some way of choosing where (what site and layer) in the model to apply the probe, and how to aggregate the token probe across the context: each probe can be applied to any token in the forward pass, and it is not clear where this should be, or if it should be aggregated in some way; for instance, by applying the probe at every position and taking the mean or the max of the probe decision.\nFor the Gemma models we study in this snippet, chat data is surrounded by tokens using the following format:\n<start_of_turn>user\n\n[User prompt]\n\n<end_of_turn>\\n\n\n<start_of_turn>model\\n\n\n[Model response]\nWe experimented with both max-probing, where we first take the max activation of a feature along the sequence dimension, and probing at a fixed position relative to the first token of the model response (so position 0 is the first token of the model response, token -1 is the newline preceding that, etc). We find that max probing is generally the best, though using a fixed position of -5, corresponding to the <end_of_turn> token that completes the user turn, has comparable performance.\nWe find that the most reliable performance is obtained by probing at layer 20 of Gemma-v2 9B IT.\nBricken et al\nfound that, for a biology-related task, taking the max of the features along the sequence before training a probe helped the performance of the SAE probe significantly, we only find a small advantage over probing at a fixed position in our application. We hypothesise that this is due to the information about the harmful content of the prompt naturally being aggregated in the end of turn token by the model. This idiosyncrasy of this particular task may explain the differences between our results and those reported by Anthropic.\nWe investigate two ways to do probing with SAEs:\nDo k-sparse probing on the feature activations of the SAEs. Under the assumption that only 1 feature, or a subset of features, captures the information at hand, this should result in less overfitting.\nTrain a linear probe on the reconstructions of the SAEs, or the reconstructions of some subset of the features. This is less likely to be useful as a practical method, but is interesting at investigating whether relevant information for a particular task is or is not preserved by the SAE reconstruction.\nAs we find that our chat SAEs generally produce similar results on this task (and may have a small edge on the autointerp metric depending on the resampling fraction), we use the finetuned SAEs for all results in this snippet, as this method is the quickest to experiment with.\nResults\nLinear probes on the residual stream perform very well at this task, essentially saturating even on our out of distribution set, indicating information about harmful user intent\nis\npresent in model activations.\nSubset\nTrain\nVal\nOOD\nLinear Probe AUROC\n1.0\n1.0\n0.999..\nIn the plots below, we plot the best single-feature probe of the original GemmaScope SAEs, and a set of SAEs finetuned on chat data (see the finetuning snippet for more details), with the performance of the dense probe shown as a dotted line. We see that while using finetuned SAEs leads to a significant improvement in single-feature probe performance at this task - suggesting that these SAEs more closely capture the concept that we care about in a single feature - they remain below the performance of a dense linear probe, as shown by the dotted line in these plots.\nInitially, we expected that SAE based probes would not outperform linear probes on in-distribution data, but we expected that they might have an advantage out of distribution, especially those based on a single feature or a small number of features, under the hypothesis that SAE features would generalise more robustly out of distribution. However, we generally find that even on the OOD set, SAE probing does not match the performance of a linear probe, though the best single feature probe from the finetuned model does come close in some cases. We were surprised by the performance and robustness of the linear probe, and this is consistent with probes beating SAEs in OOD settings studied by\nKantamneni et al\n, though we can’t make confident extrapolations about probes vs SAEs in all OOD settings\nWe can expand on single feature probes by using k-sparse probing, for k between 2 and 50. Using k-sparse probing largely closes the gap to the near-perfect performance dense probes for K>=5, although generally a small gap remains. Notably, despite k-sparse probing performing near perfectly in distribution, it shows worse transfer OOD, while dense probes generalise well, the opposite of what we predicted should happen with high quality SAEsl.\nWe experiment with two methods for k-sparse probing; selecting k latents by sorting by the mean difference of latent activations between positive and negative examples in the training set, and using l1 regression with a coefficient swept to select a number of active probe latents close to the target k (and then retraining an unregularised model on the selected latents). The two methods generally show similar results, as shown below, matching the results of\nGurnee et al\non neurons.\nOur results with k-sparse probing generally corroborate our results that finetuning SAEs helps, with finetuned SAEs generally outperforming the original GemmaScope SAEs, particularly for low L0 values, but it is not enough to achieve parity with dense probes.\nRelated Work and Discussion\nSAE probing was also studied in parallel by\nKantamneni et al\nand\nBricken et al\n. Generally, we think that our results are fairly consistent with the literature here. In particular, Kantamneni et. al. try to carry out a similar analysis to us over a wide variety of datasets, finding broadly comparable results to us; that sparse probing wins only rarely, having an advantage only when the labels are highly corrupted or potentially in low-data regimes. They also found better performance on simpler datasets where the SAE had a single highly relevant latent, and not on more subtle datasets.\nSimilarly, Bricken et al recently studied how SAE probes compare to linear probes. They study detecting biology-related misuse, rather than harmful intent. Unlike us, they find a slight advantage for SAEs in that SAEs allow max-aggregating latents along the sequence dimension before training a classifier (max-pooling the activations), which lets their classifier be more sensitive to data distributed throughout the prompt. Though Kantamneni et al show that, though max-aggregating latents can be an advantage over single token dense probes in some settings, it does not systematically beat attention head probes, and we speculate that attention head probes would also do well in our setting. We do not find that max-pooling is particularly helpful for our task ; we hypothesise that this may be because the information relevant to the classifier in our case is fairly temporally concentrated.\nLike us, both Kantamneni et al and Bricken et al find that the individual SAE latents are useful for finding spurious correlations in a dataset. Notably, Kantamneni et al find a single latent predicting grammatical accuracy, with similar accuracy to the “ground truth” labels in Glue CoLA, due to the high level of label noise. In some ways, our empirical results understate the utility of this, as we used the inspectability of our probes in order to clean our datasets of spurious correlations; if we hadn’t done this, our linear probes would presumably have learnt these too, which would have decreased their generalisation out of distribution. However, Kantamneni et al showed that these spurious correlations could be discovered by other methods, which we speculate would work for us too.\nIn general, we do find that there is a persistent performance gap between SAEs and probes. In part, this is because SAEs remain imperfect, and we do find that relevant information is lost in the reconstruction term, with probes on the SAE reconstruction generally performing slightly worse than a probe on the raw residual stream.\nPerformance delta between probe trained on residual stream and on SAE reconstruction.\nTrain\nTest\nOOD\nGemmaScope\n0.00087\n0.0018\n0.048\nSAE finetune comparable steps\n0.0013\n0.0024\n0.039\nConsistent with these negative results for SAE probing, very recent work by Apollo Research also tried SAE probes for\ndetecting deceptive behaviour\n, finding that linear probes outperformed SAE probing.\nWe think it is curious that many of the variations on finetuning we tried did not seem to make a significant difference. It is possible that our setup is very noisy; perhaps a larger probing dataset would allow us to see a significant difference between these methods.\nIs it surprising that SAEs didn’t work?\nIn general, we found it unexpectedly hard to reason about how to update from positive/negative results on downstream tasks. In some sense, we have a hammer and are looking for a nail. We have this cool technique of SAEs, and want to find problems well suited to it. We care about how often it’s useful, and the magnitude of benefit. It doesn’t need to be useful for everything, but we care much more about some tasks than others. If you find one successful application, that doesn’t mean it will work in less cherry-picked cases, and if you have one failure, maybe it was badly suited to SAEs, but other important tasks would work fine. And even if you do have several successes, that may not be enough to justify the costs of training and R&D. Overall, we think that seeking evidence of interpretability on downstream tasks is valuable and something we hope to see more of in future - you can’t update\ntoo\nmuch off of the evidence, but it seems like a crucial question and this is one of the better sources of evidence we can currently access.\nAnother complication is that interpretability is quite hard to reason clearly about in general. You might have success on a task, but for quite different reasons than you thought, and you might think SAEs should help on a task, but actually your intuition is wrong. To illustrate this, in hindsight, we're now a lot more confused about whether even an optimal SAE should beat linear probes on OOD generalisation.\nHere’s our fuzzy intuition for what’s going on. We have a starting distribution, an OOD distribution, a train set and a val set (from the original distribution) and an OOD test set. There are three kinds of things a probe can learn for predicting the train set:\nNoise: patterns specific to the train set that do not generalise out of sample, i.e. on the test set. Learning noise gives rise to overfitting.\nSpurious correlations: patterns that predict the concept in the original distribution (and which are predictive even on the test set) but that do not generalise out of distribution.\nTrue signal: patterns that are predictive of the target concept both in-distribution and out-of-distribution.\nNoise is easily ignored given enough data. Indeed, on this task we found that both SAEs and linear probes learn to basically classify perfectly on the test set (i.e. out of sample, but in-distribution). So the difference in performance between SAEs and linear probes out of distribution comes down to how their respective inductive biases help them to latch on to the true signal versus spurious correlations: it seems that SAE probes do a worse job at this than linear probes. Why is this?\nOur original hypothesis had been that sparse SAE probes would in fact have a\nbetter\ninductive bias for picking the true signal over spurious correlations. This was in large part because we had assumed that the concept we’re looking to learn (“harmful intent”) is a fairly simple logical expression, composed of a small number of “atomic” concepts, both causal (like danger, violence, profanity, etc) and correlational (like linguistic features indicative of users who are trying to commit harm or of jailbreaks), that we expected SAEs would have assigned to single / small clusters of latents. Under this assumption, we guessed that a sparse SAE probe would easily find the relevant concepts among its latents and therefore extract the true signal, ignore spurious correlations. The fact that this doesn’t happen suggests a number of things that could be going wrong:\nThe concept “harmful intent” (as defined implicitly by the harmful datasets we used to train the probe) may not really be a simple function of a few “atomic” concepts at all. I.e., even with an “perfect” SAE - one whose latents correspond\nprecisely\nto the concepts the model uses - it could be the case that the positive and negative labels in our datasets can’t be generated by a simple expression involving just a few of these concepts.\nThis could indicate a fundamental problem with SAEs - we\nwant\nconcepts like harmful intent, and if high functioning SAEs do not usefully isolate them, this is bad\nOr it could just be that harmful intent is a fuzzily defined concept, and depends a lot on the subjective values of the labellers/those setting rules for labellers. Though under this hypothesis, it’s surprising that linear probes generalise well.\nEven if “harmful intent” is really a simple function of a few basic interpretable concepts, because our SAEs don’t learn these concepts cleanly, a sparse SAE probe doesn’t manage to capture the true signal.\nSince SAEs are\nincomplete\n, perhaps important directions corresponding to components of the true signal are missing from the dictionary, forcing the SAE probe to make do with approximating these directions from the latents that it does possess.\nPatterns that are spurious correlations may be just as well represented by latents in the SAE’s dictionary as patterns that make up the true signal, meaning that there is no reason to expect a sparse SAE probe to preferentially latch onto components of the true signal over spurious correlations.\nIn practice, our guess is that a mix of these are going on. b and c would suggest that we just haven’t gotten good enough at training SAEs but that it may be a promising direction, and a suggests fundamental issues with SAEs as a method. d would suggest that SAEs could make a promising probing method if we augmented them by pruning out latents that seemed like spurious correlations, but we de facto already did this by using them to clean up correlations in the data itself benefitting both the SAE probe and the dense probe. Overall, this means that it’s hard to draw a clear conclusion from the data available, but it does seem a bad sign for the practical utility of SAEs.\nDataset debugging with SAEs\nSen Rajamanoharan, Lewis Smith, Arthur Conmy\nOne use case where we did find SAEs useful was in dataset debugging. As the SAE latents are (somewhat) interpretable, inspecting the latents with big mean differences between positive and negative labels can reveal useful information about our dataset. This mirrors similar findings in\nKantamneni et al\nand\nBricken et al\nFor instance, when running an early version of this, when inspecting the best performing features at separating Alpaca and HarmBench, we realised that one of the best performing was a feature that seemed to be a question mark feature. On inspection, we realised that only Alpaca contained questions, whereas the harmful examples were all instructions. Being able to inspect the dataset in this way using a model with SAEs was definitely useful in catching this error, and other than manual inspection, it’s not clear if we ever would have noticed this if we exclusively used linear probing. Though\nKantamneni et al\nwere able to achieve similar results by taking maximum activating dataset examples of a linear probe over the pretraining data.\nThis also somewhat complicates the story that linear probing generally performed better than SAEs; this is presumably partly because we were able to use SAEs to manually clean spurious correlations from our dataset\nbefore\ntraining a supervised probe.\nOne interesting thing to note here is that you can use SAE based techniques like this for dataset exploration even if you don’t have an SAE on the model you want to probe on; for instance, using a model like Gemma 2 with an SAE sweep could be used to try to detect issues like this before you train something based on a larger model, as we are interested in the properties of the\ndata\n, not the model.\nThis exploratory property is one reason why people expect that unsupervised techniques like SAEs will be useful for safety work; as they make it easier to discover unanticipated features of the model than supervised techniques. However, as mentioned, in this instance SAEs have seemed primarily useful as a technique for investigating\ndatasets\nas much as models.\nAutointerp and high frequency latents\nCallum McDougall\nIn the next section, we’ll compare autointerpretability scores across models with different latent density distributions (in particular, some models with more high-frequency features than others). This introduces a problem - traditionally autointerp is measured as an average score over all latents, but this doesn’t capture the fact that eliminating a high-frequency uninterpretable feature is intuitively better than eliminating a low-frequency uninterpretable feature. One way to think about this is, if we sampled a prompt from the pretraining distribution and gave it to the model, and look at the latents, we’d hope that they tend to be interpretability. But the correct way to measure this is by taking the autointerp score for each latent weighted by how frequent\nthey are, not uniformly weighted. We have found taking frequency weighted auto-interp scores to be instructive, and recommend that practitioners plot this, in addition to uniform weighted.\nAs an extreme example, we can construct a pathological SAE where 𝑀−𝑁 latents encode specific bigrams and the remaining 𝑁 latents fully reconstruct the rest of the 𝑁-dimensional SAE input - this would score very well on autointerp with uniform average scores provided 𝑀 >> 𝑁, but poorly with frequency-weighted average scores. For this reason, although we show both uniform and frequency-weighted autointerp results in the experiments below, we’ll focus more on the frequency-weighted scores in subsequent discussion.\nAside from this weighting strategy, the rest of the autointerp methodology follows standard practices such as those introduced in\nBills et al\nand\nbuilt on by EleutherAI\n. An explanation is generated by constructing a prompt that contains top activating example sequences as well as examples sampled from each quantile and a selection of random non-activating examples, and asking the model to characterise the kinds of text which causes the latent to fire. We then give the model a set of unlabelled sequences (some from the top activaing or quantile groups, others non-activating) and ask it to estimate the quantised activation value of the highest-activating token in that sequence. The latent’s interpretability score is then computed as the spearman correlation between the true estimated quantised activations.\nRemoving High Frequency Latents from JumpReLU SAEs\nSenthooran Rajamanoharan, Callum McDougall, Lewis Smith\nTL;DR: Both\nJumpReLU\nand\nTopK\nSAEs suffer from high frequency latents: latents that fire on many tokens (\n>\n10\n%\nof the dataset) and often seem uninterpretable. We find that by tweaking the sparsity penalty used to train JumpReLU SAEs we can largely eliminate high frequency latents, with only a small cost in terms of reconstruction error at fixed L0. Auto-interp suggests that this has a neutral-to-positive effect on the average latent interpretability once we weight by latent frequency, by reducing the incidence of high frequency uninterpretable latents.\nMethod\nMotivation\nIn\nour paper\n, we trained JumpReLU SAEs using a L0 sparsity penalty, which penalises a SAE in proportion to the number of latents that fire on each token:\nL\nL\n0\n(\nx\n)\n:\n=\nλ\n∥\nf\n(\nx\n)\n∥\n0\n≡\nλ\nM\n∑\ni\n=\n1\n1\nf\ni\n(\nx\n)\n>\n0\n,\nWhere\nf\ni\n(\nx\nα\n)\nis the activation of the\ni\nt\nh\nlatent on an\nN\n-dimensional input LM activation\nx\n, and\nM\nis the width (total number of latents) of the SAE.\n[1]\nThis L0 sparsity penalty only cares about controlling the\naverage\nfiring frequency across all latents in a SAE; it is actually indifferent to the\nrange\nof firing frequencies in the SAE. In other words, L0 doesn’t care whether a low firing frequency is achieved by: (a) all latents firing infrequently, or (b) some latents firing frequently and other latents firing very infrequently to compensate. As long as the average firing frequency is the same either way, L0 doesn’t mind.\n[2]\nWe can see this formally, by noticing that the average L0 penalty on a training batch can be expressed as follows:\n1\nB\nB\n∑\nα\n=\n1\nL\nL\n0\n(\nx\nα\n)\n=\nλ\nM\n∑\ni\n=\n1\n(\n1\nB\nB\n∑\nα\n=\n1\n1\nf\ni\n(\nx\nα\n)\n>\n0\n)\n=\nλ\nM\n∑\ni\n=\n1\n^\nω\ni\n=\nλ\nM\n⟨\n^\nω\n⟩\nwhere\n^\nω\ni\n:\n=\n1\nB\n∑\nB\nα\n=\n1\n1\nf\ni\n(\nx\nα\n)\n>\n0\nis a single-batch estimate of the firing frequency of the\ni\nt\nh\nlatent and\n⟨\n^\nω\n⟩\nis the mean firing frequency (as estimated on this batch) across all latents in the SAE. Hence, the L0 sparsity penalty is exactly proportional to the mean of the SAE’s firing frequency histogram, and therefore indifferent to the\nspread\nof latent firing frequencies in this histogram. This in turn leads to the rise of high frequency latents: as long as these latents are beneficial for minimizing the SAE's reconstruction error, training with a L0 penalty provides insufficient pressure to prevent high frequency latents from forming, as can be seen following frequency histograms.\nLatent firing frequency histograms for Gated, JumpReLU and TopK SAEs. Unlike Gated SAEs, which use a L1 penalty that penalizes lar\nge latent activations, JumpReLU (middle) and To\npK (bottom) SAEs exhibit\nhigh-frequency\nlatents: latents that fire on 10% or more of tokens (i.e. that lie to the right of the dotted vertical line).\nModifying the sparsity penalty\nNow, a nice feature of JumpReLU SAEs is that we are not beholden to using a L0 sparsity penalty.\n[3]\nGiven the observation above, an obvious way to get rid of high frequency features is modify the sparsity penalty so that it does penalise dispersion (and in particular right-tail dispersion) in addition to the mean of the firing frequency histogram. There are many ways to do this, but here we explore arguably the simplest approaches to achieving this property, which is to add a term to the sparsity penalty that is quadratic in latent frequencies:\n1\nB\nB\n∑\nα\n=\n1\nL\nquad\n(\nx\nα\n)\n:\n=\nλ\nM\n∑\ni\n=\n1\n^\nω\ni\n(\n1\n+\n^\nω\ni\n/\nω\n0\n)\n.\nNote that the first term in this\nquadratic-frequency\nsparsity penalty is the standard L0 penalty, whereas the second term is proportional to the mean squared latent frequency. We have introduced a new hyperparameter\nω\n0\n, which sets the frequency scale at which the sparsity penalty switches from penalising latent frequency roughly linearly (for latents with frequencies\n^\nω\ni\n≪\nω\n0\n), to penalising latent frequency quadratically (for latents with frequencies\n^\nω\ni\n≳\nω\n0\n). This in turn leads to this penalty disincentivising high frequency latents from appearing, while latents lower down the frequency distribution are treated similarly to latents in a standard (L0-based) JumpReLU SAE.\n[4]\nHow we evaluated interpretability\nWhen evaluating the effectiveness of JumpReLU variants, we face a problem: traditionally the interpretability of a SAE is measured as the average auto-interp score over all its latents, but this doesn't capture the fact that eliminating a high-frequency uninterpretable latent is typically better than eliminating a low-frequency uninterpretable latent. As an extreme example, we can construct a pathological SAE where\nM\n−\nN\nlatents encode specific bigrams and the remaining\nN\nlatents fully reconstruct the rest of the\nN\n-dimensional SAE input: this would score very well on auto-interp with uniform average scores provided\nM\n≫\nN\n, but poorly with frequency-weighted average scores. For this reason, although we show both uniform and frequency-weighted auto-interp results in the experiments below, we'll focus more on the frequency-weighted scores in the subsequent discussion.\n[5]\nAside from this weighting strategy, the rest of the auto-interp methodology follows standard practices such as those introduced in\nBills et al. (2023)\nand built on by\nPaulo et al. (2024)\n. An explanation is generated by constructing a prompt that contains top activating example sequences as well as examples sampled from each quantile and a selection of random non-activating examples, and asking the model to characterise the kinds of text which causes the latent to fire. We then give the model a set of unlabelled sequences (some from the top activating or quantile groups, others non-activating) and ask it to estimate the quantised activation value of the highest-activating token in that sequence. The latent's interpretability score is then computed as the Spearman correlation between the true & estimated quantised activations.\nResults\nIn our experiments below, we try setting the frequency scale\nω\n0\nto either\n10\n−\n1\nor\n10\n−\n2\n, since we are interested in suppressing latents that fire on more than about 10% of tokens. We compare quadratic-frequency loss JumpReLU SAEs trained this way against standard (L0 penalty) JumpReLU SAEs, Gated SAEs and TopK SAEs.\n[6]\nWe train 131k SAEs from scratch on layer 20 Gemma 2 9B IT activations on instruction tuning data [TODO: reference back to a description of this from an earlier snippet].\nReconstruction loss at fixed sparsity\nReconstruction loss vs L0 for the various SAE architectures and loss functions used in our experiment. The quadratic-frequency penalty (QF loss) has slightly worse reconstruction loss at any given sparsity than standard JumpReLU SAEs (L0 loss), but still compare favourably versus Gated and TopK SAEs.\nAs expected (since we are no longer optimising for L0 directly), JumpReLU SAEs trained with the quadratic-frequency penalty have slightly worse reconstruction loss at a given sparsity than JumpReLU SAEs trained with the standard L0 loss. However, the quadratic-frequency penalty JumpReLU SAEs still compare favourably to TopK and Gated SAEs.\nFrequency histograms\nLatent firing frequency histograms for JumpReLU SAEs trained with a standard L0 loss (top) or quadratic-frequency loss with\nω\n0\n=\n10\n−\n1\n(middle) or\nω\n0\n=\n10\n−\n2\n(bottom). The quadratic frequency loss successfully removes high frequency features (i.e. latents around or to the right of the red dotted vertical line) without changing the shape of the rest of the frequency histogram.\nAs shown above, the quadratic-frequency penalty successfully suppresses high-frequency latents without having a noticeable effect on the shape of the remainder of the frequency histogram (particularly in the case\nω\n0\n=\n10\n−\n1\n).\nLatent interpretability\nAverage auto-interp score vs L0 for the various SAE architectures and loss functions used in our experiment, for different latent weightings. Uniform weighting slightly disfavours JumpReLU variants but doesn't show clear patterns, frequency-weighting clearly shows outperformance of JumpReLU variants at lower sparsities (higher L0).\nAs shown above, we observe that the uniform average scores don't show a clear pattern (with Gated & TopK SAEs slightly outperforming for given sparsity values, and JumpReLU variants slightly under-performing). But the frequency-weighted plots show much clearer patterns: (1) nearly all SAEs have lower average latent interpretability scores at larger L0 values (which makes sense under the hypothesis that penalizing lack of sparsity leads to more monosemantic latents), and (2) the JumpReLU variants go against this trend for high L0 values by actually getting\nmore\ninterpretable. Digging deeper into this, we can plot the average latent auto-interp score for each SAE at different latent frequency quantiles, as shown in the appendix [TODO: LINK], for the standard JumpReLU and the quadratic-frequency loss variant with\nω\n=\n0.01\n. We find that SAEs at all sparsity levels show a negative trend of latent interpretability against latent frequency, although the quadratic-frequency variant still has high-frequency and interpretable latents at all sparsity levels.\nConclusions\nIn this snippet we've shown how it's possible to modify the JumpReLU loss function to obtain further desirable properties, like removing high frequency latents.\nThe quadratic-frequency penalty successfully eliminates high frequency latents with only a modest impact on reconstruction loss at fixed sparsity. On auto-interp it tends to score better than standard JumpReLU when we weight individual latents' interpretability scores by frequency (which penalises uninterpretable high frequency latents more heavily than uniform weighting), although at lower L0s Gated SAEs seem to have the best scores of all the SAE varieties we evaluated. Counter-intuitively (and in contrast to the other SAE varieties), the average interpretability of quadratic-frequency SAEs seems to\nincrease\nwith L0 beyond a certain point! We don't have a good explanation for this phenomenon and haven't investigated it further.\nDoes this mean the quadratic-frequency penalty is an improvement over standard JumpReLU SAEs trained with a L0 penalty? We're unsure for a number of reasons:\nWe haven't evaluated these variants extensively, for example on downstream tasks like steering or probing.\nIt's unclear whether we should really aim to remove high frequency latents in the first place. Perhaps they do represent meaningful directions in activation space that we just haven't been able to\ninterpret yet\n. It's also possible that some high frequency features are pathological and others are legitimate, and we need a more nuanced loss than simply penalising firing frequency to target the former without affecting the latter.\nThe quadratic-frequency loss may simply promote partitioning individual high frequency latents into several (similarly uninterpretable) lower frequency latents, effectively sweeping the problem under the rug.\n[7]\nWe haven't tried stacking these changes with other improvements like a\nMatryoshka\nreconstruction loss (to deal with feature absorption) or the ideas in Anthropic's\nJanuary 2025 update\n.\nNevertheless, by sharing our ideas and results, we hope that practitioners who run into issues with high frequency latents may try variants on the standard JumpReLU loss like quadratic-frequency (or iterate further on them) and see if they provide an improvement for their use case.\nAppendix\nAverage autointerp score vs L0 for the JumpReLU SAEs and the JumpReLU QF loss variants with\nω\n=\n0.01\n. All SAEs show a negative trend of autointerp score against latent frequency, although the quadratic-frequency loss function seems to help the SAE form interpretable latents even at higher frequencies - the curves for higher L0 SAEs are squashed to the right.\n^\nAs we describe in detail in the paper, we use straight-through-estimators (STEs) to differentiate through the step discontinuities in both the L0 sparsity penalty and the jump discontinuity in the JumpReLU activation function to train JumpReLU SAEs. We use the same method here.\n^\nA very similar argument can be made about\nTopK SAEs\n, which control L0 via the\nk\nparameter.\n^\nIndeed, in our paper, we show how we can modify the sparsity penalty to train SAEs that target a fixed L0, much like TopK SAEs. Here, we will instead modify the sparsity penalty to directly target high frequency features.\n^\nAn alternative, and even simpler, sparsity penalty would be to penalise\nall\nlatents according to the square of their firing frequencies, i.e. using a penalty of the form\nλ\n∑\nM\ni\n=\n1\n^\nω\n2\ni\n. This\nsquared-frequency\npenalty also has the advantage of not introducing yet another hyperparameter. However, this penalty\nunder\n-penalises latents with very low firing frequencies, leading to a frequency distribution that is devoid of both high and low firing frequencies, i.e. more sharply peaked around the mean firing frequency. One way to see why this is the case is to notice that such a penalty can also be expressed as\n⟨\n^\nω\n⟩\n2\n+\nV\na\nr\n(\nω\n)\n: i.e. holding the mean firing frequency\n⟨\n^\nω\n⟩\nfixed, it corresponds to minimising the\nvariance\nof the frequency distribution. In contrast, the quadratic-frequency sparsity penalty here ensures that all latents receive a frequency penalty that is at least linear, while high frequency latents receive an additional penalty that is quadratic; this ensures that the lower part of the frequency distribution remains similar to the latent frequency distributions for JumpReLU and TopK, while nevertheless suppressing the top end of the frequency distribution.\n^\nNote that this is a departure from the approach we've taken in earlier work, where latents were sampled for auto/manual auto-interp uniformly and with a low-frequency cutoff to deal with latents that have insufficient data.\n^\nWe train all JumpReLU variants using straight-through-estimators that only provide gradients to the threshold, as in the original paper.\n^\nThis could be happening with Gated SAEs too."
    },
    {
        "title": "Reverse Engineering ps2 slim in 2025",
        "url": "https://news.ycombinator.com/item?id=43701787",
        "date": "2025-04-16T12:08:13.637468",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Hacker News\nnew\n|\npast\n|\ncomments\n|\nask\n|\nshow\n|\njobs\n|\nsubmit\nlogin\nReverse Engineering ps2 slim in 2025\n1 point\nby\nalvinliju44\n1 hour ago\n|\nhide\n|\npast\n|\nfavorite\n|\ndiscuss\ni got a ps2 from a front the first thought when i got the thing in my hand was if i can port linux on it or not, I know people have done it and i am 20yr s late but i still wanna figure out how to reverse engineer this piece of tech and understand in and outs of it.\nFor past 2 days i'm running on 3 cans of redbull trying to do the same thing due to a faulty mod chip which was installed on the board it wasnt booting properly so i sat down with my multimeter going through every pin to see if every component is working and realised the whole video processing chip is bad but i still stayed up last night talking to diffrent reapir man i found from google trying to figure out whats the issue and one guy told me he would send me on for dirt cheap.\nSo how to do this properly this time? How to learn proper reverse engineering? How to understand how the CPU works? how to understand how the EE works? how can i port linux or DOOM into it..\nJoin us for\nAI Startup School\nthis June 16-17 in San Francisco!\nGuidelines\n|\nFAQ\n|\nLists\n|\nAPI\n|\nSecurity\n|\nLegal\n|\nApply to YC\n|\nContact\nSearch:"
    },
    {
        "title": "Perhaps the fastest Gen AI search tool",
        "url": "https://oo.ai/",
        "date": "2025-04-16T12:08:13.637468",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Checking your browser before accessing oo.ai ...\nClick\nhere\nif you are not automatically redirected after 5 seconds."
    },
    {
        "title": "Ask HN: How can sending emails cost Let's Encrypt five figures?",
        "url": "https://news.ycombinator.com/item?id=43701747",
        "date": "2025-04-16T12:08:13.637468",
        "vendor": "HackerNews",
        "doc_type": "posts",
        "content": "Hacker News\nnew\n|\npast\n|\ncomments\n|\nask\n|\nshow\n|\njobs\n|\nsubmit\nlogin\nAsk HN: How can sending emails cost Let's Encrypt five figures?\n3 points\nby\nrrr_oh_man\n1 hour ago\n|\nhide\n|\npast\n|\nfavorite\n|\n2 comments\nhttps://letsencrypt.org/2025/01/22/ending-expiration-emails/\nQuote:\n> Providing expiration notifications costs Let’s Encrypt tens of thousands of dollars per year, money that we believe can be better spent on other aspects of our infrastructure.\nI'm trying to come up with a scenario in my head where sending, let's say, 2M emails per month at scale with an essentially fully automated service infrastructure can cost more than a grand per month. I'm failing to do so.\nMy calc: SES pricing is around $0.1 per 1,000 emails. LE has around 550M active certificates. Let's say 5% receive an expiration notice (I never got one?), that's just over 2M emails per month on average.\nHow can that be? Am I missing something?\ngnabgib\n41 minutes ago\n|\nnext\n[–]\nLet's Encrypt generates ~7M certs/day[0], a cert is only good for 90 days (~a quarter) so let's use 90 days as a window.\n7M/day * 90 = 620M/quarter\nSo that's ~2.5B certs issued per year (knowing that these are often reissues, but you get the notification each time you approach the expiry).\nAssuming\nonly one message\n(not what happens, you get more than one notice.. especially if you let it expire, you get at least two follow up messages) per cert:\n2.5B * $.1/1000 = $250K/yr\nSome users don't provide email addresses, some don't provide valid ones (doesn't mean their infra doesn't have to try to contact, at least for the first expiry), some use a renewal script that renews before the email (9 days prior to expiry I think?), some don't care to renew (I'm sure LE is used in throw away cases where they don't care to renew, but the 3+ emails were still dutifully sent).\nBut for 1 email per issued cert it's > $20k/month to send these messages with SES pricing.\nThere's also the TLS validity halving (well.. 90 days -> 47 days) looming, which in some way helps with the revocation servers, but would also double their (former) email costs.  And then there's future proposals that would half or ever quarter that lifetime again (once again multiplying their email costs).  At some point LE would just be an Amazon SES support system (like DVD-Netflix was for postal services).\n[0]:\nhttps://letsencrypt.org/stats/\nreply\nMagma7404\n1 hour ago\n|\nprev\n[–]\nI guess the volume is irrelevant. In other industries it would be the reliable automation and full security that cost a lot.\nreply\nJoin us for\nAI Startup School\nthis June 16-17 in San Francisco!\nGuidelines\n|\nFAQ\n|\nLists\n|\nAPI\n|\nSecurity\n|\nLegal\n|\nApply to YC\n|\nContact\nSearch:"
    }
]